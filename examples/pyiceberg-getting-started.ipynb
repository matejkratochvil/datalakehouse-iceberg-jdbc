{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T00:57:07.698521Z",
     "start_time": "2025-06-25T00:57:07.693742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyiceberg import __version__\n",
    "\n",
    "__version__\n"
   ],
   "id": "f9fbb279cfc79314",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ## Load NYC Taxi/Limousine Trip Data\n",
    "# \n",
    "# For this notebook, we will use the New York City Taxi and Limousine Commision Trip Record Data that's available on the AWS Open Data Registry. This contains data of trips taken by taxis and for-hire vehicles in New York City. We'll save this into an iceberg table called `taxis`.\n",
    "# \n",
    "# First, load the Parquet file using PyArrow:\n"
   ],
   "id": "1e0c48cbb4f08180"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T00:57:08.867073Z",
     "start_time": "2025-06-25T00:57:08.366645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "tbl_taxis = pq.read_table('/home/iceberg/data/yellow_tripdata_2021-04.parquet')\n",
    "tbl_taxis\n"
   ],
   "id": "ee13fbd7c5b040b",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "/home/iceberg/data/yellow_tripdata_2021-04.parquet",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpyarrow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mparquet\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpq\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m tbl_taxis = \u001B[43mpq\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_table\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m/home/iceberg/data/yellow_tripdata_2021-04.parquet\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      4\u001B[39m tbl_taxis\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/proj/my/data_tutorials/.venv/lib/python3.12/site-packages/pyarrow/parquet/core.py:1774\u001B[39m, in \u001B[36mread_table\u001B[39m\u001B[34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001B[39m\n\u001B[32m   1764\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mread_table\u001B[39m(source, *, columns=\u001B[38;5;28;01mNone\u001B[39;00m, use_threads=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m   1765\u001B[39m                schema=\u001B[38;5;28;01mNone\u001B[39;00m, use_pandas_metadata=\u001B[38;5;28;01mFalse\u001B[39;00m, read_dictionary=\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1766\u001B[39m                memory_map=\u001B[38;5;28;01mFalse\u001B[39;00m, buffer_size=\u001B[32m0\u001B[39m, partitioning=\u001B[33m\"\u001B[39m\u001B[33mhive\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1770\u001B[39m                thrift_container_size_limit=\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1771\u001B[39m                page_checksum_verification=\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1774\u001B[39m         dataset = \u001B[43mParquetDataset\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1775\u001B[39m \u001B[43m            \u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1776\u001B[39m \u001B[43m            \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m=\u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1777\u001B[39m \u001B[43m            \u001B[49m\u001B[43mfilesystem\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfilesystem\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1778\u001B[39m \u001B[43m            \u001B[49m\u001B[43mpartitioning\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpartitioning\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1779\u001B[39m \u001B[43m            \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1780\u001B[39m \u001B[43m            \u001B[49m\u001B[43mread_dictionary\u001B[49m\u001B[43m=\u001B[49m\u001B[43mread_dictionary\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1781\u001B[39m \u001B[43m            \u001B[49m\u001B[43mbuffer_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbuffer_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1782\u001B[39m \u001B[43m            \u001B[49m\u001B[43mfilters\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfilters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1783\u001B[39m \u001B[43m            \u001B[49m\u001B[43mignore_prefixes\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_prefixes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1784\u001B[39m \u001B[43m            \u001B[49m\u001B[43mpre_buffer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpre_buffer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1785\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcoerce_int96_timestamp_unit\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcoerce_int96_timestamp_unit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1786\u001B[39m \u001B[43m            \u001B[49m\u001B[43mdecryption_properties\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecryption_properties\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1787\u001B[39m \u001B[43m            \u001B[49m\u001B[43mthrift_string_size_limit\u001B[49m\u001B[43m=\u001B[49m\u001B[43mthrift_string_size_limit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1788\u001B[39m \u001B[43m            \u001B[49m\u001B[43mthrift_container_size_limit\u001B[49m\u001B[43m=\u001B[49m\u001B[43mthrift_container_size_limit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1789\u001B[39m \u001B[43m            \u001B[49m\u001B[43mpage_checksum_verification\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpage_checksum_verification\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1790\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1791\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n\u001B[32m   1792\u001B[39m         \u001B[38;5;66;03m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001B[39;00m\n\u001B[32m   1793\u001B[39m         \u001B[38;5;66;03m# module is not available\u001B[39;00m\n\u001B[32m   1794\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m filters \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/proj/my/data_tutorials/.venv/lib/python3.12/site-packages/pyarrow/parquet/core.py:1361\u001B[39m, in \u001B[36mParquetDataset.__init__\u001B[39m\u001B[34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001B[39m\n\u001B[32m   1357\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m partitioning == \u001B[33m\"\u001B[39m\u001B[33mhive\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m   1358\u001B[39m     partitioning = ds.HivePartitioning.discover(\n\u001B[32m   1359\u001B[39m         infer_dictionary=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m-> \u001B[39m\u001B[32m1361\u001B[39m \u001B[38;5;28mself\u001B[39m._dataset = \u001B[43mds\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath_or_paths\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilesystem\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfilesystem\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1362\u001B[39m \u001B[43m                           \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m=\u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m=\u001B[49m\u001B[43mparquet_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1363\u001B[39m \u001B[43m                           \u001B[49m\u001B[43mpartitioning\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpartitioning\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1364\u001B[39m \u001B[43m                           \u001B[49m\u001B[43mignore_prefixes\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_prefixes\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/proj/my/data_tutorials/.venv/lib/python3.12/site-packages/pyarrow/dataset.py:794\u001B[39m, in \u001B[36mdataset\u001B[39m\u001B[34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001B[39m\n\u001B[32m    783\u001B[39m kwargs = \u001B[38;5;28mdict\u001B[39m(\n\u001B[32m    784\u001B[39m     schema=schema,\n\u001B[32m    785\u001B[39m     filesystem=filesystem,\n\u001B[32m   (...)\u001B[39m\u001B[32m    790\u001B[39m     selector_ignore_prefixes=ignore_prefixes\n\u001B[32m    791\u001B[39m )\n\u001B[32m    793\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m _is_path_like(source):\n\u001B[32m--> \u001B[39m\u001B[32m794\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_filesystem_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    795\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(source, (\u001B[38;5;28mtuple\u001B[39m, \u001B[38;5;28mlist\u001B[39m)):\n\u001B[32m    796\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mall\u001B[39m(_is_path_like(elem) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, FileInfo) \u001B[38;5;28;01mfor\u001B[39;00m elem \u001B[38;5;129;01min\u001B[39;00m source):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/proj/my/data_tutorials/.venv/lib/python3.12/site-packages/pyarrow/dataset.py:476\u001B[39m, in \u001B[36m_filesystem_dataset\u001B[39m\u001B[34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001B[39m\n\u001B[32m    474\u001B[39m         fs, paths_or_selector = _ensure_multiple_sources(source, filesystem)\n\u001B[32m    475\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m476\u001B[39m     fs, paths_or_selector = \u001B[43m_ensure_single_source\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilesystem\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    478\u001B[39m options = FileSystemFactoryOptions(\n\u001B[32m    479\u001B[39m     partitioning=partitioning,\n\u001B[32m    480\u001B[39m     partition_base_dir=partition_base_dir,\n\u001B[32m    481\u001B[39m     exclude_invalid_files=exclude_invalid_files,\n\u001B[32m    482\u001B[39m     selector_ignore_prefixes=selector_ignore_prefixes\n\u001B[32m    483\u001B[39m )\n\u001B[32m    484\u001B[39m factory = FileSystemDatasetFactory(fs, paths_or_selector, \u001B[38;5;28mformat\u001B[39m, options)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/proj/my/data_tutorials/.venv/lib/python3.12/site-packages/pyarrow/dataset.py:441\u001B[39m, in \u001B[36m_ensure_single_source\u001B[39m\u001B[34m(path, filesystem)\u001B[39m\n\u001B[32m    439\u001B[39m     paths_or_selector = [path]\n\u001B[32m    440\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m441\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(path)\n\u001B[32m    443\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m filesystem, paths_or_selector\n",
      "\u001B[31mFileNotFoundError\u001B[39m: /home/iceberg/data/yellow_tripdata_2021-04.parquet"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ## Creating the table\n",
    "# \n",
    "# Next, create the namespace, and the `taxis` table from the schema that's derived from the Arrow schema:\n"
   ],
   "id": "2f069e3ea5a329c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyiceberg.catalog import load_catalog\n",
    "from pyiceberg.exceptions import NamespaceAlreadyExistsError\n",
    "\n",
    "cat = load_catalog('default')\n",
    "\n",
    "try:\n",
    "    cat.create_namespace('default')\n",
    "except NamespaceAlreadyExistsError:\n",
    "    pass\n"
   ],
   "id": "9d78708259dfcf7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyiceberg.exceptions import NoSuchTableError\n",
    "\n",
    "try:\n",
    "    cat.drop_table('default.taxis')\n",
    "except NoSuchTableError:\n",
    "    pass\n",
    "\n",
    "tbl = cat.create_table(\n",
    "    'default.taxis',\n",
    "    schema=tbl_taxis.schema\n",
    ")\n",
    "\n",
    "tbl\n"
   ],
   "id": "72249e3ac2f7314e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ## Write the actual data into the table\n",
    "# \n",
    "# This will create a new snapshot on the table:\n"
   ],
   "id": "19cc37ec9e40a089"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tbl.overwrite(tbl_taxis)\n",
    "\n",
    "tbl\n"
   ],
   "id": "9819914fd49b2294"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ## Append more data\n",
    "# \n",
    "# Let's append another month of data to the table:\n"
   ],
   "id": "307e8a7d9c3ff6a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tbl.append(pq.read_table('/home/iceberg/data/yellow_tripdata_2021-05.parquet'))\n",
    "tbl\n"
   ],
   "id": "95f9f61efb630b0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ## Load data into a PyArrow Dataframe\n",
    "# \n",
    "# We'll fetch the table using the REST catalog that comes with the setup.\n"
   ],
   "id": "2b8b15fb72d8ad7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tbl = cat.load_table('default.taxis')\n",
    "\n",
    "sc = tbl.scan(row_filter=\"tpep_pickup_datetime >= '2021-05-01T00:00:00.000000'\")\n"
   ],
   "id": "f36a4bd2de463f78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df = sc.to_arrow().to_pandas()\n",
   "id": "b256e1fb9bdf664e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "len(df)\n",
   "id": "898120d523535dec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.info()\n",
   "id": "34b348376c443543"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df\n",
   "id": "9b874fea0d22337c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.hist(column='fare_amount')\n",
   "id": "d5c19a836fc289af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "stats.zscore(df['fare_amount'])\n",
    "\n",
    "# Remove everything larger than 3 stddev\n",
    "df = df[(np.abs(stats.zscore(df['fare_amount'])) < 3)]\n",
    "# Remove everything below zero\n",
    "df = df[df['fare_amount'] > 0]\n"
   ],
   "id": "40faa50d831d11c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.hist(column='fare_amount')\n",
   "id": "6c689c47167f8dac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # DuckDB\n",
    "# \n",
    "# Use DuckDB to Query the PyArrow Dataframe directly.\n"
   ],
   "id": "87f9e33369e466be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T15:26:02.223559Z",
     "start_time": "2025-06-24T15:26:01.636258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext sql\n",
    "%config SqlMagic.autopandas = True\n",
    "%config SqlMagic.feedback = False\n",
    "%config SqlMagic.displaycon = False\n",
    "%sql duckdb:///:memory:\n"
   ],
   "id": "f557f75fb63f6d67",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/kratochvmat/proj/my/data_tutorials/.venv/lib/python3.12/site-packages/sql/connection.py\", line 45, in __init__\n",
      "    engine = sqlalchemy.create_engine(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<string>\", line 2, in create_engine\n",
      "  File \"/Users/kratochvmat/proj/my/data_tutorials/.venv/lib/python3.12/site-packages/sqlalchemy/util/deprecations.py\", line 281, in warned\n",
      "    return fn(*args, **kwargs)  # type: ignore[no-any-return]\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kratochvmat/proj/my/data_tutorials/.venv/lib/python3.12/site-packages/sqlalchemy/engine/create.py\", line 553, in create_engine\n",
      "    entrypoint = u._get_entrypoint()\n",
      "                 ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kratochvmat/proj/my/data_tutorials/.venv/lib/python3.12/site-packages/sqlalchemy/engine/url.py\", line 772, in _get_entrypoint\n",
      "    cls = registry.load(name)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kratochvmat/proj/my/data_tutorials/.venv/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py\", line 453, in load\n",
      "    raise exc.NoSuchModuleError(\n",
      "sqlalchemy.exc.NoSuchModuleError: Can't load plugin: sqlalchemy.dialects:duckdb\n",
      "\n",
      "Connection info needed in SQLAlchemy format, example:\n",
      "               postgresql://username:password@hostname/dbname\n",
      "               or an existing connection: dict_keys([])\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kratochvmat/proj/my/data_tutorials/.venv/lib/python3.12/site-packages/sql/magic.py\", line 196, in execute\n",
      "    conn = sql.connection.Connection.set(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kratochvmat/proj/my/data_tutorials/.venv/lib/python3.12/site-packages/sql/connection.py\", line 70, in set\n",
      "    cls.current = existing or Connection(descriptor, connect_args, creator)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kratochvmat/proj/my/data_tutorials/.venv/lib/python3.12/site-packages/sql/connection.py\", line 45, in __init__\n",
      "    engine = sqlalchemy.create_engine(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<string>\", line 2, in create_engine\n",
      "  File \"/Users/kratochvmat/proj/my/data_tutorials/.venv/lib/python3.12/site-packages/sqlalchemy/util/deprecations.py\", line 281, in warned\n",
      "    return fn(*args, **kwargs)  # type: ignore[no-any-return]\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kratochvmat/proj/my/data_tutorials/.venv/lib/python3.12/site-packages/sqlalchemy/engine/create.py\", line 553, in create_engine\n",
      "    entrypoint = u._get_entrypoint()\n",
      "                 ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kratochvmat/proj/my/data_tutorials/.venv/lib/python3.12/site-packages/sqlalchemy/engine/url.py\", line 772, in _get_entrypoint\n",
      "    cls = registry.load(name)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kratochvmat/proj/my/data_tutorials/.venv/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py\", line 453, in load\n",
      "    raise exc.NoSuchModuleError(\n",
      "sqlalchemy.exc.NoSuchModuleError: Can't load plugin: sqlalchemy.dialects:duckdb\n",
      "\n",
      "Connection info needed in SQLAlchemy format, example:\n",
      "               postgresql://username:password@hostname/dbname\n",
      "               or an existing connection: dict_keys([])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%sql SELECT * FROM df LIMIT 20\n",
   "id": "f1231ea16983c500"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%sql --save tip_amount --no-execute\n",
    "\n",
    "SELECT tip_amount\n",
    "FROM df\n"
   ],
   "id": "3467adc471c75f20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%sqlplot histogram --table df --column tip_amount --bins 22 --with tip_amount\n",
   "id": "b4eb4bf3950e21bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%sql --save tip_amount_filtered --no-execute\n",
    "\n",
    "WITH tip_amount_stddev AS (\n",
    "    SELECT STDDEV_POP(tip_amount) AS tip_amount_stddev\n",
    "    FROM df\n",
    ")\n",
    "\n",
    "SELECT tip_amount\n",
    "FROM df, tip_amount_stddev\n",
    "WHERE tip_amount > 0\n",
    "  AND tip_amount < tip_amount_stddev * 3\n"
   ],
   "id": "b31bdfa145a33064"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%sqlplot histogram --table tip_amount_filtered --column tip_amount --bins 50 --with tip_amount_filtered\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "2c33a0c9f354f750"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
