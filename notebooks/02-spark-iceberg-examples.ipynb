{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b213c576",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Import PySpark and Create SparkSession\n",
    " - The SparkSession should be automatically configured by PYSPARK_SUBMIT_ARGS\n",
    " - defined in docker-compose.yml for the jupyterlab service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cff1942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "061b3ac2-a387-4520-855a-f44c5fc0880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0258b9a1-f759-4053-b178-485ae1c6e868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6eec9d",
   "metadata": {},
   "source": [
    "### Create SparkSession, manually set configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e6eaf67-ef48-49a7-adf0-1e846eb94774",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-115d588d-2513-425f-8f2c-f64df4972beb;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.9.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound org.postgresql#postgresql;42.6.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      ":: resolution report :: resolve 121ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.9.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.6.0 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   6   |   0   |   0   |   0   ||   6   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-115d588d-2513-425f-8f2c-f64df4972beb\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 6 already retrieved (0kB/2ms)\n",
      "25/05/14 11:33:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg_jdbc\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    # .config(\"spark.sql.catalog.iceberg_jdbc.catalog-impl\", \"org.apache.iceberg.jdbc.JdbcCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg_jdbc.uri\", \"jdbc:postgresql://postgres_catalog:5432/iceberg_catalog\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg_jdbc.jdbc.user\", \"iceberg\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg_jdbc.jdbc.password\", \"icebergpassword\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg_jdbc.warehouse\", \"s3a://iceberg-warehouse/\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"password\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "                     ).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0ba14ef-c6ae-414b-9e38-39e5a7f83196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession created successfully!\n",
      "Spark version: 3.5.3\n",
      "+-------------+---------+\n",
      "|      catalog|namespace|\n",
      "+-------------+---------+\n",
      "|spark_catalog|  default|\n",
      "+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"SparkSession created successfully!\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "# Verify Iceberg catalog configuration - Should show default once catalog is used\n",
    "spark.sql(\"SHOW CURRENT NAMESPACE\").show()\n",
    "# Define the catalog name we configured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43ad7709-3ca0-4343-bd2d-078ce956c556",
   "metadata": {},
   "outputs": [],
   "source": [
    "iceberg_catalog_name = \"iceberg_jdbc\" # Must match spark.sql.catalog.iceberg_jdbc in config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6d20fe",
   "metadata": {},
   "source": [
    "Create a Database/Schema in Iceberg using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "699a65fa-1838-4e64-afc6-4b4541e0a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"spark_schema\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b73464b-98b6-47df-9314-6cedcd8228e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+------------+\n",
      "|   namespace|\n",
      "+------------+\n",
      "|   my_schema|\n",
      "|spark_schema|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {iceberg_catalog_name}.{db_name}\").show()\n",
    "spark.sql(f\"USE {iceberg_catalog_name}.{db_name}\")\n",
    "spark.sql(f\"SHOW DATABASES IN {iceberg_catalog_name}\").show()\n",
    "# In Spark, `DATABASE` and `SCHEMA` are often used interchangeably.\n",
    "# Iceberg uses `NAMESPACE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18a62f3a-ff0a-4b98-aac8-cc4e45fc74b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name='my_schema'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8ba79c",
   "metadata": {},
   "source": [
    " Create an Iceberg Table with Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f110f3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/14 11:33:52 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_name = \"spark_orders\"\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {iceberg_catalog_name}.{db_name}.{table_name} (\n",
    "    order_id STRING,\n",
    "    customer_id STRING,\n",
    "    order_date DATE,\n",
    "    amount DECIMAL(10, 2),\n",
    "    category STRING\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (category, days(order_date)) -- Column partitioning and hidden partitioning by day\n",
    "TBLPROPERTIES (\n",
    "    'write.format.default'='parquet',\n",
    "    'format-version'='2'\n",
    ")\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2c0a931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-----------+\n",
      "|namespace|   tableName|isTemporary|\n",
      "+---------+------------+-----------+\n",
      "|my_schema|spark_orders|      false|\n",
      "+---------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"SHOW TABLES IN {iceberg_catalog_name}.{db_name}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc75e32",
   "metadata": {},
   "source": [
    "Insert Data using Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb6734d8",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted into my_schema.spark_orders\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "INSERT INTO {iceberg_catalog_name}.{db_name}.{table_name} VALUES\n",
    "('ORD001', 'CUST101', DATE '2023-01-15', 100.50, 'electronics'),\n",
    "('ORD002', 'CUST102', DATE '2023-01-16', 75.20, 'books'),\n",
    "('ORD003', 'CUST101', DATE '2023-01-16', 250.00, 'electronics'),\n",
    "('ORD004', 'CUST103', DATE '2023-01-17', 45.99, 'home'),\n",
    "('ORD005', 'CUST102', DATE '2023-01-18', 120.00, 'books')\n",
    "\"\"\")\n",
    "print(f\"Data inserted into {db_name}.{table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72d94ace-b12e-4018-aa57-f7be7b839739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying all data from my_schema.spark_orders:\n",
      "+--------+-----------+----------+------+-----------+\n",
      "|order_id|customer_id|order_date|amount|   category|\n",
      "+--------+-----------+----------+------+-----------+\n",
      "|  ORD001|    CUST101|2023-01-15|100.50|electronics|\n",
      "|  ORD001|    CUST101|2023-01-15|100.50|electronics|\n",
      "|  ORD001|    CUST101|2023-01-15|100.50|electronics|\n",
      "|  ORD003|    CUST101|2023-01-16|250.00|electronics|\n",
      "|  ORD003|    CUST101|2023-01-16|250.00|electronics|\n",
      "|  ORD002|    CUST102|2023-01-16| 75.20|      books|\n",
      "|  ORD002|    CUST102|2023-01-16| 75.20|      books|\n",
      "|  ORD003|    CUST101|2023-01-16|250.00|electronics|\n",
      "|  ORD002|    CUST102|2023-01-16| 75.20|      books|\n",
      "|  ORD004|    CUST103|2023-01-17| 45.99|       home|\n",
      "|  ORD004|    CUST103|2023-01-17| 45.99|       home|\n",
      "|  ORD004|    CUST103|2023-01-17| 45.99|       home|\n",
      "|  ORD005|    CUST102|2023-01-18|120.00|      books|\n",
      "|  ORD005|    CUST102|2023-01-18|120.00|      books|\n",
      "|  ORD005|    CUST102|2023-01-18|120.00|      books|\n",
      "|  ORD006|    CUST104|2023-01-19|300.20|electronics|\n",
      "|  ORD007|    CUST105|2023-01-19| 22.20|      books|\n",
      "+--------+-----------+----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Querying all data from {db_name}.{table_name}:\")\n",
    "spark.sql(f\"SELECT * FROM {iceberg_catalog_name}.{db_name}.{table_name} ORDER BY order_date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfcdce6",
   "metadata": {},
   "source": [
    "Select Data using Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "684ac83e-062b-4c53-8bd0-8b379a258d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying all data from my_schema.spark_orders:\n",
      "+--------+-----------+----------+------+-----------+\n",
      "|order_id|customer_id|order_date|amount|   category|\n",
      "+--------+-----------+----------+------+-----------+\n",
      "|  ORD001|    CUST101|2023-01-15|100.50|electronics|\n",
      "|  ORD001|    CUST101|2023-01-15|100.50|electronics|\n",
      "|  ORD001|    CUST101|2023-01-15|100.50|electronics|\n",
      "|  ORD003|    CUST101|2023-01-16|250.00|electronics|\n",
      "|  ORD002|    CUST102|2023-01-16| 75.20|      books|\n",
      "|  ORD002|    CUST102|2023-01-16| 75.20|      books|\n",
      "|  ORD003|    CUST101|2023-01-16|250.00|electronics|\n",
      "|  ORD003|    CUST101|2023-01-16|250.00|electronics|\n",
      "|  ORD002|    CUST102|2023-01-16| 75.20|      books|\n",
      "|  ORD004|    CUST103|2023-01-17| 45.99|       home|\n",
      "|  ORD004|    CUST103|2023-01-17| 45.99|       home|\n",
      "|  ORD004|    CUST103|2023-01-17| 45.99|       home|\n",
      "|  ORD005|    CUST102|2023-01-18|120.00|      books|\n",
      "|  ORD005|    CUST102|2023-01-18|120.00|      books|\n",
      "|  ORD005|    CUST102|2023-01-18|120.00|      books|\n",
      "|  ORD006|    CUST104|2023-01-19|300.20|electronics|\n",
      "|  ORD007|    CUST105|2023-01-19| 22.20|      books|\n",
      "+--------+-----------+----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Querying all data from {db_name}.{table_name}:\")\n",
    "spark.sql(f\"SELECT * FROM {iceberg_catalog_name}.{db_name}.{table_name} ORDER BY order_date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0543deef-0be1-4c78-a5c8-bdf55b3f87a6",
   "metadata": {},
   "source": [
    "### Querying electronics orders (demonstrating partition filter pushdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f20b073-bc3b-4262-b0f2-484c5b103eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------+-----------+\n",
      "|order_id|customer_id|order_date|amount|   category|\n",
      "+--------+-----------+----------+------+-----------+\n",
      "|  ORD003|    CUST101|2023-01-16|250.00|electronics|\n",
      "|  ORD003|    CUST101|2023-01-16|250.00|electronics|\n",
      "|  ORD003|    CUST101|2023-01-16|250.00|electronics|\n",
      "+--------+-----------+----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "SELECT * FROM {iceberg_catalog_name}.{db_name}.{table_name}\n",
    "WHERE category = 'electronics' AND order_date = DATE '2023-01-16'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f0b873",
   "metadata": {},
   "source": [
    "To see the query plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57ec6db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Filter (order_date#291 = 2023-01-16)\n",
      "+- *(1) ColumnarToRow\n",
      "   +- BatchScan iceberg_jdbc.my_schema.spark_orders[order_id#289, customer_id#290, order_date#291, amount#292, category#293] iceberg_jdbc.my_schema.spark_orders (branch=null) [filters=category IS NOT NULL, order_date IS NOT NULL, category = 'electronics', order_date = 19373, groupedBy=] RuntimeFilters: []\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(spark.sql(f\"\"\"EXPLAIN SELECT * FROM {iceberg_catalog_name}.{db_name}.{table_name} \n",
    "          WHERE category = 'electronics' AND order_date = DATE '2023-01-16'\"\"\")\\\n",
    ".collect()[0][0])#.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7552d72d",
   "metadata": {},
   "source": [
    "### DataFrame API for Writing and Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2fa2246c-c413-44db-b1c6-0a6d138acd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType,FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09c9a12d-0214-4366-82ea-5f9848800b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"order_date\", DateType(), True),\n",
    "    StructField(\"amount\", FloatType(), True),\n",
    "    StructField(\"category\", StringType(), True)\n",
    "])\n",
    "data = [\n",
    "    ('ORD006', 'CUST104', '2023-01-19', 300.20, 'electronics'),\n",
    "    ('ORD007', 'CUST105', '2023-01-19', 22.2, 'books')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bc12709-e75d-4b60-81d1-67f7eecdb3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string dates to DateType for DataFrame creation\n",
    "from datetime import datetime\n",
    "data_typed = [(r[0], r[1], datetime.strptime(r[2], '%Y-%m-%d').date(), r[3], r[4]) for r in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6a468f5-a969-4f7f-99ed-4b4bb626d45e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ORD006', 'CUST104', datetime.date(2023, 1, 19), 300.2, 'electronics'),\n",
       " ('ORD007', 'CUST105', datetime.date(2023, 1, 19), 22.2, 'books')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_typed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6a04e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New orders DataFrame:\n",
      "+--------+-----------+----------+------+-----------+\n",
      "|order_id|customer_id|order_date|amount|   category|\n",
      "+--------+-----------+----------+------+-----------+\n",
      "|  ORD006|    CUST104|2023-01-19| 300.2|electronics|\n",
      "|  ORD007|    CUST105|2023-01-19|  22.2|      books|\n",
      "+--------+-----------+----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_orders_df = spark.createDataFrame(data_typed, schema=schema)\n",
    "print(\"\\nNew orders DataFrame:\")\n",
    "new_orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b553774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data after DataFrame append:\n",
      "+--------+-----------+----------+------+-----------+\n",
      "|order_id|customer_id|order_date|amount|   category|\n",
      "+--------+-----------+----------+------+-----------+\n",
      "|  ORD001|    CUST101|2023-01-15|100.50|electronics|\n",
      "|  ORD001|    CUST101|2023-01-15|100.50|electronics|\n",
      "|  ORD001|    CUST101|2023-01-15|100.50|electronics|\n",
      "|  ORD002|    CUST102|2023-01-16| 75.20|      books|\n",
      "|  ORD002|    CUST102|2023-01-16| 75.20|      books|\n",
      "|  ORD002|    CUST102|2023-01-16| 75.20|      books|\n",
      "|  ORD003|    CUST101|2023-01-16|250.00|electronics|\n",
      "|  ORD003|    CUST101|2023-01-16|250.00|electronics|\n",
      "|  ORD003|    CUST101|2023-01-16|250.00|electronics|\n",
      "|  ORD004|    CUST103|2023-01-17| 45.99|       home|\n",
      "|  ORD004|    CUST103|2023-01-17| 45.99|       home|\n",
      "|  ORD004|    CUST103|2023-01-17| 45.99|       home|\n",
      "|  ORD005|    CUST102|2023-01-18|120.00|      books|\n",
      "|  ORD005|    CUST102|2023-01-18|120.00|      books|\n",
      "|  ORD005|    CUST102|2023-01-18|120.00|      books|\n",
      "|  ORD006|    CUST104|2023-01-19|300.20|electronics|\n",
      "|  ORD006|    CUST104|2023-01-19|300.20|electronics|\n",
      "|  ORD007|    CUST105|2023-01-19| 22.20|      books|\n",
      "|  ORD007|    CUST105|2023-01-19| 22.20|      books|\n",
      "+--------+-----------+----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Append data using DataFrameWriter\n",
    "new_orders_df.writeTo(f\"{iceberg_catalog_name}.{db_name}.{table_name}\").append()\n",
    "\n",
    "print(f\"\\nData after DataFrame append:\")\n",
    "spark.sql(f\"\"\"SELECT * FROM {iceberg_catalog_name}.{db_name}.{table_name}\n",
    "           ORDER BY order_date, order_id\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f3fc4c",
   "metadata": {},
   "source": [
    "#### Iceberg Metadata - Snapshots, History, Manifests, Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60bb74a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Snapshots for my_schema.spark_orders:\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|        committed_at|        snapshot_id|          parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|2025-05-14 11:24:...|1518423295053039432|               NULL|   append|s3a://iceberg-war...|{spark.app.id -> ...|\n",
      "|2025-05-14 11:25:...|4748236537466741489|1518423295053039432|   append|s3a://iceberg-war...|{spark.app.id -> ...|\n",
      "|2025-05-14 11:28:...|4489780900243996563|4748236537466741489|   append|s3a://iceberg-war...|{spark.app.id -> ...|\n",
      "|2025-05-14 11:34:...|5204727257343880368|4489780900243996563|   append|s3a://iceberg-war...|{spark.app.id -> ...|\n",
      "|2025-05-14 11:37:...|1428847637346319753|5204727257343880368|   append|s3a://iceberg-war...|{spark.app.id -> ...|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSnapshots for {db_name}.{table_name}:\")\n",
    "spark.sql(f\"SELECT * FROM {iceberg_catalog_name}.{db_name}.{table_name}.snapshots\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1bac4ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Manifests for my_schema.spark_orders:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>path</th>\n",
       "      <th>length</th>\n",
       "      <th>partition_spec_id</th>\n",
       "      <th>added_snapshot_id</th>\n",
       "      <th>added_data_files_count</th>\n",
       "      <th>existing_data_files_count</th>\n",
       "      <th>deleted_data_files_count</th>\n",
       "      <th>added_delete_files_count</th>\n",
       "      <th>existing_delete_files_count</th>\n",
       "      <th>deleted_delete_files_count</th>\n",
       "      <th>partition_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://iceberg-warehouse/my_schema/spark_orders...</td>\n",
       "      <td>7829</td>\n",
       "      <td>0</td>\n",
       "      <td>1428847637346319753</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(False, False, books, electronics), (False, F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://iceberg-warehouse/my_schema/spark_orders...</td>\n",
       "      <td>7946</td>\n",
       "      <td>0</td>\n",
       "      <td>5204727257343880368</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(False, False, books, home), (False, False, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://iceberg-warehouse/my_schema/spark_orders...</td>\n",
       "      <td>7830</td>\n",
       "      <td>0</td>\n",
       "      <td>4489780900243996563</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(False, False, books, electronics), (False, F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://iceberg-warehouse/my_schema/spark_orders...</td>\n",
       "      <td>7948</td>\n",
       "      <td>0</td>\n",
       "      <td>4748236537466741489</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(False, False, books, home), (False, False, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://iceberg-warehouse/my_schema/spark_orders...</td>\n",
       "      <td>7949</td>\n",
       "      <td>0</td>\n",
       "      <td>1518423295053039432</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(False, False, books, home), (False, False, 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   content                                               path  length  \\\n",
       "0        0  s3a://iceberg-warehouse/my_schema/spark_orders...    7829   \n",
       "1        0  s3a://iceberg-warehouse/my_schema/spark_orders...    7946   \n",
       "2        0  s3a://iceberg-warehouse/my_schema/spark_orders...    7830   \n",
       "3        0  s3a://iceberg-warehouse/my_schema/spark_orders...    7948   \n",
       "4        0  s3a://iceberg-warehouse/my_schema/spark_orders...    7949   \n",
       "\n",
       "   partition_spec_id    added_snapshot_id  added_data_files_count  \\\n",
       "0                  0  1428847637346319753                       2   \n",
       "1                  0  5204727257343880368                       5   \n",
       "2                  0  4489780900243996563                       2   \n",
       "3                  0  4748236537466741489                       5   \n",
       "4                  0  1518423295053039432                       5   \n",
       "\n",
       "   existing_data_files_count  deleted_data_files_count  \\\n",
       "0                          0                         0   \n",
       "1                          0                         0   \n",
       "2                          0                         0   \n",
       "3                          0                         0   \n",
       "4                          0                         0   \n",
       "\n",
       "   added_delete_files_count  existing_delete_files_count  \\\n",
       "0                         0                            0   \n",
       "1                         0                            0   \n",
       "2                         0                            0   \n",
       "3                         0                            0   \n",
       "4                         0                            0   \n",
       "\n",
       "   deleted_delete_files_count  \\\n",
       "0                           0   \n",
       "1                           0   \n",
       "2                           0   \n",
       "3                           0   \n",
       "4                           0   \n",
       "\n",
       "                                 partition_summaries  \n",
       "0  [(False, False, books, electronics), (False, F...  \n",
       "1  [(False, False, books, home), (False, False, 2...  \n",
       "2  [(False, False, books, electronics), (False, F...  \n",
       "3  [(False, False, books, home), (False, False, 2...  \n",
       "4  [(False, False, books, home), (False, False, 2...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"\\nManifests for {db_name}.{table_name}:\")\n",
    "spark.sql(f\"SELECT * FROM {iceberg_catalog_name}.{db_name}.{table_name}.manifests\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5caf3461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Files for my_schema.spark_orders:\n",
      "+--------------------+------------+--------------------+\n",
      "|           file_path|record_count|           partition|\n",
      "+--------------------+------------+--------------------+\n",
      "|s3a://iceberg-war...|           1|{electronics, 202...|\n",
      "|s3a://iceberg-war...|           1| {books, 2023-01-19}|\n",
      "|s3a://iceberg-war...|           1| {books, 2023-01-16}|\n",
      "|s3a://iceberg-war...|           1| {books, 2023-01-18}|\n",
      "|s3a://iceberg-war...|           1|  {home, 2023-01-17}|\n",
      "|s3a://iceberg-war...|           1|{electronics, 202...|\n",
      "|s3a://iceberg-war...|           1|{electronics, 202...|\n",
      "|s3a://iceberg-war...|           1|{electronics, 202...|\n",
      "|s3a://iceberg-war...|           1| {books, 2023-01-19}|\n",
      "|s3a://iceberg-war...|           1| {books, 2023-01-16}|\n",
      "|s3a://iceberg-war...|           1| {books, 2023-01-18}|\n",
      "|s3a://iceberg-war...|           1|  {home, 2023-01-17}|\n",
      "|s3a://iceberg-war...|           1|{electronics, 202...|\n",
      "|s3a://iceberg-war...|           1|{electronics, 202...|\n",
      "|s3a://iceberg-war...|           1| {books, 2023-01-16}|\n",
      "|s3a://iceberg-war...|           1| {books, 2023-01-18}|\n",
      "|s3a://iceberg-war...|           1|  {home, 2023-01-17}|\n",
      "|s3a://iceberg-war...|           1|{electronics, 202...|\n",
      "|s3a://iceberg-war...|           1|{electronics, 202...|\n",
      "+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nData Files for {db_name}.{table_name}:\")\n",
    "spark.sql(f\"SELECT file_path, record_count, partition FROM {iceberg_catalog_name}.{db_name}.{table_name}.files\")\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fea93d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Partitions for my_schema.spark_orders:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>partition</th>\n",
       "      <th>spec_id</th>\n",
       "      <th>record_count</th>\n",
       "      <th>file_count</th>\n",
       "      <th>total_data_file_size_in_bytes</th>\n",
       "      <th>position_delete_record_count</th>\n",
       "      <th>position_delete_file_count</th>\n",
       "      <th>equality_delete_record_count</th>\n",
       "      <th>equality_delete_file_count</th>\n",
       "      <th>last_updated_at</th>\n",
       "      <th>last_updated_snapshot_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(electronics, 2023-01-19)</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3310</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-14 11:37:30.245</td>\n",
       "      <td>1428847637346319753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(books, 2023-01-19)</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3226</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-14 11:37:30.245</td>\n",
       "      <td>1428847637346319753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(books, 2023-01-16)</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-14 11:34:04.792</td>\n",
       "      <td>5204727257343880368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(books, 2023-01-18)</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4692</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-14 11:34:04.792</td>\n",
       "      <td>5204727257343880368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(home, 2023-01-17)</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4668</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-14 11:34:04.792</td>\n",
       "      <td>5204727257343880368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(electronics, 2023-01-15)</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4818</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-14 11:34:04.792</td>\n",
       "      <td>5204727257343880368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(electronics, 2023-01-16)</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4818</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-05-14 11:34:04.792</td>\n",
       "      <td>5204727257343880368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   partition  spec_id  record_count  file_count  \\\n",
       "0  (electronics, 2023-01-19)        0             2           2   \n",
       "1        (books, 2023-01-19)        0             2           2   \n",
       "2        (books, 2023-01-16)        0             3           3   \n",
       "3        (books, 2023-01-18)        0             3           3   \n",
       "4         (home, 2023-01-17)        0             3           3   \n",
       "5  (electronics, 2023-01-15)        0             3           3   \n",
       "6  (electronics, 2023-01-16)        0             3           3   \n",
       "\n",
       "   total_data_file_size_in_bytes  position_delete_record_count  \\\n",
       "0                           3310                             0   \n",
       "1                           3226                             0   \n",
       "2                           4689                             0   \n",
       "3                           4692                             0   \n",
       "4                           4668                             0   \n",
       "5                           4818                             0   \n",
       "6                           4818                             0   \n",
       "\n",
       "   position_delete_file_count  equality_delete_record_count  \\\n",
       "0                           0                             0   \n",
       "1                           0                             0   \n",
       "2                           0                             0   \n",
       "3                           0                             0   \n",
       "4                           0                             0   \n",
       "5                           0                             0   \n",
       "6                           0                             0   \n",
       "\n",
       "   equality_delete_file_count         last_updated_at  \\\n",
       "0                           0 2025-05-14 11:37:30.245   \n",
       "1                           0 2025-05-14 11:37:30.245   \n",
       "2                           0 2025-05-14 11:34:04.792   \n",
       "3                           0 2025-05-14 11:34:04.792   \n",
       "4                           0 2025-05-14 11:34:04.792   \n",
       "5                           0 2025-05-14 11:34:04.792   \n",
       "6                           0 2025-05-14 11:34:04.792   \n",
       "\n",
       "   last_updated_snapshot_id  \n",
       "0       1428847637346319753  \n",
       "1       1428847637346319753  \n",
       "2       5204727257343880368  \n",
       "3       5204727257343880368  \n",
       "4       5204727257343880368  \n",
       "5       5204727257343880368  \n",
       "6       5204727257343880368  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"\\nPartitions for {db_name}.{table_name}:\")\n",
    "# This shows how data is partitioned based on `category` and `order_date_day` (hidden transform)\n",
    "spark.sql(f\"SELECT * FROM {iceberg_catalog_name}.{db_name}.{table_name}.partitions\")\\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c20285c",
   "metadata": {},
   "source": [
    "#### Time Travel Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5b9ecd54-b0e3-4c83-b85a-f8b3ccf249af",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = spark.sql(f\"SELECT * FROM {iceberg_catalog_name}.{db_name}.{table_name}.history ORDER BY made_current_at\")\n",
    "history_list = history_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4113de05-dda7-4ba6-b200-5618cb5c3a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|          parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|2025-05-14 11:24:...|1518423295053039432|               NULL|               true|\n",
      "|2025-05-14 11:25:...|4748236537466741489|1518423295053039432|               true|\n",
      "|2025-05-14 11:28:...|4489780900243996563|4748236537466741489|               true|\n",
      "|2025-05-14 11:34:...|5204727257343880368|4489780900243996563|               true|\n",
      "|2025-05-14 11:37:...|1428847637346319753|5204727257343880368|               true|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0ee307d4-3f45-41f7-9664-332551fac110",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshots_df = spark.sql(f\"SELECT * FROM {iceberg_catalog_name}.{db_name}.{table_name}.snapshots ORDER BY committed_at\")\n",
    "snapshot_list = snapshots_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "76fa7d31-83a2-477d-9ff5-297906cf356e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Querying table state AS OF VERSION 4748236537466741489 (after initial SQL INSERT):\n",
      "+--------+-----------+----------+------+-----------+\n",
      "|order_id|customer_id|order_date|amount|   category|\n",
      "+--------+-----------+----------+------+-----------+\n",
      "|  ORD001|    CUST101|2023-01-15|100.50|electronics|\n",
      "|  ORD001|    CUST101|2023-01-15|100.50|electronics|\n",
      "|  ORD002|    CUST102|2023-01-16| 75.20|      books|\n",
      "|  ORD002|    CUST102|2023-01-16| 75.20|      books|\n",
      "|  ORD003|    CUST101|2023-01-16|250.00|electronics|\n",
      "|  ORD003|    CUST101|2023-01-16|250.00|electronics|\n",
      "|  ORD004|    CUST103|2023-01-17| 45.99|       home|\n",
      "|  ORD004|    CUST103|2023-01-17| 45.99|       home|\n",
      "|  ORD005|    CUST102|2023-01-18|120.00|      books|\n",
      "|  ORD005|    CUST102|2023-01-18|120.00|      books|\n",
      "+--------+-----------+----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if len(snapshot_list) > 1:\n",
    "    # Get the snapshot ID of the first insert operation (before the DataFrame append)\n",
    "    # Assuming the first operation was the SQL INSERT and second was DataFrame append\n",
    "    first_op_snapshot_id = history_list[0][\"snapshot_id\"] # The very first snapshot after table creation might be empty if no data was inserted then.\n",
    "                                                            # The first data snapshot is what we usually want.\n",
    "    \n",
    "    # Find the snapshot *after* the first batch of INSERTs\n",
    "    # The first row is the earliest snapshot.\n",
    "    if len(snapshot_list) >= 2 and snapshot_list[1][\"operation\"] == \"append\": # Assuming first user data insert is an append\n",
    "            target_snapshot_id = snapshot_list[1][\"snapshot_id\"] # This would be after the first SQL INSERT\n",
    "            print(f\"\\nQuerying table state AS OF VERSION {target_snapshot_id} (after initial SQL INSERT):\")\n",
    "            spark.read.option(\"snapshot-id\", target_snapshot_id)\\\n",
    "                .table(f\"{iceberg_catalog_name}.{db_name}.{table_name}\")\\\n",
    "                .orderBy(\"order_date\", \"order_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d3ff0f47-ff8a-4cf3-923c-f574889c4386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Querying table state AS OF LATEST SNAPSHOT 1428847637346319753 (current state):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_date</th>\n",
       "      <th>amount</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ORD001</td>\n",
       "      <td>CUST101</td>\n",
       "      <td>2023-01-15</td>\n",
       "      <td>100.50</td>\n",
       "      <td>electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ORD001</td>\n",
       "      <td>CUST101</td>\n",
       "      <td>2023-01-15</td>\n",
       "      <td>100.50</td>\n",
       "      <td>electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ORD001</td>\n",
       "      <td>CUST101</td>\n",
       "      <td>2023-01-15</td>\n",
       "      <td>100.50</td>\n",
       "      <td>electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ORD002</td>\n",
       "      <td>CUST102</td>\n",
       "      <td>2023-01-16</td>\n",
       "      <td>75.20</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ORD002</td>\n",
       "      <td>CUST102</td>\n",
       "      <td>2023-01-16</td>\n",
       "      <td>75.20</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ORD002</td>\n",
       "      <td>CUST102</td>\n",
       "      <td>2023-01-16</td>\n",
       "      <td>75.20</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ORD003</td>\n",
       "      <td>CUST101</td>\n",
       "      <td>2023-01-16</td>\n",
       "      <td>250.00</td>\n",
       "      <td>electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ORD003</td>\n",
       "      <td>CUST101</td>\n",
       "      <td>2023-01-16</td>\n",
       "      <td>250.00</td>\n",
       "      <td>electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ORD003</td>\n",
       "      <td>CUST101</td>\n",
       "      <td>2023-01-16</td>\n",
       "      <td>250.00</td>\n",
       "      <td>electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ORD004</td>\n",
       "      <td>CUST103</td>\n",
       "      <td>2023-01-17</td>\n",
       "      <td>45.99</td>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ORD004</td>\n",
       "      <td>CUST103</td>\n",
       "      <td>2023-01-17</td>\n",
       "      <td>45.99</td>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ORD004</td>\n",
       "      <td>CUST103</td>\n",
       "      <td>2023-01-17</td>\n",
       "      <td>45.99</td>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ORD005</td>\n",
       "      <td>CUST102</td>\n",
       "      <td>2023-01-18</td>\n",
       "      <td>120.00</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ORD005</td>\n",
       "      <td>CUST102</td>\n",
       "      <td>2023-01-18</td>\n",
       "      <td>120.00</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ORD005</td>\n",
       "      <td>CUST102</td>\n",
       "      <td>2023-01-18</td>\n",
       "      <td>120.00</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ORD006</td>\n",
       "      <td>CUST104</td>\n",
       "      <td>2023-01-19</td>\n",
       "      <td>300.20</td>\n",
       "      <td>electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ORD006</td>\n",
       "      <td>CUST104</td>\n",
       "      <td>2023-01-19</td>\n",
       "      <td>300.20</td>\n",
       "      <td>electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ORD007</td>\n",
       "      <td>CUST105</td>\n",
       "      <td>2023-01-19</td>\n",
       "      <td>22.20</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ORD007</td>\n",
       "      <td>CUST105</td>\n",
       "      <td>2023-01-19</td>\n",
       "      <td>22.20</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   order_id customer_id  order_date  amount     category\n",
       "0    ORD001     CUST101  2023-01-15  100.50  electronics\n",
       "1    ORD001     CUST101  2023-01-15  100.50  electronics\n",
       "2    ORD001     CUST101  2023-01-15  100.50  electronics\n",
       "3    ORD002     CUST102  2023-01-16   75.20        books\n",
       "4    ORD002     CUST102  2023-01-16   75.20        books\n",
       "5    ORD002     CUST102  2023-01-16   75.20        books\n",
       "6    ORD003     CUST101  2023-01-16  250.00  electronics\n",
       "7    ORD003     CUST101  2023-01-16  250.00  electronics\n",
       "8    ORD003     CUST101  2023-01-16  250.00  electronics\n",
       "9    ORD004     CUST103  2023-01-17   45.99         home\n",
       "10   ORD004     CUST103  2023-01-17   45.99         home\n",
       "11   ORD004     CUST103  2023-01-17   45.99         home\n",
       "12   ORD005     CUST102  2023-01-18  120.00        books\n",
       "13   ORD005     CUST102  2023-01-18  120.00        books\n",
       "14   ORD005     CUST102  2023-01-18  120.00        books\n",
       "15   ORD006     CUST104  2023-01-19  300.20  electronics\n",
       "16   ORD006     CUST104  2023-01-19  300.20  electronics\n",
       "17   ORD007     CUST105  2023-01-19   22.20        books\n",
       "18   ORD007     CUST105  2023-01-19   22.20        books"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_snapshot_id = snapshot_list[-1][\"snapshot_id\"]\n",
    "print(f\"\\nQuerying table state AS OF LATEST SNAPSHOT {latest_snapshot_id} (current state):\")\n",
    "spark.read.table(f\"{iceberg_catalog_name}.{db_name}.{table_name}\").orderBy(\"order_date\", \"order_id\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "946ff22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Querying table state AS OF TIMESTAMP '2025-05-14 11:34:04.792000':\n",
      "+--------+-----------+----------+------+-----------+\n",
      "|order_id|customer_id|order_date|amount|   category|\n",
      "+--------+-----------+----------+------+-----------+\n",
      "|  ORD001|    CUST101|2023-01-15|100.50|electronics|\n",
      "|  ORD001|    CUST101|2023-01-15|100.50|electronics|\n",
      "|  ORD001|    CUST101|2023-01-15|100.50|electronics|\n",
      "|  ORD002|    CUST102|2023-01-16| 75.20|      books|\n",
      "|  ORD002|    CUST102|2023-01-16| 75.20|      books|\n",
      "|  ORD002|    CUST102|2023-01-16| 75.20|      books|\n",
      "|  ORD003|    CUST101|2023-01-16|250.00|electronics|\n",
      "|  ORD003|    CUST101|2023-01-16|250.00|electronics|\n",
      "|  ORD003|    CUST101|2023-01-16|250.00|electronics|\n",
      "|  ORD004|    CUST103|2023-01-17| 45.99|       home|\n",
      "|  ORD004|    CUST103|2023-01-17| 45.99|       home|\n",
      "|  ORD004|    CUST103|2023-01-17| 45.99|       home|\n",
      "|  ORD005|    CUST102|2023-01-18|120.00|      books|\n",
      "|  ORD005|    CUST102|2023-01-18|120.00|      books|\n",
      "|  ORD005|    CUST102|2023-01-18|120.00|      books|\n",
      "|  ORD006|    CUST104|2023-01-19|300.20|electronics|\n",
      "|  ORD007|    CUST105|2023-01-19| 22.20|      books|\n",
      "+--------+-----------+----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Time travel using timestamp (requires made_current_at timestamp)\n",
    "if len(snapshot_list) >= 2:\n",
    "    timestamp_before_last_append = history_list[-2][\"made_current_at\"] # Timestamp of the snapshot before the last one\n",
    "    print(f\"\\nQuerying table state AS OF TIMESTAMP '{timestamp_before_last_append}':\")\n",
    "    spark.read.option(\"as-of-timestamp\", str(int(timestamp_before_last_append.timestamp() * 1000))) \\\n",
    "         .table(f\"{iceberg_catalog_name}.{db_name}.{table_name}\") \\\n",
    "         .orderBy(\"order_date\", \"order_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46367fe0",
   "metadata": {},
   "source": [
    "Schema Evolution (Example: Add a new column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bf4e3961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Schema before evolution:\n",
      "+--------------+----------------+-------+\n",
      "|      col_name|       data_type|comment|\n",
      "+--------------+----------------+-------+\n",
      "|      order_id|          string|   NULL|\n",
      "|   customer_id|          string|   NULL|\n",
      "|    order_date|            date|   NULL|\n",
      "|        amount|   decimal(10,2)|   NULL|\n",
      "|      category|          string|   NULL|\n",
      "|              |                |       |\n",
      "|# Partitioning|                |       |\n",
      "|        Part 0|        category|       |\n",
      "|        Part 1|days(order_date)|       |\n",
      "+--------------+----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSchema before evolution:\")\n",
    "spark.sql(f\"DESCRIBE {iceberg_catalog_name}.{db_name}.{table_name}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1cb25ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Schema after adding 'is_returned' column:\n",
      "+--------------+----------------+-------+\n",
      "|      col_name|       data_type|comment|\n",
      "+--------------+----------------+-------+\n",
      "|      order_id|          string|   NULL|\n",
      "|   customer_id|          string|   NULL|\n",
      "|    order_date|            date|   NULL|\n",
      "|        amount|   decimal(10,2)|   NULL|\n",
      "|      category|          string|   NULL|\n",
      "|   is_returned|         boolean|   NULL|\n",
      "|              |                |       |\n",
      "|# Partitioning|                |       |\n",
      "|        Part 0|        category|       |\n",
      "|        Part 1|days(order_date)|       |\n",
      "+--------------+----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new column 'is_returned'\n",
    "spark.sql(f\"ALTER TABLE {iceberg_catalog_name}.{db_name}.{table_name} ADD COLUMN is_returned BOOLEAN\")\n",
    "print(f\"\\nSchema after adding 'is_returned' column:\")\n",
    "spark.sql(f\"DESCRIBE {iceberg_catalog_name}.{db_name}.{table_name}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1209b70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data after inserting with new column (old rows will have null for 'is_returned'):\n",
      "+--------+-----------+----------+------+-----------+\n",
      "|order_id|   category|order_date|amount|is_returned|\n",
      "+--------+-----------+----------+------+-----------+\n",
      "|  ORD001|electronics|2023-01-15|100.50|       NULL|\n",
      "|  ORD001|electronics|2023-01-15|100.50|       NULL|\n",
      "|  ORD001|electronics|2023-01-15|100.50|       NULL|\n",
      "|  ORD002|      books|2023-01-16| 75.20|       NULL|\n",
      "|  ORD002|      books|2023-01-16| 75.20|       NULL|\n",
      "|  ORD002|      books|2023-01-16| 75.20|       NULL|\n",
      "|  ORD003|electronics|2023-01-16|250.00|       NULL|\n",
      "|  ORD003|electronics|2023-01-16|250.00|       NULL|\n",
      "|  ORD003|electronics|2023-01-16|250.00|       NULL|\n",
      "|  ORD004|       home|2023-01-17| 45.99|       NULL|\n",
      "|  ORD004|       home|2023-01-17| 45.99|       NULL|\n",
      "|  ORD004|       home|2023-01-17| 45.99|       NULL|\n",
      "|  ORD005|      books|2023-01-18|120.00|       NULL|\n",
      "|  ORD005|      books|2023-01-18|120.00|       NULL|\n",
      "|  ORD005|      books|2023-01-18|120.00|       NULL|\n",
      "|  ORD006|electronics|2023-01-19|300.20|       NULL|\n",
      "|  ORD006|electronics|2023-01-19|300.20|       NULL|\n",
      "|  ORD007|      books|2023-01-19| 22.20|       NULL|\n",
      "|  ORD007|      books|2023-01-19| 22.20|       NULL|\n",
      "|  ORD008|       home|2023-01-20| 55.00|       true|\n",
      "+--------+-----------+----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Insert data with the new column\n",
    "spark.sql(f\"\"\"\n",
    "INSERT INTO {iceberg_catalog_name}.{db_name}.{table_name}\n",
    "VALUES ('ORD008', 'CUST101', DATE '2023-01-20', 55.00, 'home', true)\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\nData after inserting with new column (old rows will have null for 'is_returned'):\")\n",
    "spark.sql(f\"\"\"SELECT order_id, category, order_date, amount, is_returned \n",
    "          FROM {iceberg_catalog_name}.{db_name}.{table_name} \n",
    "          ORDER BY order_date, order_id\"\"\")\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aacb63",
   "metadata": {},
   "source": [
    "\n",
    "Data Compaction (Rewrite Data Files - Small File Compaction)\n",
    "- Iceberg procedures are called using CALL\n",
    "- Insert some more data to potentially create small files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "586d3a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Files before compaction for my_schema.spark_orders (partition: category=books):\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------+\n",
      "|file_path                                                                                                                                                 |record_count|file_size_in_bytes|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------+\n",
      "|s3a://iceberg-warehouse/my_schema/spark_orders/data/category=books/order_date_day=2023-01-21/00000-79-4e53f015-5157-4256-9f88-3d64c3ff9e86-0-00001.parquet|1           |1799              |\n",
      "|s3a://iceberg-warehouse/my_schema/spark_orders/data/category=books/order_date_day=2023-01-21/00000-77-439017ff-e0bc-43c1-83bb-ea2b2eb9e833-0-00001.parquet|1           |1798              |\n",
      "|s3a://iceberg-warehouse/my_schema/spark_orders/data/category=books/order_date_day=2023-01-21/00000-75-f353e32d-c2c9-469a-8b1f-d6b4fdc5cfb6-0-00001.parquet|1           |1798              |\n",
      "|s3a://iceberg-warehouse/my_schema/spark_orders/data/category=books/order_date_day=2023-01-19/00000-18-0cacdff4-019c-41a9-b903-392918f63236-0-00002.parquet|1           |1613              |\n",
      "|s3a://iceberg-warehouse/my_schema/spark_orders/data/category=books/order_date_day=2023-01-16/00000-2-6b3bf954-040a-4bea-908a-a4c204b8b95e-0-00002.parquet |1           |1563              |\n",
      "|s3a://iceberg-warehouse/my_schema/spark_orders/data/category=books/order_date_day=2023-01-18/00000-2-6b3bf954-040a-4bea-908a-a4c204b8b95e-0-00005.parquet |1           |1564              |\n",
      "|s3a://iceberg-warehouse/my_schema/spark_orders/data/category=books/order_date_day=2023-01-19/00000-18-2f9fba60-48fe-4e5f-a537-35ab49f73644-0-00002.parquet|1           |1613              |\n",
      "|s3a://iceberg-warehouse/my_schema/spark_orders/data/category=books/order_date_day=2023-01-16/00000-5-515cebea-b861-4165-b9f5-adb4e9bc6c56-0-00002.parquet |1           |1563              |\n",
      "|s3a://iceberg-warehouse/my_schema/spark_orders/data/category=books/order_date_day=2023-01-18/00000-5-515cebea-b861-4165-b9f5-adb4e9bc6c56-0-00005.parquet |1           |1564              |\n",
      "|s3a://iceberg-warehouse/my_schema/spark_orders/data/category=books/order_date_day=2023-01-16/00000-2-7e623a95-b642-499c-91a6-0f79edee5280-0-00002.parquet |1           |1563              |\n",
      "|s3a://iceberg-warehouse/my_schema/spark_orders/data/category=books/order_date_day=2023-01-18/00000-2-7e623a95-b642-499c-91a6-0f79edee5280-0-00005.parquet |1           |1564              |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"INSERT INTO {iceberg_catalog_name}.{db_name}.{table_name} \n",
    "          VALUES ('ORD009', 'CUST106', DATE '2023-01-21', 10.00, 'books', false)\"\"\")\n",
    "spark.sql(f\"\"\"INSERT INTO {iceberg_catalog_name}.{db_name}.{table_name}\n",
    "          VALUES ('ORD010', 'CUST106', DATE '2023-01-21', 12.00, 'books', false)\"\"\")\n",
    "\n",
    "print(f\"\\nData Files before compaction for {db_name}.{table_name} (partition: category=books):\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT file_path, record_count, file_size_in_bytes\n",
    "    FROM {iceberg_catalog_name}.{db_name}.{table_name}.files\n",
    "    WHERE partition.category = 'books'\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343f2773",
   "metadata": {},
   "source": [
    "Execute rewrite_data_files procedure for compaction\n",
    "- This will compact small files into larger ones, default strategy is 'sort' which also sorts data within files\n",
    "- For very small tables, the effect might be limited or group all into one file per partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ffee40b4-2b3b-45cd-94ac-9e233ca6c69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running data compaction (rewrite_data_files) for table my_schema.spark_orders...\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "|rewritten_data_files_count|added_data_files_count|rewritten_bytes_count|failed_data_files_count|\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "|                        22|                     8|                35616|                      0|\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "\n",
      "Compaction procedure finished.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nRunning data compaction (rewrite_data_files) for table {db_name}.{table_name}...\")\n",
    "# You can specify sorting options or a where clause to limit compaction scope\n",
    "result_df = spark.sql(f\"\"\"CALL {iceberg_catalog_name}.system.rewrite_data_files(\n",
    "                      table => '{db_name}.{table_name}', \n",
    "                      strategy => 'sort', sort_order => 'order_date ASC, amount DESC', \n",
    "                      options => map('min-input-files','1'))\"\"\")\n",
    "# options => map('min-input-files','1') to force compaction even with few files for demo\n",
    "result_df.show()\n",
    "print(\"Compaction procedure finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0775d57c-2867-4c5d-b4d7-6fec67fe642a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Files after compaction for my_schema.spark_orders (partition: category=books):\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------+\n",
      "|file_path                                                                                                                                                 |record_count|file_size_in_bytes|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------+\n",
      "|s3a://iceberg-warehouse/my_schema/spark_orders/data/category=books/order_date_day=2023-01-19/00000-96-40d32e29-a242-4246-9b55-f3da68637cc8-0-00001.parquet|2           |2065              |\n",
      "|s3a://iceberg-warehouse/my_schema/spark_orders/data/category=books/order_date_day=2023-01-18/00000-92-dae44c0f-791e-4dcb-b422-c65da4ab80e1-0-00001.parquet|3           |2065              |\n",
      "|s3a://iceberg-warehouse/my_schema/spark_orders/data/category=books/order_date_day=2023-01-21/00000-95-c395d9cd-159b-4ae7-b454-845a2dfe9bab-0-00001.parquet|3           |2066              |\n",
      "|s3a://iceberg-warehouse/my_schema/spark_orders/data/category=books/order_date_day=2023-01-16/00000-86-33b5e62e-25e3-40cd-8d62-7052242cb44f-0-00001.parquet|3           |2064              |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nData Files after compaction for {db_name}.{table_name} (partition: category=books):\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT file_path, record_count, file_size_in_bytes\n",
    "    FROM {iceberg_catalog_name}.{db_name}.{table_name}.files\n",
    "    WHERE partition.category = 'books'\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b474fde1-6a4f-4a4d-8898-3d20190ce84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Snapshots after compaction:\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|        committed_at|        snapshot_id|          parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|2025-05-14 11:46:...|6517193885541184603|7894115058018408800|  replace|s3a://iceberg-war...|{added-data-files...|\n",
      "|2025-05-14 11:46:...|7894115058018408800|3235839622240604194|   append|s3a://iceberg-war...|{spark.app.id -> ...|\n",
      "|2025-05-14 11:46:...|3235839622240604194|4314309291397521213|   append|s3a://iceberg-war...|{spark.app.id -> ...|\n",
      "|2025-05-14 11:45:...|4314309291397521213|6075184588584117234|   append|s3a://iceberg-war...|{spark.app.id -> ...|\n",
      "|2025-05-14 11:45:...|6075184588584117234|1428847637346319753|   append|s3a://iceberg-war...|{spark.app.id -> ...|\n",
      "|2025-05-14 11:37:...|1428847637346319753|5204727257343880368|   append|s3a://iceberg-war...|{spark.app.id -> ...|\n",
      "|2025-05-14 11:34:...|5204727257343880368|4489780900243996563|   append|s3a://iceberg-war...|{spark.app.id -> ...|\n",
      "|2025-05-14 11:28:...|4489780900243996563|4748236537466741489|   append|s3a://iceberg-war...|{spark.app.id -> ...|\n",
      "|2025-05-14 11:25:...|4748236537466741489|1518423295053039432|   append|s3a://iceberg-war...|{spark.app.id -> ...|\n",
      "|2025-05-14 11:24:...|1518423295053039432|               NULL|   append|s3a://iceberg-war...|{spark.app.id -> ...|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSnapshots after compaction:\")\n",
    "spark.sql(f\"SELECT * FROM {iceberg_catalog_name}.{db_name}.{table_name}.snapshots ORDER BY committed_at DESC\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a4dff9",
   "metadata": {},
   "source": [
    "Filter Pushdown Demonstration\n",
    "- The table is partitioned by (category, days(order_date))\n",
    "- Spark should be able to push down filters on `category` and `order_date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b3d455be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query with partition column filters:\n",
      "+--------+-----------+----------+------+-----------+-----------+\n",
      "|order_id|customer_id|order_date|amount|   category|is_returned|\n",
      "+--------+-----------+----------+------+-----------+-----------+\n",
      "|  ORD003|    CUST101|2023-01-16|250.00|electronics|       NULL|\n",
      "|  ORD003|    CUST101|2023-01-16|250.00|electronics|       NULL|\n",
      "|  ORD003|    CUST101|2023-01-16|250.00|electronics|       NULL|\n",
      "|  ORD006|    CUST104|2023-01-19|300.20|electronics|       NULL|\n",
      "|  ORD006|    CUST104|2023-01-19|300.20|electronics|       NULL|\n",
      "|  ORD001|    CUST101|2023-01-15|100.50|electronics|       NULL|\n",
      "|  ORD001|    CUST101|2023-01-15|100.50|electronics|       NULL|\n",
      "|  ORD001|    CUST101|2023-01-15|100.50|electronics|       NULL|\n",
      "+--------+-----------+----------+------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_with_filters = f\"\"\"\n",
    "SELECT * FROM {iceberg_catalog_name}.{db_name}.{table_name}\n",
    "WHERE category = 'electronics' AND order_date >= DATE '2023-01-15' AND order_date < DATE '2023-01-20'\n",
    "\"\"\"\n",
    "print(\"\\nQuery with partition column filters:\")\n",
    "spark.sql(query_with_filters).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5dab18f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXPLAIN plan for the query (look for PushedFilters in ParquetScan or IcebergScan):\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- BatchScan iceberg_jdbc.my_schema.spark_orders[order_id#1854, customer_id#1855, order_date#1856, amount#1857, category#1858, is_returned#1859] iceberg_jdbc.my_schema.spark_orders (branch=null) [filters=category IS NOT NULL, order_date IS NOT NULL, category = 'electronics', order_date >= 19372, order_date < 19377, groupedBy=] RuntimeFilters: []\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEXPLAIN plan for the query (look for PushedFilters in ParquetScan or IcebergScan):\")\n",
    "print(spark.sql(f\"EXPLAIN {query_with_filters}\").collect()[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c463b1",
   "metadata": {},
   "source": [
    "Interoperability Check - Read data created by Trino (if my_schema.employees exists)\n",
    " - Ensure the Trino notebook (01-trino-iceberg-getting-started.ipynb) has been run to create this table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "812399bf-91ca-49c2-b1d0-b068ee865a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.catalog.iceberg_catalog\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "spark.conf.set(\"spark.sql.catalog.iceberg_catalog.catalog-impl\", \"org.apache.iceberg.jdbc.JdbcCatalog\") \n",
    "spark.conf.set(\"spark.sql.catalog.iceberg_catalog.uri\", \"jdbc:postgresql://postgres_catalog:5432/iceberg_catalog\")\n",
    "spark.conf.set(\"spark.sql.catalog.iceberg_catalog.jdbc.user\", \"iceberg\") \n",
    "spark.conf.set(\"spark.sql.catalog.iceberg_catalog.jdbc.password\", \"icebergpassword\") \n",
    "spark.conf.set(\"spark.sql.catalog.iceberg_catalog.warehouse\", \"s3a://iceberg-warehouse/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ee330cec-8009-4c65-8d37-3ec6ce00344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentCatalog('iceberg_catalog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e5ceabc4-abcf-49b0-a6e2-916dd635bcc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CatalogMetadata(name='default_cache_iceberg', description=None),\n",
       " CatalogMetadata(name='iceberg_catalog', description=None),\n",
       " CatalogMetadata(name='iceberg_jdbc', description=None),\n",
       " CatalogMetadata(name='spark_catalog', description=None)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listCatalogs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1565180c-98f2-47e9-8706-3d4a7bfca621",
   "metadata": {},
   "outputs": [],
   "source": [
    "iceberg_trino_catalog_name='iceberg_catalog'\n",
    "trino_table_name = \"my_schema.employees\" # From Trino notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3e138b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempting to read Trino-created table: iceberg_catalog.my_schema.employees\n",
      "+--------------------+-------------+-------+\n",
      "|            col_name|    data_type|comment|\n",
      "+--------------------+-------------+-------+\n",
      "|                  id|          int|   NULL|\n",
      "|                name|       string|   NULL|\n",
      "|          department|       string|   NULL|\n",
      "|              salary|decimal(10,2)|   NULL|\n",
      "|           hire_date|         date|   NULL|\n",
      "|# Partition Infor...|             |       |\n",
      "|          # col_name|    data_type|comment|\n",
      "|          department|       string|   NULL|\n",
      "+--------------------+-------------+-------+\n",
      "\n",
      "+---+-------------+-----------+--------+----------+\n",
      "| id|         name| department|  salary| hire_date|\n",
      "+---+-------------+-----------+--------+----------+\n",
      "|  7|George Yellow|         HR|65000.00|2023-03-15|\n",
      "|  3|Charlie Brown|         HR|70000.00|2021-03-10|\n",
      "|  7|George Yellow|         HR|65000.00|2023-03-15|\n",
      "|  1|  Alice Smith|Engineering|90000.00|2020-01-15|\n",
      "|  2|  Bob Johnson|Engineering|85000.00|2019-07-01|\n",
      "+---+-------------+-----------+--------+----------+\n",
      "\n",
      "Successfully read Trino-created table with Spark.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nAttempting to read Trino-created table: {iceberg_trino_catalog_name}.{trino_table_name}\")\n",
    "# Spark needs to know the schema. If Trino created it, Spark should discover it via the JDBC catalog.\n",
    "spark.sql(f\"DESCRIBE {iceberg_trino_catalog_name}.{trino_table_name}\").show()\n",
    "spark.sql(f\"SELECT * FROM {iceberg_trino_catalog_name}.{trino_table_name} LIMIT 5\").show()\n",
    "print(\"Successfully read Trino-created table with Spark.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "15ffebaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spark Iceberg Datalakehouse Demo (Phase 2) completed.\n"
     ]
    }
   ],
   "source": [
    "# spark.stop() # Commented out to allow re-running cells easily\n",
    "print(\"\\nSpark Iceberg Datalakehouse Demo (Phase 2) completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
