{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b213c576",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Import PySpark and Create SparkSession\n",
    " - The SparkSession should be automatically configured by PYSPARK_SUBMIT_ARGS\n",
    " - defined in docker-compose.yml for the jupyterlab service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cff1942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "061b3ac2-a387-4520-855a-f44c5fc0880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0258b9a1-f759-4053-b178-485ae1c6e868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.6'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6eec9d",
   "metadata": {},
   "source": [
    "### Create SparkSession, manually set configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4590259-a528-4205-9727-f2fc90c65bb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "org.postgresql#postgresql added as a dependency\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-5935c306-9df2-463d-849d-ef98cbd507a0;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.9.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound org.postgresql#postgresql;42.6.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.5.6 in central\n",
      "\tfound org.tukaani#xz;1.9 in central\n",
      ":: resolution report :: resolve 132ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.9.1 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.5.6 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.6.0 from central in [default]\n",
      "\torg.tukaani#xz;1.9 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   8   |   0   |   0   |   0   ||   8   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-5935c306-9df2-463d-849d-ef98cbd507a0\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 8 already retrieved (0kB/3ms)\n",
      "25/07/10 10:39:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "    .config(\"spark.sql.catalog.iceberg_jdbc\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.iceberg_jdbc.uri\", \"jdbc:postgresql://postgres:5432/iceberg_catalog\")\n",
    "    .config(\"spark.sql.catalog.iceberg_jdbc.jdbc.user\", \"iceberg\")\n",
    "    .config(\"spark.sql.catalog.iceberg_jdbc.jdbc.password\", \"icebergpassword\")\n",
    "    .config(\"spark.sql.catalog.iceberg_jdbc.warehouse\", \"s3a://iceberg-warehouse/\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint.region\",\"eu-central-1\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"password\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    ).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbdfdd18-3c94-4282-a9f8-9abc7522c260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://8732fe22959e:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff9dfbad20>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6acc9ef7-db57-4c79-8458-c0a6f3ef3b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession created successfully!\n",
      "Spark version: 3.5.6\n",
      "+-------------+---------+\n",
      "|      catalog|namespace|\n",
      "+-------------+---------+\n",
      "|spark_catalog|  default|\n",
      "+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"SparkSession created successfully!\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "# Verify Iceberg catalog configuration - Should show default once catalog is used\n",
    "spark.sql(\"SHOW CURRENT NAMESPACE\").show()\n",
    "# Define the catalog name we configured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83526689-6b62-4a17-ae06-a09f08cf951e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1437e6be-5147-4cee-bf1a-3dc67eb777c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "iceberg_catalog_name = \"iceberg_jdbc\" # Must match spark.sql.catalog.iceberg_jdbc in config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc03009-bfe5-451b-86a1-15f46c5c2952",
   "metadata": {},
   "source": [
    "Create a Database/Schema in Iceberg using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e6eaf67-ef48-49a7-adf0-1e846eb94774",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "db_name = \"spark_schema\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0ba14ef-c6ae-414b-9e38-39e5a7f83196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+------------+\n",
      "|   namespace|\n",
      "+------------+\n",
      "|spark_schema|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {iceberg_catalog_name}.{db_name}\").show()\n",
    "spark.sql(f\"USE {iceberg_catalog_name}.{db_name}\")\n",
    "spark.sql(f\"SHOW DATABASES IN {iceberg_catalog_name}\").show()\n",
    "# In Spark, `DATABASE` and `SCHEMA` are often used interchangeably.\n",
    "# Iceberg uses `NAMESPACE`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ad7709-3ca0-4343-bd2d-078ce956c556",
   "metadata": {},
   "source": [
    " Create an Iceberg Table with Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a6d20fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/10 10:39:30 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_name = \"spark_orders\"\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {iceberg_catalog_name}.{db_name}.{table_name} (\n",
    "    order_id STRING,\n",
    "    customer_id STRING,\n",
    "    order_date DATE,\n",
    "    amount DECIMAL(10, 2),\n",
    "    category STRING\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (category, days(order_date)) -- Column partitioning and hidden partitioning by day\n",
    "TBLPROPERTIES (\n",
    "    'write.format.default'='parquet',\n",
    "    'format-version'='2'\n",
    ")\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "699a65fa-1838-4e64-afc6-4b4541e0a93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----------+\n",
      "|   namespace|   tableName|isTemporary|\n",
      "+------------+------------+-----------+\n",
      "|spark_schema|spark_orders|      false|\n",
      "+------------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"SHOW TABLES IN {iceberg_catalog_name}.{db_name}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b73464b-98b6-47df-9314-6cedcd8228e3",
   "metadata": {},
   "source": [
    "Insert Data using Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b8ba79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted into spark_schema.spark_orders\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "INSERT INTO {iceberg_catalog_name}.{db_name}.{table_name} VALUES\n",
    "('ORD001', 'CUST101', DATE '2023-01-15', 100.50, 'electronics'),\n",
    "('ORD002', 'CUST102', DATE '2023-01-16', 75.20, 'books'),\n",
    "('ORD003', 'CUST101', DATE '2023-01-16', 250.00, 'electronics'),\n",
    "('ORD004', 'CUST103', DATE '2023-01-17', 45.99, 'home'),\n",
    "('ORD005', 'CUST102', DATE '2023-01-18', 120.00, 'books')\n",
    "\"\"\")\n",
    "print(f\"Data inserted into {db_name}.{table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f110f3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying all data from spark_schema.spark_orders:\n",
      "+--------+-----------+----------+------+-----------+\n",
      "|order_id|customer_id|order_date|amount|   category|\n",
      "+--------+-----------+----------+------+-----------+\n",
      "|  ORD001|    CUST101|2023-01-15|100.50|electronics|\n",
      "|  ORD003|    CUST101|2023-01-16|250.00|electronics|\n",
      "|  ORD002|    CUST102|2023-01-16| 75.20|      books|\n",
      "|  ORD004|    CUST103|2023-01-17| 45.99|       home|\n",
      "|  ORD005|    CUST102|2023-01-18|120.00|      books|\n",
      "+--------+-----------+----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Querying all data from {db_name}.{table_name}:\")\n",
    "spark.sql(f\"SELECT * FROM {iceberg_catalog_name}.{db_name}.{table_name} ORDER BY order_date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c0a931",
   "metadata": {},
   "source": [
    "Select Data using Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbc75e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying all data from spark_schema.spark_orders:\n",
      "+--------+-----------+----------+------+-----------+\n",
      "|order_id|customer_id|order_date|amount|   category|\n",
      "+--------+-----------+----------+------+-----------+\n",
      "|  ORD001|    CUST101|2023-01-15|100.50|electronics|\n",
      "|  ORD003|    CUST101|2023-01-16|250.00|electronics|\n",
      "|  ORD002|    CUST102|2023-01-16| 75.20|      books|\n",
      "|  ORD004|    CUST103|2023-01-17| 45.99|       home|\n",
      "|  ORD005|    CUST102|2023-01-18|120.00|      books|\n",
      "+--------+-----------+----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Querying all data from {db_name}.{table_name}:\")\n",
    "spark.sql(f\"SELECT * FROM {iceberg_catalog_name}.{db_name}.{table_name} ORDER BY order_date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6734d8",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Querying electronics orders (demonstrating partition filter pushdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72d94ace-b12e-4018-aa57-f7be7b839739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------+-----------+\n",
      "|order_id|customer_id|order_date|amount|   category|\n",
      "+--------+-----------+----------+------+-----------+\n",
      "|  ORD003|    CUST101|2023-01-16|250.00|electronics|\n",
      "+--------+-----------+----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "SELECT * FROM {iceberg_catalog_name}.{db_name}.{table_name}\n",
    "WHERE category = 'electronics' AND order_date = DATE '2023-01-16'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfcdce6",
   "metadata": {},
   "source": [
    "To see the query plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "684ac83e-062b-4c53-8bd0-8b379a258d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Filter (order_date#206 = 2023-01-16)\n",
      "+- *(1) ColumnarToRow\n",
      "   +- BatchScan iceberg_jdbc.spark_schema.spark_orders[order_id#204, customer_id#205, order_date#206, amount#207, category#208] iceberg_jdbc.spark_schema.spark_orders (branch=null) [filters=category IS NOT NULL, order_date IS NOT NULL, category = 'electronics', order_date = 19373, groupedBy=] RuntimeFilters: []\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(spark.sql(f\"\"\"EXPLAIN SELECT * FROM {iceberg_catalog_name}.{db_name}.{table_name} \n",
    "          WHERE category = 'electronics' AND order_date = DATE '2023-01-16'\"\"\")\\\n",
    ".collect()[0][0])#.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0543deef-0be1-4c78-a5c8-bdf55b3f87a6",
   "metadata": {},
   "source": [
    "### DataFrame API for Writing and Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f20b073-bc3b-4262-b0f2-484c5b103eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType,FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17f0b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"order_date\", DateType(), True),\n",
    "    StructField(\"amount\", FloatType(), True),\n",
    "    StructField(\"category\", StringType(), True)\n",
    "])\n",
    "data = [\n",
    "    ('ORD006', 'CUST104', '2023-01-19', 300.20, 'electronics'),\n",
    "    ('ORD007', 'CUST105', '2023-01-19', 22.2, 'books')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57ec6db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string dates to DateType for DataFrame creation\n",
    "from datetime import datetime\n",
    "data_typed = [(r[0], r[1], datetime.strptime(r[2], '%Y-%m-%d').date(), r[3], r[4]) for r in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7552d72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ORD006', 'CUST104', datetime.date(2023, 1, 19), 300.2, 'electronics'),\n",
       " ('ORD007', 'CUST105', datetime.date(2023, 1, 19), 22.2, 'books')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_typed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fa2246c-c413-44db-b1c6-0a6d138acd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New orders DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------+-----------+\n",
      "|order_id|customer_id|order_date|amount|   category|\n",
      "+--------+-----------+----------+------+-----------+\n",
      "|  ORD006|    CUST104|2023-01-19| 300.2|electronics|\n",
      "|  ORD007|    CUST105|2023-01-19|  22.2|      books|\n",
      "+--------+-----------+----------+------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "new_orders_df = spark.createDataFrame(data_typed, schema=schema)\n",
    "print(\"\\nNew orders DataFrame:\")\n",
    "new_orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09c9a12d-0214-4366-82ea-5f9848800b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data after DataFrame append:\n",
      "+--------+-----------+----------+------+-----------+\n",
      "|order_id|customer_id|order_date|amount|   category|\n",
      "+--------+-----------+----------+------+-----------+\n",
      "|  ORD001|    CUST101|2023-01-15|100.50|electronics|\n",
      "|  ORD002|    CUST102|2023-01-16| 75.20|      books|\n",
      "|  ORD003|    CUST101|2023-01-16|250.00|electronics|\n",
      "|  ORD004|    CUST103|2023-01-17| 45.99|       home|\n",
      "|  ORD005|    CUST102|2023-01-18|120.00|      books|\n",
      "|  ORD006|    CUST104|2023-01-19|300.20|electronics|\n",
      "|  ORD007|    CUST105|2023-01-19| 22.20|      books|\n",
      "+--------+-----------+----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Append data using DataFrameWriter\n",
    "new_orders_df.writeTo(f\"{iceberg_catalog_name}.{db_name}.{table_name}\").append()\n",
    "\n",
    "print(f\"\\nData after DataFrame append:\")\n",
    "spark.sql(f\"\"\"SELECT * FROM {iceberg_catalog_name}.{db_name}.{table_name}\n",
    "           ORDER BY order_date, order_id\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc12709-e75d-4b60-81d1-67f7eecdb3da",
   "metadata": {},
   "source": [
    "#### Iceberg Metadata - Snapshots, History, Manifests, Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6a468f5-a969-4f7f-99ed-4b4bb626d45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Snapshots for spark_schema.spark_orders:\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|        committed_at|        snapshot_id|          parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|2025-07-10 10:39:...|7702743561916277481|               NULL|   append|s3a://iceberg-war...|{spark.app.id -> ...|\n",
      "|2025-07-10 10:39:...|2531062654959842889|7702743561916277481|   append|s3a://iceberg-war...|{spark.app.id -> ...|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSnapshots for {db_name}.{table_name}:\")\n",
    "spark.sql(f\"SELECT * FROM {iceberg_catalog_name}.{db_name}.{table_name}.snapshots\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6a04e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Manifests for spark_schema.spark_orders:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>path</th>\n",
       "      <th>length</th>\n",
       "      <th>partition_spec_id</th>\n",
       "      <th>added_snapshot_id</th>\n",
       "      <th>added_data_files_count</th>\n",
       "      <th>existing_data_files_count</th>\n",
       "      <th>deleted_data_files_count</th>\n",
       "      <th>added_delete_files_count</th>\n",
       "      <th>existing_delete_files_count</th>\n",
       "      <th>deleted_delete_files_count</th>\n",
       "      <th>partition_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://iceberg-warehouse/spark_schema/spark_ord...</td>\n",
       "      <td>7828</td>\n",
       "      <td>0</td>\n",
       "      <td>2531062654959842889</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(False, False, books, electronics), (False, F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://iceberg-warehouse/spark_schema/spark_ord...</td>\n",
       "      <td>7946</td>\n",
       "      <td>0</td>\n",
       "      <td>7702743561916277481</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(False, False, books, home), (False, False, 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   content                                               path  length  \\\n",
       "0        0  s3a://iceberg-warehouse/spark_schema/spark_ord...    7828   \n",
       "1        0  s3a://iceberg-warehouse/spark_schema/spark_ord...    7946   \n",
       "\n",
       "   partition_spec_id    added_snapshot_id  added_data_files_count  \\\n",
       "0                  0  2531062654959842889                       2   \n",
       "1                  0  7702743561916277481                       5   \n",
       "\n",
       "   existing_data_files_count  deleted_data_files_count  \\\n",
       "0                          0                         0   \n",
       "1                          0                         0   \n",
       "\n",
       "   added_delete_files_count  existing_delete_files_count  \\\n",
       "0                         0                            0   \n",
       "1                         0                            0   \n",
       "\n",
       "   deleted_delete_files_count  \\\n",
       "0                           0   \n",
       "1                           0   \n",
       "\n",
       "                                 partition_summaries  \n",
       "0  [(False, False, books, electronics), (False, F...  \n",
       "1  [(False, False, books, home), (False, False, 2...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"\\nManifests for {db_name}.{table_name}:\")\n",
    "spark.sql(f\"SELECT * FROM {iceberg_catalog_name}.{db_name}.{table_name}.manifests\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b553774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Files for spark_schema.spark_orders:\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-------------------------+\n",
      "|file_path                                                                                                                                                          |record_count|partition                |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-------------------------+\n",
      "|s3a://iceberg-warehouse/spark_schema/spark_orders/data/category=electronics/order_date_day=2023-01-19/00000-12-3d818398-c4b4-4230-bbf0-4dc4f2e2691c-0-00001.parquet|1           |{electronics, 2023-01-19}|\n",
      "|s3a://iceberg-warehouse/spark_schema/spark_orders/data/category=books/order_date_day=2023-01-19/00000-12-3d818398-c4b4-4230-bbf0-4dc4f2e2691c-0-00002.parquet      |1           |{books, 2023-01-19}      |\n",
      "|s3a://iceberg-warehouse/spark_schema/spark_orders/data/category=books/order_date_day=2023-01-16/00000-2-ce9b65fa-78c3-44de-8417-625219c0f5ee-0-00002.parquet       |1           |{books, 2023-01-16}      |\n",
      "|s3a://iceberg-warehouse/spark_schema/spark_orders/data/category=books/order_date_day=2023-01-18/00000-2-ce9b65fa-78c3-44de-8417-625219c0f5ee-0-00005.parquet       |1           |{books, 2023-01-18}      |\n",
      "|s3a://iceberg-warehouse/spark_schema/spark_orders/data/category=home/order_date_day=2023-01-17/00000-2-ce9b65fa-78c3-44de-8417-625219c0f5ee-0-00004.parquet        |1           |{home, 2023-01-17}       |\n",
      "|s3a://iceberg-warehouse/spark_schema/spark_orders/data/category=electronics/order_date_day=2023-01-15/00000-2-ce9b65fa-78c3-44de-8417-625219c0f5ee-0-00001.parquet |1           |{electronics, 2023-01-15}|\n",
      "|s3a://iceberg-warehouse/spark_schema/spark_orders/data/category=electronics/order_date_day=2023-01-16/00000-2-ce9b65fa-78c3-44de-8417-625219c0f5ee-0-00003.parquet |1           |{electronics, 2023-01-16}|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nData Files for {db_name}.{table_name}:\")\n",
    "spark.sql(f\"SELECT file_path, record_count, partition FROM {iceberg_catalog_name}.{db_name}.{table_name}.files\")\\\n",
    ".show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33f3fc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Partitions for spark_schema.spark_orders:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>partition</th>\n",
       "      <th>spec_id</th>\n",
       "      <th>record_count</th>\n",
       "      <th>file_count</th>\n",
       "      <th>total_data_file_size_in_bytes</th>\n",
       "      <th>position_delete_record_count</th>\n",
       "      <th>position_delete_file_count</th>\n",
       "      <th>equality_delete_record_count</th>\n",
       "      <th>equality_delete_file_count</th>\n",
       "      <th>last_updated_at</th>\n",
       "      <th>last_updated_snapshot_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(electronics, 2023-01-19)</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1655</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-10 10:39:46.212</td>\n",
       "      <td>2531062654959842889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(books, 2023-01-19)</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1613</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-10 10:39:46.212</td>\n",
       "      <td>2531062654959842889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(books, 2023-01-16)</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1563</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-10 10:39:41.795</td>\n",
       "      <td>7702743561916277481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(books, 2023-01-18)</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1564</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-10 10:39:41.795</td>\n",
       "      <td>7702743561916277481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(home, 2023-01-17)</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1556</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-10 10:39:41.795</td>\n",
       "      <td>7702743561916277481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(electronics, 2023-01-15)</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1606</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-10 10:39:41.795</td>\n",
       "      <td>7702743561916277481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(electronics, 2023-01-16)</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1606</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-10 10:39:41.795</td>\n",
       "      <td>7702743561916277481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   partition  spec_id  record_count  file_count  \\\n",
       "0  (electronics, 2023-01-19)        0             1           1   \n",
       "1        (books, 2023-01-19)        0             1           1   \n",
       "2        (books, 2023-01-16)        0             1           1   \n",
       "3        (books, 2023-01-18)        0             1           1   \n",
       "4         (home, 2023-01-17)        0             1           1   \n",
       "5  (electronics, 2023-01-15)        0             1           1   \n",
       "6  (electronics, 2023-01-16)        0             1           1   \n",
       "\n",
       "   total_data_file_size_in_bytes  position_delete_record_count  \\\n",
       "0                           1655                             0   \n",
       "1                           1613                             0   \n",
       "2                           1563                             0   \n",
       "3                           1564                             0   \n",
       "4                           1556                             0   \n",
       "5                           1606                             0   \n",
       "6                           1606                             0   \n",
       "\n",
       "   position_delete_file_count  equality_delete_record_count  \\\n",
       "0                           0                             0   \n",
       "1                           0                             0   \n",
       "2                           0                             0   \n",
       "3                           0                             0   \n",
       "4                           0                             0   \n",
       "5                           0                             0   \n",
       "6                           0                             0   \n",
       "\n",
       "   equality_delete_file_count         last_updated_at  \\\n",
       "0                           0 2025-07-10 10:39:46.212   \n",
       "1                           0 2025-07-10 10:39:46.212   \n",
       "2                           0 2025-07-10 10:39:41.795   \n",
       "3                           0 2025-07-10 10:39:41.795   \n",
       "4                           0 2025-07-10 10:39:41.795   \n",
       "5                           0 2025-07-10 10:39:41.795   \n",
       "6                           0 2025-07-10 10:39:41.795   \n",
       "\n",
       "   last_updated_snapshot_id  \n",
       "0       2531062654959842889  \n",
       "1       2531062654959842889  \n",
       "2       7702743561916277481  \n",
       "3       7702743561916277481  \n",
       "4       7702743561916277481  \n",
       "5       7702743561916277481  \n",
       "6       7702743561916277481  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"\\nPartitions for {db_name}.{table_name}:\")\n",
    "# This shows how data is partitioned based on `category` and `order_date_day` (hidden transform)\n",
    "spark.sql(f\"SELECT * FROM {iceberg_catalog_name}.{db_name}.{table_name}.partitions\")\\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bb74a8",
   "metadata": {},
   "source": [
    "#### Time Travel Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bac4ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = spark.sql(f\"SELECT * FROM {iceberg_catalog_name}.{db_name}.{table_name}.history ORDER BY made_current_at\")\n",
    "history_list = history_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5caf3461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id          |is_current_ancestor|\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|2025-07-10 10:39:41.795|7702743561916277481|NULL               |true               |\n",
      "|2025-07-10 10:39:46.212|2531062654959842889|7702743561916277481|true               |\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fea93d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshots_df = spark.sql(f\"SELECT * FROM {iceberg_catalog_name}.{db_name}.{table_name}.snapshots ORDER BY committed_at\")\n",
    "snapshot_list = snapshots_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c20285c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Querying table state AS OF VERSION 2531062654959842889 (after initial SQL INSERT):\n",
      "+--------+-----------+----------+------+-----------+\n",
      "|order_id|customer_id|order_date|amount|   category|\n",
      "+--------+-----------+----------+------+-----------+\n",
      "|  ORD001|    CUST101|2023-01-15|100.50|electronics|\n",
      "|  ORD002|    CUST102|2023-01-16| 75.20|      books|\n",
      "|  ORD003|    CUST101|2023-01-16|250.00|electronics|\n",
      "|  ORD004|    CUST103|2023-01-17| 45.99|       home|\n",
      "|  ORD005|    CUST102|2023-01-18|120.00|      books|\n",
      "|  ORD006|    CUST104|2023-01-19|300.20|electronics|\n",
      "|  ORD007|    CUST105|2023-01-19| 22.20|      books|\n",
      "+--------+-----------+----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if len(snapshot_list) > 1:\n",
    "    # Get the snapshot ID of the first insert operation (before the DataFrame append)\n",
    "    # Assuming the first operation was the SQL INSERT and second was DataFrame append\n",
    "    first_op_snapshot_id = history_list[0][\"snapshot_id\"] # The very first snapshot after table creation might be empty if no data was inserted then.\n",
    "                                                            # The first data snapshot is what we usually want.\n",
    "    \n",
    "    # Find the snapshot *after* the first batch of INSERTs\n",
    "    # The first row is the earliest snapshot.\n",
    "    if len(snapshot_list) >= 2 and snapshot_list[1][\"operation\"] == \"append\": # Assuming first user data insert is an append\n",
    "            target_snapshot_id = snapshot_list[1][\"snapshot_id\"] # This would be after the first SQL INSERT\n",
    "            print(f\"\\nQuerying table state AS OF VERSION {target_snapshot_id} (after initial SQL INSERT):\")\n",
    "            spark.read.option(\"snapshot-id\", target_snapshot_id)\\\n",
    "                .table(f\"{iceberg_catalog_name}.{db_name}.{table_name}\")\\\n",
    "                .orderBy(\"order_date\", \"order_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b9ecd54-b0e3-4c83-b85a-f8b3ccf249af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Querying table state AS OF LATEST SNAPSHOT 2531062654959842889 (current state):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_date</th>\n",
       "      <th>amount</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ORD001</td>\n",
       "      <td>CUST101</td>\n",
       "      <td>2023-01-15</td>\n",
       "      <td>100.50</td>\n",
       "      <td>electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ORD002</td>\n",
       "      <td>CUST102</td>\n",
       "      <td>2023-01-16</td>\n",
       "      <td>75.20</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ORD003</td>\n",
       "      <td>CUST101</td>\n",
       "      <td>2023-01-16</td>\n",
       "      <td>250.00</td>\n",
       "      <td>electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ORD004</td>\n",
       "      <td>CUST103</td>\n",
       "      <td>2023-01-17</td>\n",
       "      <td>45.99</td>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ORD005</td>\n",
       "      <td>CUST102</td>\n",
       "      <td>2023-01-18</td>\n",
       "      <td>120.00</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ORD006</td>\n",
       "      <td>CUST104</td>\n",
       "      <td>2023-01-19</td>\n",
       "      <td>300.20</td>\n",
       "      <td>electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ORD007</td>\n",
       "      <td>CUST105</td>\n",
       "      <td>2023-01-19</td>\n",
       "      <td>22.20</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  order_id customer_id  order_date  amount     category\n",
       "0   ORD001     CUST101  2023-01-15  100.50  electronics\n",
       "1   ORD002     CUST102  2023-01-16   75.20        books\n",
       "2   ORD003     CUST101  2023-01-16  250.00  electronics\n",
       "3   ORD004     CUST103  2023-01-17   45.99         home\n",
       "4   ORD005     CUST102  2023-01-18  120.00        books\n",
       "5   ORD006     CUST104  2023-01-19  300.20  electronics\n",
       "6   ORD007     CUST105  2023-01-19   22.20        books"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_snapshot_id = snapshot_list[-1][\"snapshot_id\"]\n",
    "print(f\"\\nQuerying table state AS OF LATEST SNAPSHOT {latest_snapshot_id} (current state):\")\n",
    "spark.read.table(f\"{iceberg_catalog_name}.{db_name}.{table_name}\").orderBy(\"order_date\", \"order_id\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4113de05-dda7-4ba6-b200-5618cb5c3a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Querying table state AS OF TIMESTAMP '2025-07-10 10:39:41.795000':\n",
      "+--------+-----------+----------+------+-----------+\n",
      "|order_id|customer_id|order_date|amount|   category|\n",
      "+--------+-----------+----------+------+-----------+\n",
      "|  ORD001|    CUST101|2023-01-15|100.50|electronics|\n",
      "|  ORD002|    CUST102|2023-01-16| 75.20|      books|\n",
      "|  ORD003|    CUST101|2023-01-16|250.00|electronics|\n",
      "|  ORD004|    CUST103|2023-01-17| 45.99|       home|\n",
      "|  ORD005|    CUST102|2023-01-18|120.00|      books|\n",
      "+--------+-----------+----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Time travel using timestamp (requires made_current_at timestamp)\n",
    "if len(snapshot_list) >= 2:\n",
    "    timestamp_before_last_append = history_list[-2][\"made_current_at\"] # Timestamp of the snapshot before the last one\n",
    "    print(f\"\\nQuerying table state AS OF TIMESTAMP '{timestamp_before_last_append}':\")\n",
    "    spark.read.option(\"as-of-timestamp\", str(int(timestamp_before_last_append.timestamp() * 1000))) \\\n",
    "         .table(f\"{iceberg_catalog_name}.{db_name}.{table_name}\") \\\n",
    "         .orderBy(\"order_date\", \"order_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee307d4-3f45-41f7-9664-332551fac110",
   "metadata": {},
   "source": [
    "Schema Evolution (Example: Add a new column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76fa7d31-83a2-477d-9ff5-297906cf356e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Schema before evolution:\n",
      "+--------------+----------------+-------+\n",
      "|      col_name|       data_type|comment|\n",
      "+--------------+----------------+-------+\n",
      "|      order_id|          string|   NULL|\n",
      "|   customer_id|          string|   NULL|\n",
      "|    order_date|            date|   NULL|\n",
      "|        amount|   decimal(10,2)|   NULL|\n",
      "|      category|          string|   NULL|\n",
      "|              |                |       |\n",
      "|# Partitioning|                |       |\n",
      "|        Part 0|        category|       |\n",
      "|        Part 1|days(order_date)|       |\n",
      "+--------------+----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSchema before evolution:\")\n",
    "spark.sql(f\"DESCRIBE {iceberg_catalog_name}.{db_name}.{table_name}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3ff0f47-ff8a-4cf3-923c-f574889c4386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Schema after adding 'is_returned' column:\n",
      "+--------------+----------------+-------+\n",
      "|      col_name|       data_type|comment|\n",
      "+--------------+----------------+-------+\n",
      "|      order_id|          string|   NULL|\n",
      "|   customer_id|          string|   NULL|\n",
      "|    order_date|            date|   NULL|\n",
      "|        amount|   decimal(10,2)|   NULL|\n",
      "|      category|          string|   NULL|\n",
      "|   is_returned|         boolean|   NULL|\n",
      "|              |                |       |\n",
      "|# Partitioning|                |       |\n",
      "|        Part 0|        category|       |\n",
      "|        Part 1|days(order_date)|       |\n",
      "+--------------+----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new column 'is_returned'\n",
    "spark.sql(f\"ALTER TABLE {iceberg_catalog_name}.{db_name}.{table_name} ADD COLUMN is_returned BOOLEAN\")\n",
    "print(f\"\\nSchema after adding 'is_returned' column:\")\n",
    "spark.sql(f\"DESCRIBE {iceberg_catalog_name}.{db_name}.{table_name}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "946ff22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data after inserting with new column (old rows will have null for 'is_returned'):\n",
      "+--------+-----------+----------+------+-----------+\n",
      "|order_id|   category|order_date|amount|is_returned|\n",
      "+--------+-----------+----------+------+-----------+\n",
      "|  ORD001|electronics|2023-01-15|100.50|       NULL|\n",
      "|  ORD002|      books|2023-01-16| 75.20|       NULL|\n",
      "|  ORD003|electronics|2023-01-16|250.00|       NULL|\n",
      "|  ORD004|       home|2023-01-17| 45.99|       NULL|\n",
      "|  ORD005|      books|2023-01-18|120.00|       NULL|\n",
      "|  ORD006|electronics|2023-01-19|300.20|       NULL|\n",
      "|  ORD007|      books|2023-01-19| 22.20|       NULL|\n",
      "|  ORD008|       home|2023-01-20| 55.00|       true|\n",
      "+--------+-----------+----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Insert data with the new column\n",
    "spark.sql(f\"\"\"\n",
    "INSERT INTO {iceberg_catalog_name}.{db_name}.{table_name}\n",
    "VALUES ('ORD008', 'CUST101', DATE '2023-01-20', 55.00, 'home', true)\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\nData after inserting with new column (old rows will have null for 'is_returned'):\")\n",
    "spark.sql(f\"\"\"SELECT order_id, category, order_date, amount, is_returned \n",
    "          FROM {iceberg_catalog_name}.{db_name}.{table_name} \n",
    "          ORDER BY order_date, order_id\"\"\")\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46367fe0",
   "metadata": {},
   "source": [
    "\n",
    "Data Compaction (Rewrite Data Files - Small File Compaction)\n",
    "- Iceberg procedures are called using CALL\n",
    "- Insert some more data to potentially create small files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf4e3961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Files before compaction for spark_schema.spark_orders (partition: category=books):\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------+\n",
      "|file_path                                                                                                                                                    |record_count|file_size_in_bytes|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------+\n",
      "|s3a://iceberg-warehouse/spark_schema/spark_orders/data/category=books/order_date_day=2023-01-21/00000-42-f367cde7-290f-479d-8e38-721a67e2ae05-0-00001.parquet|1           |1799              |\n",
      "|s3a://iceberg-warehouse/spark_schema/spark_orders/data/category=books/order_date_day=2023-01-21/00000-40-f8195a6d-d91f-4f8c-a657-2c96046dabc3-0-00001.parquet|1           |1798              |\n",
      "|s3a://iceberg-warehouse/spark_schema/spark_orders/data/category=books/order_date_day=2023-01-19/00000-12-3d818398-c4b4-4230-bbf0-4dc4f2e2691c-0-00002.parquet|1           |1613              |\n",
      "|s3a://iceberg-warehouse/spark_schema/spark_orders/data/category=books/order_date_day=2023-01-16/00000-2-ce9b65fa-78c3-44de-8417-625219c0f5ee-0-00002.parquet |1           |1563              |\n",
      "|s3a://iceberg-warehouse/spark_schema/spark_orders/data/category=books/order_date_day=2023-01-18/00000-2-ce9b65fa-78c3-44de-8417-625219c0f5ee-0-00005.parquet |1           |1564              |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"INSERT INTO {iceberg_catalog_name}.{db_name}.{table_name} \n",
    "          VALUES ('ORD009', 'CUST106', DATE '2023-01-21', 10.00, 'books', false)\"\"\")\n",
    "spark.sql(f\"\"\"INSERT INTO {iceberg_catalog_name}.{db_name}.{table_name}\n",
    "          VALUES ('ORD010', 'CUST106', DATE '2023-01-21', 12.00, 'books', false)\"\"\")\n",
    "\n",
    "print(f\"\\nData Files before compaction for {db_name}.{table_name} (partition: category=books):\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT file_path, record_count, file_size_in_bytes\n",
    "    FROM {iceberg_catalog_name}.{db_name}.{table_name}.files\n",
    "    WHERE partition.category = 'books'\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb25ec1",
   "metadata": {},
   "source": [
    "Execute rewrite_data_files procedure for compaction\n",
    "- This will compact small files into larger ones, default strategy is 'sort' which also sorts data within files\n",
    "- For very small tables, the effect might be limited or group all into one file per partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1209b70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running data compaction (rewrite_data_files) for table spark_schema.spark_orders...\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "|rewritten_data_files_count|added_data_files_count|rewritten_bytes_count|failed_data_files_count|\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "|                         2|                     1|                 3597|                      0|\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "\n",
      "Compaction procedure finished.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nRunning data compaction (rewrite_data_files) for table {db_name}.{table_name}...\")\n",
    "# You can specify sorting options or a where clause to limit compaction scope\n",
    "result_df = spark.sql(f\"\"\"CALL {iceberg_catalog_name}.system.rewrite_data_files(\n",
    "                      table => '{db_name}.{table_name}', \n",
    "                      strategy => 'sort', sort_order => 'order_date ASC, amount DESC', \n",
    "                      options => map('min-input-files','1'))\"\"\")\n",
    "# options => map('min-input-files','1') to force compaction even with few files for demo\n",
    "result_df.show()\n",
    "print(\"Compaction procedure finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8aacb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Files after compaction for spark_schema.spark_orders (partition: category=books):\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------+\n",
      "|file_path                                                                                                                                                    |record_count|file_size_in_bytes|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------+\n",
      "|s3a://iceberg-warehouse/spark_schema/spark_orders/data/category=books/order_date_day=2023-01-21/00000-45-c6668fad-6c6d-4093-ac5d-36cba9abc5fd-0-00001.parquet|2           |1974              |\n",
      "|s3a://iceberg-warehouse/spark_schema/spark_orders/data/category=books/order_date_day=2023-01-19/00000-12-3d818398-c4b4-4230-bbf0-4dc4f2e2691c-0-00002.parquet|1           |1613              |\n",
      "|s3a://iceberg-warehouse/spark_schema/spark_orders/data/category=books/order_date_day=2023-01-16/00000-2-ce9b65fa-78c3-44de-8417-625219c0f5ee-0-00002.parquet |1           |1563              |\n",
      "|s3a://iceberg-warehouse/spark_schema/spark_orders/data/category=books/order_date_day=2023-01-18/00000-2-ce9b65fa-78c3-44de-8417-625219c0f5ee-0-00005.parquet |1           |1564              |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nData Files after compaction for {db_name}.{table_name} (partition: category=books):\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT file_path, record_count, file_size_in_bytes\n",
    "    FROM {iceberg_catalog_name}.{db_name}.{table_name}.files\n",
    "    WHERE partition.category = 'books'\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "586d3a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Snapshots after compaction:\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|        committed_at|        snapshot_id|          parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|2025-07-10 10:39:...|8755651545572138546|5272369974064174615|  replace|s3a://iceberg-war...|{added-data-files...|\n",
      "|2025-07-10 10:39:...|5272369974064174615|8777658452431709262|   append|s3a://iceberg-war...|{spark.app.id -> ...|\n",
      "|2025-07-10 10:39:...|8777658452431709262|3887949544037912632|   append|s3a://iceberg-war...|{spark.app.id -> ...|\n",
      "|2025-07-10 10:39:...|3887949544037912632|2531062654959842889|   append|s3a://iceberg-war...|{spark.app.id -> ...|\n",
      "|2025-07-10 10:39:...|2531062654959842889|7702743561916277481|   append|s3a://iceberg-war...|{spark.app.id -> ...|\n",
      "|2025-07-10 10:39:...|7702743561916277481|               NULL|   append|s3a://iceberg-war...|{spark.app.id -> ...|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSnapshots after compaction:\")\n",
    "spark.sql(f\"SELECT * FROM {iceberg_catalog_name}.{db_name}.{table_name}.snapshots ORDER BY committed_at DESC\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343f2773",
   "metadata": {},
   "source": [
    "Filter Pushdown Demonstration\n",
    "- The table is partitioned by (category, days(order_date))\n",
    "- Spark should be able to push down filters on `category` and `order_date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ffee40b4-2b3b-45cd-94ac-9e233ca6c69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query with partition column filters:\n",
      "+--------+-----------+----------+------+-----------+-----------+\n",
      "|order_id|customer_id|order_date|amount|   category|is_returned|\n",
      "+--------+-----------+----------+------+-----------+-----------+\n",
      "|  ORD001|    CUST101|2023-01-15|100.50|electronics|       NULL|\n",
      "|  ORD003|    CUST101|2023-01-16|250.00|electronics|       NULL|\n",
      "|  ORD006|    CUST104|2023-01-19|300.20|electronics|       NULL|\n",
      "+--------+-----------+----------+------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_with_filters = f\"\"\"\n",
    "SELECT * FROM {iceberg_catalog_name}.{db_name}.{table_name}\n",
    "WHERE category = 'electronics' AND order_date >= DATE '2023-01-15' AND order_date < DATE '2023-01-20'\n",
    "\"\"\"\n",
    "print(\"\\nQuery with partition column filters:\")\n",
    "spark.sql(query_with_filters).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0775d57c-2867-4c5d-b4d7-6fec67fe642a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXPLAIN plan for the query (look for PushedFilters in ParquetScan or IcebergScan):\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- BatchScan iceberg_jdbc.spark_schema.spark_orders[order_id#1083, customer_id#1084, order_date#1085, amount#1086, category#1087, is_returned#1088] iceberg_jdbc.spark_schema.spark_orders (branch=null) [filters=category IS NOT NULL, order_date IS NOT NULL, category = 'electronics', order_date >= 19372, order_date < 19377, groupedBy=] RuntimeFilters: []\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEXPLAIN plan for the query (look for PushedFilters in ParquetScan or IcebergScan):\")\n",
    "print(spark.sql(f\"EXPLAIN {query_with_filters}\").collect()[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b474fde1-6a4f-4a4d-8898-3d20190ce84f",
   "metadata": {},
   "source": [
    "Interoperability Check - Read data created by Trino (if trino_schema.employees exists)\n",
    " - Ensure the Trino notebook (01-trino-iceberg-getting-started.ipynb) has been run to create this table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f1a4dff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.catalog.iceberg_catalog\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "spark.conf.set(\"spark.sql.catalog.iceberg_catalog.catalog-impl\", \"org.apache.iceberg.jdbc.JdbcCatalog\") \n",
    "spark.conf.set(\"spark.sql.catalog.iceberg_catalog.uri\", \"jdbc:postgresql://postgres:5432/iceberg_catalog\")\n",
    "spark.conf.set(\"spark.sql.catalog.iceberg_catalog.jdbc.user\", \"iceberg\") \n",
    "spark.conf.set(\"spark.sql.catalog.iceberg_catalog.jdbc.password\", \"icebergpassword\") \n",
    "spark.conf.set(\"spark.sql.catalog.iceberg_catalog.warehouse\", \"s3a://iceberg-warehouse/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b3d455be",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentCatalog('iceberg_catalog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5dab18f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CatalogMetadata(name='default_cache_iceberg', description=None),\n",
       " CatalogMetadata(name='iceberg_catalog', description=None),\n",
       " CatalogMetadata(name='iceberg_jdbc', description=None),\n",
       " CatalogMetadata(name='spark_catalog', description=None)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listCatalogs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "35c463b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "iceberg_trino_catalog_name='iceberg_catalog'\n",
    "trino_table_name = \"trino_schema.employees\" # From Trino notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "812399bf-91ca-49c2-b1d0-b068ee865a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempting to read Trino-created table: iceberg_catalog.trino_schema.employees\n",
      "+--------------------+-------------+-------+\n",
      "|            col_name|    data_type|comment|\n",
      "+--------------------+-------------+-------+\n",
      "|                  id|          int|   NULL|\n",
      "|                name|       string|   NULL|\n",
      "|          department|       string|   NULL|\n",
      "|              salary|decimal(10,2)|   NULL|\n",
      "|           hire_date|         date|   NULL|\n",
      "|# Partition Infor...|             |       |\n",
      "|          # col_name|    data_type|comment|\n",
      "|          department|       string|   NULL|\n",
      "+--------------------+-------------+-------+\n",
      "\n",
      "+---+-------------+----------+---------+----------+\n",
      "| id|         name|department|   salary| hire_date|\n",
      "+---+-------------+----------+---------+----------+\n",
      "|  4|  Diana Green|     Sales| 95000.00|2018-05-22|\n",
      "|  5| Edward Black|     Sales|105000.00|2017-11-30|\n",
      "|  7|George Yellow|        HR| 65000.00|2023-03-15|\n",
      "|  7|George Yellow|        HR| 65000.00|2023-03-15|\n",
      "|  3|Charlie Brown|        HR| 70000.00|2021-03-10|\n",
      "+---+-------------+----------+---------+----------+\n",
      "\n",
      "Successfully read Trino-created table with Spark.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nAttempting to read Trino-created table: {iceberg_trino_catalog_name}.{trino_table_name}\")\n",
    "# Spark needs to know the schema. If Trino created it, Spark should discover it via the JDBC catalog.\n",
    "spark.sql(f\"DESCRIBE {iceberg_trino_catalog_name}.{trino_table_name}\").show()\n",
    "spark.sql(f\"SELECT * FROM {iceberg_trino_catalog_name}.{trino_table_name} LIMIT 5\").show()\n",
    "print(\"Successfully read Trino-created table with Spark.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ee330cec-8009-4c65-8d37-3ec6ce00344b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spark Iceberg Datalakehouse Demo (Phase 2) completed.\n"
     ]
    }
   ],
   "source": [
    "# spark.stop() # Commented out to allow re-running cells easily\n",
    "print(\"\\nSpark Iceberg Datalakehouse Demo (Phase 2) completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "15ffebaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spark Iceberg Datalakehouse Demo (Phase 2) completed.\n"
     ]
    }
   ],
   "source": [
    "# spark.stop() # Commented out to allow re-running cells easily\n",
    "print(\"\\nSpark Iceberg Datalakehouse Demo (Phase 2) completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df0cf58-24d5-4809-b69e-75c0165fb437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410cd90f-8b55-42c7-b709-65bc7579d425",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
