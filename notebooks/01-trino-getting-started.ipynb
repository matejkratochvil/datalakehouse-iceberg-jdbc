{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff62cb39",
   "metadata": {},
   "source": [
    "# Trino Iceberg Datalakehouse - Getting Started\n",
    "\n",
    "This notebook demonstrates basic DDL and DML operations on Iceberg tables using Trino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a11634b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting trino\n",
      "  Downloading trino-0.334.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: sqlalchemy in /opt/conda/lib/python3.12/site-packages (2.0.40)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: lz4 in /opt/conda/lib/python3.12/site-packages (from trino) (4.3.3)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.12/site-packages (from trino) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.12/site-packages (from trino) (2025.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/conda/lib/python3.12/site-packages (from trino) (2.32.3)\n",
      "Collecting tzlocal (from trino)\n",
      "  Downloading tzlocal-5.3.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: zstandard in /opt/conda/lib/python3.12/site-packages (from trino) (0.23.0)\n",
      "Requirement already satisfied: greenlet>=1 in /opt/conda/lib/python3.12/site-packages (from sqlalchemy) (3.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/conda/lib/python3.12/site-packages (from sqlalchemy) (4.13.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil->trino) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.31.0->trino) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests>=2.31.0->trino) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.31.0->trino) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.31.0->trino) (2025.1.31)\n",
      "Downloading trino-0.334.0-py3-none-any.whl (57 kB)\n",
      "Downloading tzlocal-5.3.1-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: tzlocal, trino\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [trino]\n",
      "\u001b[1A\u001b[2KSuccessfully installed trino-0.334.0 tzlocal-5.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install Trino client if not already in the Jupyter image\n",
    "%pip install trino sqlalchemy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "973b3015-e5ee-456e-87f9-3da18d418ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "import pandas as pd\n",
    "\n",
    "TRINO_HOST = 'trino-coordinator' # Service name in docker-compose\n",
    "TRINO_PORT = 8080\n",
    "TRINO_USER = 'testuser' # Can be any string, Trino by default doesn't enforce auth in this setup\n",
    "CATALOG = 'iceberg' # Catalog name as defined in iceberg.properties\n",
    "\n",
    "# Connection string for Trino\n",
    "trino_conn_str = f'trino://{TRINO_USER}@{TRINO_HOST}:{TRINO_PORT}/{CATALOG}'\n",
    "engine = create_engine(trino_conn_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "159c8959-e155-47cd-991d-4c932b653195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Trino: trino://testuser@trino-coordinator:8080/iceberg\n"
     ]
    }
   ],
   "source": [
    "def run_trino_query(query, fetch_results=True):\n",
    "    \"\"\"Executes a Trino query and optionally fetches results into a Pandas DataFrame.\"\"\"\n",
    "    with engine.connect() as connection:\n",
    "        # For queries that modify data or schema, autocommit is usually the default or not needed to be set explicitly for Trino\n",
    "        # For DML/DDL, we might not always fetch results\n",
    "        result_proxy = connection.execute(text(query))\n",
    "        if fetch_results and result_proxy.returns_rows:\n",
    "            df = pd.DataFrame(result_proxy.fetchall(), columns=result_proxy.keys())\n",
    "            return df\n",
    "        elif fetch_results: # No rows returned but fetch_results was true\n",
    "            return pd.DataFrame(columns=result_proxy.keys() if result_proxy.returns_rows else [])\n",
    "        else:\n",
    "            print(f\"Query executed successfully (returns_rows={result_proxy.returns_rows}).\")\n",
    "            # For DDL/DML, we might want to check row count if available\n",
    "            # print(f\"Rows affected (approx): {result_proxy.rowcount}\") # rowcount might not be reliable for all statements/drivers\n",
    "            return None\n",
    "\n",
    "print(f\"Connected to Trino: {trino_conn_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cb4631",
   "metadata": {},
   "source": [
    "## 1. Create Schema (Namespace in Iceberg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24380e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully (returns_rows=False).\n",
      "Schema 'my_schema' created or already exists.\n",
      "\n",
      "Available schemas in Iceberg catalog:\n",
      "               Schema\n",
      "0  information_schema\n",
      "1           my_schema\n",
      "2              system\n"
     ]
    }
   ],
   "source": [
    "SCHEMA_NAME = 'my_schema'\n",
    "\n",
    "run_trino_query(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA_NAME} WITH (location = 's3a://iceberg-warehouse/{SCHEMA_NAME}/')\", fetch_results=False)\n",
    "print(f\"Schema '{SCHEMA_NAME}' created or already exists.\")\n",
    "\n",
    "print(\"\\nAvailable schemas in Iceberg catalog:\")\n",
    "schemas_df = run_trino_query(f\"SHOW SCHEMAS FROM {CATALOG}\")\n",
    "print(schemas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8ead9c",
   "metadata": {},
   "source": [
    "## 2. Create an Iceberg Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69e41a84-cf8a-402e-a64b-4627c90324f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Table</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Table]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_trino_query(f\"SHOW TABLES FROM {CATALOG}.{SCHEMA_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25fe83a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully (returns_rows=False).\n",
      "Table 'iceberg.my_schema.employees' created or already exists.\n",
      "\n",
      "Tables in schema 'my_schema':\n",
      "       Table\n",
      "0  employees\n"
     ]
    }
   ],
   "source": [
    "TABLE_NAME = 'employees'\n",
    "FQN_TABLE_NAME = f\"{CATALOG}.{SCHEMA_NAME}.{TABLE_NAME}\"\n",
    "\n",
    "create_table_sql = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {FQN_TABLE_NAME} (\n",
    "    id INT,\n",
    "    name VARCHAR,\n",
    "    department VARCHAR,\n",
    "    salary DECIMAL(10, 2),\n",
    "    hire_date DATE\n",
    ")\n",
    "WITH (\n",
    "    format = 'PARQUET',\n",
    "    partitioning = ARRAY['department']\n",
    ")\n",
    "\"\"\"\n",
    "run_trino_query(create_table_sql, fetch_results=False)\n",
    "print(f\"Table '{FQN_TABLE_NAME}' created or already exists.\")\n",
    "\n",
    "print(f\"\\nTables in schema '{SCHEMA_NAME}':\")\n",
    "tables_df = run_trino_query(f\"SHOW TABLES FROM {CATALOG}.{SCHEMA_NAME}\")\n",
    "print(tables_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790c6357",
   "metadata": {},
   "source": [
    "## 3. Insert Data (DML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03d4cfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully (returns_rows=True).\n",
      "Data inserted into iceberg.my_schema.employees.\n"
     ]
    }
   ],
   "source": [
    "insert_sql = f\"\"\"\n",
    "INSERT INTO {FQN_TABLE_NAME} VALUES\n",
    "(1, 'Alice Smith', 'Engineering', 90000.00, DATE '2020-01-15'),\n",
    "(2, 'Bob Johnson', 'Engineering', 85000.00, DATE '2019-07-01'),\n",
    "(3, 'Charlie Brown', 'HR', 70000.00, DATE '2021-03-10'),\n",
    "(4, 'Diana Green', 'Sales', 95000.00, DATE '2018-05-22'),\n",
    "(5, 'Edward Black', 'Sales', 105000.00, DATE '2017-11-30')\n",
    "\"\"\"\n",
    "run_trino_query(insert_sql, fetch_results=False)\n",
    "print(f\"Data inserted into {FQN_TABLE_NAME}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc8d623",
   "metadata": {},
   "source": [
    "## 4. Select Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8efe098c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All employees:\n",
      "   id           name   department     salary   hire_date\n",
      "0   1    Alice Smith  Engineering   90000.00  2020-01-15\n",
      "1   3  Charlie Brown           HR   70000.00  2021-03-10\n",
      "2   4    Diana Green        Sales   95000.00  2018-05-22\n",
      "3   5   Edward Black        Sales  105000.00  2017-11-30\n",
      "4   2    Bob Johnson  Engineering   85000.00  2019-07-01\n",
      "\n",
      "Engineering department employees (filter pushdown check):\n",
      "   id         name   department    salary   hire_date\n",
      "0   1  Alice Smith  Engineering  90000.00  2020-01-15\n",
      "1   2  Bob Johnson  Engineering  85000.00  2019-07-01\n"
     ]
    }
   ],
   "source": [
    "print(\"All employees:\")\n",
    "all_employees_df = run_trino_query(f\"SELECT * FROM {FQN_TABLE_NAME}\")\n",
    "print(all_employees_df)\n",
    "\n",
    "print(\"\\nEngineering department employees (filter pushdown check):\")\n",
    "eng_employees_df = run_trino_query(f\"SELECT * FROM {FQN_TABLE_NAME} WHERE department = 'Engineering'\")\n",
    "print(eng_employees_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b9e55d",
   "metadata": {},
   "source": [
    "## 5. Iceberg Table Metadata (Snapshots, Manifests, Files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ab6266e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Table snapshots (history):\n",
      "                   made_current_at          snapshot_id     parent_id  \\\n",
      "0 2025-05-10 22:33:12.114000+00:00  8543142657744447243           NaN   \n",
      "1 2025-05-10 22:33:14.216000+00:00  5764103492386225884  8.543143e+18   \n",
      "\n",
      "   is_current_ancestor  \n",
      "0                 True  \n",
      "1                 True  \n",
      "\n",
      "Table manifest files:\n",
      "                                                path  length  \\\n",
      "0  s3a://iceberg-warehouse/my_schema/employees-8a...    7773   \n",
      "\n",
      "   partition_spec_id    added_snapshot_id  added_data_files_count  \\\n",
      "0                  0  5764103492386225884                       3   \n",
      "\n",
      "   added_rows_count  existing_data_files_count  existing_rows_count  \\\n",
      "0                 5                          0                    0   \n",
      "\n",
      "   deleted_data_files_count  deleted_rows_count  \\\n",
      "0                         0                   0   \n",
      "\n",
      "                                 partition_summaries  \n",
      "0  [(contains_null: False, contains_nan: False, l...  \n",
      "\n",
      "Table data files:\n",
      "                                           file_path  record_count  \\\n",
      "0  s3a://iceberg-warehouse/my_schema/employees-8a...             1   \n",
      "1  s3a://iceberg-warehouse/my_schema/employees-8a...             2   \n",
      "2  s3a://iceberg-warehouse/my_schema/employees-8a...             2   \n",
      "\n",
      "                     partition  \n",
      "0           (department: 'HR')  \n",
      "1  (department: 'Engineering')  \n",
      "2        (department: 'Sales')  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTable snapshots (history):\")\n",
    "history_df = run_trino_query(f\"SELECT * FROM {CATALOG}.{SCHEMA_NAME}.\\\"{TABLE_NAME}$history\\\"\") # Note escaped quotes for table name\n",
    "print(history_df)\n",
    "\n",
    "print(\"\\nTable manifest files:\")\n",
    "manifests_df = run_trino_query(f\"SELECT * FROM {CATALOG}.{SCHEMA_NAME}.\\\"{TABLE_NAME}$manifests\\\"\")\n",
    "print(manifests_df)\n",
    "\n",
    "print(\"\\nTable data files:\")\n",
    "files_df = run_trino_query(f\"SELECT file_path, record_count, partition FROM {CATALOG}.{SCHEMA_NAME}.\\\"{TABLE_NAME}$files\\\"\")\n",
    "print(files_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8970beb",
   "metadata": {},
   "source": [
    "## 6. Hidden Partitioning Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9088c09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully (returns_rows=False).\n",
      "Table 'iceberg.my_schema.events' with hidden partitioning created.\n",
      "Query executed successfully (returns_rows=True).\n",
      "Data inserted into 'iceberg.my_schema.events'.\n",
      "\n",
      "Events from 2023-10-26 (filter pushdown on hidden partition):\n",
      "  event_id event_type                   event_ts  user_id\n",
      "0   event1      click 2023-10-26 10:00:00.123456      101\n",
      "1   event2       view 2023-10-26 11:30:00.654321      102\n",
      "\n",
      "Partitions for events table (shows transformed partition values):\n",
      "                                     partition  record_count  file_count  \\\n",
      "0  (event_ts_day: datetime.date(2023, 10, 26))             2           1   \n",
      "1  (event_ts_day: datetime.date(2023, 10, 27))             2           1   \n",
      "\n",
      "   total_size                                               data  \n",
      "0         652  (event_id: (min: 'event1', max: 'event2', null...  \n",
      "1         660  (event_id: (min: 'event3', max: 'event4', null...  \n"
     ]
    }
   ],
   "source": [
    "EVENTS_TABLE_NAME = 'events'\n",
    "FQN_EVENTS_TABLE = f\"{CATALOG}.{SCHEMA_NAME}.{EVENTS_TABLE_NAME}\"\n",
    "\n",
    "create_hidden_partition_table_sql = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {FQN_EVENTS_TABLE} (\n",
    "    event_id VARCHAR,\n",
    "    event_type VARCHAR,\n",
    "    event_ts TIMESTAMP(6),  -- High precision timestamp\n",
    "    user_id INT\n",
    ")\n",
    "WITH (\n",
    "    format = 'PARQUET',\n",
    "    partitioning = ARRAY['day(event_ts)'] -- Hidden partitioning on event_ts by day\n",
    ")\n",
    "\"\"\"\n",
    "run_trino_query(create_hidden_partition_table_sql, fetch_results=False)\n",
    "print(f\"Table '{FQN_EVENTS_TABLE}' with hidden partitioning created.\")\n",
    "\n",
    "insert_events_sql = f\"\"\"\n",
    "INSERT INTO {FQN_EVENTS_TABLE} VALUES\n",
    "('event1', 'click', TIMESTAMP '2023-10-26 10:00:00.123456', 101),\n",
    "('event2', 'view', TIMESTAMP '2023-10-26 11:30:00.654321', 102),\n",
    "('event3', 'purchase', TIMESTAMP '2023-10-27 09:15:00.000000', 101),\n",
    "('event4', 'click', TIMESTAMP '2023-10-27 14:00:00.987654', 103)\n",
    "\"\"\"\n",
    "run_trino_query(insert_events_sql, fetch_results=False)\n",
    "print(f\"Data inserted into '{FQN_EVENTS_TABLE}'.\")\n",
    "\n",
    "print(\"\\nEvents from 2023-10-26 (filter pushdown on hidden partition):\")\n",
    "events_26_df = run_trino_query(f\"SELECT * FROM {FQN_EVENTS_TABLE} WHERE event_ts >= TIMESTAMP '2023-10-26 00:00:00' AND event_ts < TIMESTAMP '2023-10-27 00:00:00'\")\n",
    "print(events_26_df)\n",
    "\n",
    "print(\"\\nPartitions for events table (shows transformed partition values):\")\n",
    "try:\n",
    "    event_partitions_df = run_trino_query(f\"SELECT * FROM {CATALOG}.{SCHEMA_NAME}.\\\"{EVENTS_TABLE_NAME}$partitions\\\"\")\n",
    "    print(event_partitions_df)\n",
    "except Exception as e:\n",
    "    print(f\"Could not query partitions directly: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecef61e1",
   "metadata": {},
   "source": [
    "## 7. Data Compaction (OPTIMIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85a23f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully (returns_rows=True).\n",
      "Query executed successfully (returns_rows=True).\n",
      "Inserted more data into employees table to create more files/snapshots.\n",
      "\n",
      "Table files before OPTIMIZE:\n",
      "                                           file_path  record_count\n",
      "0  s3a://iceberg-warehouse/my_schema/employees-8a...             1\n",
      "1  s3a://iceberg-warehouse/my_schema/employees-8a...             1\n",
      "2  s3a://iceberg-warehouse/my_schema/employees-8a...             1\n",
      "3  s3a://iceberg-warehouse/my_schema/employees-8a...             1\n",
      "4  s3a://iceberg-warehouse/my_schema/employees-8a...             1\n",
      "5  s3a://iceberg-warehouse/my_schema/employees-8a...             2\n",
      "6  s3a://iceberg-warehouse/my_schema/employees-8a...             2\n",
      "\n",
      "Running OPTIMIZE (minor compaction by default on Trino):\n",
      "Query executed successfully (returns_rows=True).\n",
      "AOPTIMIZE command executed.\n",
      "\n",
      "Table files after OPTIMIZE:\n",
      "                                           file_path  record_count\n",
      "0  s3a://iceberg-warehouse/my_schema/employees-8a...             4\n",
      "1  s3a://iceberg-warehouse/my_schema/employees-8a...             3\n",
      "2  s3a://iceberg-warehouse/my_schema/employees-8a...             2\n",
      "\n",
      "Table snapshots after OPTIMIZE (should show a 'replace' operation):\n",
      "           snapshot_id operation\n",
      "0   327419525842522026   replace\n",
      "1  9202396242307286450    append\n",
      "2  7092746016936639603    append\n",
      "3  5764103492386225884    append\n",
      "4  8543142657744447243    append\n"
     ]
    }
   ],
   "source": [
    "# Insert more data to potentially create smaller files in employees table\n",
    "insert_more_employees_sql = f\"\"\"\n",
    "INSERT INTO {FQN_TABLE_NAME} VALUES\n",
    "(6, 'Fiona White', 'Engineering', 75000.00, DATE '2023-01-10'),\n",
    "(7, 'George Yellow', 'HR', 65000.00, DATE '2023-03-15')\n",
    "\"\"\"\n",
    "run_trino_query(insert_more_employees_sql, fetch_results=False) # New snapshot\n",
    "run_trino_query(insert_more_employees_sql, fetch_results=False) # Another new snapshot\n",
    "print(\"Inserted more data into employees table to create more files/snapshots.\")\n",
    "\n",
    "print(\"\\nTable files before OPTIMIZE:\")\n",
    "files_before_optimize_df = run_trino_query(f\"SELECT file_path, record_count FROM {CATALOG}.{SCHEMA_NAME}.\\\"{TABLE_NAME}$files\\\"\")\n",
    "print(files_before_optimize_df)\n",
    "\n",
    "print(\"\\nRunning OPTIMIZE (minor compaction by default on Trino):\")\n",
    "try:\n",
    "    run_trino_query(f\"ALTER TABLE {FQN_TABLE_NAME} EXECUTE OPTIMIZE\", fetch_results=False)\n",
    "    print(\"OPTIMIZE command executed.\")\n",
    "    print(\"\\nTable files after OPTIMIZE:\")\n",
    "    files_after_optimize_df = run_trino_query(f\"SELECT file_path, record_count FROM {CATALOG}.{SCHEMA_NAME}.\\\"{TABLE_NAME}$files\\\"\")\n",
    "    print(files_after_optimize_df)\n",
    "    \n",
    "    print(\"\\nTable snapshots after OPTIMIZE (should show a 'replace' operation):\")\n",
    "    history_after_optimize_df = run_trino_query(f\"SELECT snapshot_id, operation FROM {CATALOG}.{SCHEMA_NAME}.\\\"{TABLE_NAME}$snapshots\\\" ORDER BY committed_at DESC\")\n",
    "    print(history_after_optimize_df.head())\n",
    "except Exception as e:\n",
    "    print(f\"OPTIMIZE command failed or is not fully supported for this setup: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a186c05",
   "metadata": {},
   "source": [
    "## 8. Time Travel / Snapshot Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0adf361e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available snapshots for 'employees':\n",
      "                      committed_at          snapshot_id     parent_id  \\\n",
      "0 2025-05-10 22:33:12.114000+00:00  8543142657744447243           NaN   \n",
      "1 2025-05-10 22:33:14.216000+00:00  5764103492386225884  8.543143e+18   \n",
      "2 2025-05-10 22:33:34.465000+00:00  7092746016936639603  5.764103e+18   \n",
      "3 2025-05-10 22:33:34.635000+00:00  9202396242307286450  7.092746e+18   \n",
      "4 2025-05-10 22:33:34.908000+00:00   327419525842522026  9.202396e+18   \n",
      "\n",
      "  operation                                      manifest_list  \\\n",
      "0    append  s3a://iceberg-warehouse/my_schema/employees-8a...   \n",
      "1    append  s3a://iceberg-warehouse/my_schema/employees-8a...   \n",
      "2    append  s3a://iceberg-warehouse/my_schema/employees-8a...   \n",
      "3    append  s3a://iceberg-warehouse/my_schema/employees-8a...   \n",
      "4   replace  s3a://iceberg-warehouse/my_schema/employees-8a...   \n",
      "\n",
      "                                             summary  \n",
      "0  {'trino_query_id': '20250510_223311_00003_xptf...  \n",
      "1  {'trino_query_id': '20250510_223313_00005_xptf...  \n",
      "2  {'trino_query_id': '20250510_223334_00015_xptf...  \n",
      "3  {'trino_query_id': '20250510_223334_00016_xptf...  \n",
      "4  {'trino_query_id': '20250510_223334_00018_xptf...  \n",
      "\n",
      "Querying data from snapshot ID 8543142657744447243 (first append operation):\n",
      "Empty DataFrame\n",
      "Columns: [id, name, department, salary, hire_date]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "snapshots_df = run_trino_query(f\"SELECT * FROM {CATALOG}.{SCHEMA_NAME}.\\\"{TABLE_NAME}$snapshots\\\" ORDER BY committed_at ASC\")\n",
    "print(\"\\nAvailable snapshots for 'employees':\")\n",
    "print(snapshots_df)\n",
    "\n",
    "if len(snapshots_df) > 1:\n",
    "    # Try to get a snapshot before the last data modification (e.g., before OPTIMIZE or last INSERT)\n",
    "    # This depends on how many operations were performed. Let's pick the first data snapshot.\n",
    "    # The first snapshot is often table creation (empty), so pick one that likely has data.\n",
    "    # Find first 'append' operation snapshot ID\n",
    "    first_append_snapshot_id = None\n",
    "    for index, row in snapshots_df.iterrows():\n",
    "        if row['operation'] == 'append':\n",
    "            first_append_snapshot_id = row['snapshot_id']\n",
    "            break\n",
    "            \n",
    "    if first_append_snapshot_id:\n",
    "        print(f\"\\nQuerying data from snapshot ID {first_append_snapshot_id} (first append operation):\")\n",
    "        query_snapshot_sql = f\"SELECT * FROM {FQN_TABLE_NAME} FOR VERSION AS OF {first_append_snapshot_id}\"\n",
    "        snapshot_data_df = run_trino_query(query_snapshot_sql)\n",
    "        print(snapshot_data_df)\n",
    "    else:\n",
    "        print(\"\\nCould not find an 'append' snapshot for time travel example.\")\n",
    "else:\n",
    "    print(\"\\nNot enough snapshots to demonstrate time travel.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a53bba",
   "metadata": {},
   "source": [
    "## 9. Show Table DDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fa21807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Show create table for 'iceberg.my_schema.employees':\n",
      "CREATE TABLE iceberg.my_schema.employees (\n",
      "   id integer,\n",
      "   name varchar,\n",
      "   department varchar,\n",
      "   salary decimal(10, 2),\n",
      "   hire_date date\n",
      ")\n",
      "WITH (\n",
      "   format = 'PARQUET',\n",
      "   format_version = 2,\n",
      "   location = 's3a://iceberg-warehouse/my_schema/employees-8a59513ed2f4495aa605770eef242832',\n",
      "   max_commit_retry = 4,\n",
      "   partitioning = ARRAY['department']\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nShow create table for '{FQN_TABLE_NAME}':\")\n",
    "create_table_stmt_df = run_trino_query(f\"SHOW CREATE TABLE {FQN_TABLE_NAME}\")\n",
    "if not create_table_stmt_df.empty:\n",
    "    print(create_table_stmt_df.iloc[0,0])\n",
    "else:\n",
    "    print(\"Could not retrieve DDL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b768c1",
   "metadata": {},
   "source": [
    "## 10. Clean up (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b41d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(run_trino_query(f\"DROP TABLE IF EXISTS {FQN_TABLE_NAME}\", fetch_results=False))\n",
    "# print(run_trino_query(f\"DROP TABLE IF EXISTS {FQN_EVENTS_TABLE}\", fetch_results=False))\n",
    "# print(run_trino_query(f\"DROP SCHEMA IF EXISTS {CATALOG}.{SCHEMA_NAME}\", fetch_results=False))\n",
    "# print(\"\\nSchemas after potential cleanup:\")\n",
    "# schemas_after_cleanup_df = run_trino_query(f\"SHOW SCHEMAS FROM {CATALOG}\")\n",
    "# print(schemas_after_cleanup_df)\n",
    "\n",
    "print(\"\\nTrino Iceberg Datalakehouse Demo (Phase 1) completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
