{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2b6551b-a33c-496d-9e18-3ef03c0e3bdd",
   "metadata": {},
   "source": [
    "## Using local spark session\n",
    "(just as an example that spark can be used locally without managing/connecting to cluster,also used for reading local files in folder testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7cbf310-4dd7-4f9b-b515-d37be0b796be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T21:43:37.822654Z",
     "start_time": "2025-06-24T21:43:37.660366Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d945dbd-ab3e-4a4f-b249-f3b4fc8c06f6",
   "metadata": {},
   "source": [
    "as in notebook 02 - pásek a kšandy, probably not needed at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7baf1362-3d2e-4ce5-bdf6-a2c2efe472c3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!export PYSPARK_SUBMIT_ARGS='--conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\\\n",
    "        --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog\\\n",
    "        --conf spark.sql.catalog.spark_catalog.type=hive\\\n",
    "        --conf spark.sql.catalog.iceberg_catalog.type=jdbc\\\n",
    "        --conf spark.sql.catalog.iceberg_catalog.uri=jdbc:postgresql://postgres:5432/iceberg_catalog\\\n",
    "        --conf spark.sql.catalog.iceberg_catalog.jdbc.user=iceberg\\\n",
    "        --conf spark.sql.catalog.iceberg_catalog.jdbc.password=icebergpassword\\\n",
    "        --conf spark.sql.catalog.iceberg_catalog.driver=org.postgresql.Driver\\\n",
    "        --conf spark.sql.catalog.iceberg_catalog.warehouse=s3a://iceberg-warehouse/\\\n",
    "        --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000\\\n",
    "        --conf spark.hadoop.fs.s3a.access.key=admin\\\n",
    "        --conf spark.hadoop.fs.s3a.secret.key=password\\\n",
    "        --conf spark.hadoop.fs.s3a.path.style.access=true\\\n",
    "        --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false\\\n",
    "        --conf spark.driver.extraJavaOptions=\"-Daws.region=eu-central-1 -Daws.overrideDefaultRegion=true\"\\\n",
    "        --conf spark.executor.extraJavaOptions=\"-Daws.region=eu-central-1 -Daws.overrideDefaultRegion=true\"\\\n",
    "        --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.1,org.apache.hadoop:hadoop-aws:3.3.4,org.postgresql:postgresql:42.6.0,org.apache.spark:spark-avro_2.12:3.5.6\\\n",
    "        pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d7714d1-e1f3-4b0e-8305-2df9083c90a8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS']='''--conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\\\n",
    "        --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog\\\n",
    "        --conf spark.sql.catalog.spark_catalog.type=hive\\\n",
    "        --conf spark.sql.catalog.iceberg_catalog.type=jdbc\\\n",
    "        --conf spark.sql.catalog.iceberg_catalog.uri=jdbc:postgresql://postgres:5432/iceberg_catalog\\\n",
    "        --conf spark.sql.catalog.iceberg_catalog.jdbc.user=iceberg\\\n",
    "        --conf spark.sql.catalog.iceberg_catalog.jdbc.password=icebergpassword\\\n",
    "        --conf spark.sql.catalog.iceberg_catalog.driver=org.postgresql.Driver\\\n",
    "        --conf spark.sql.catalog.iceberg_catalog.warehouse=s3a://iceberg-warehouse/\\\n",
    "        --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000\\\n",
    "        --conf spark.hadoop.fs.s3a.access.key=admin\\\n",
    "        --conf spark.hadoop.fs.s3a.secret.key=password\\\n",
    "        --conf spark.hadoop.fs.s3a.path.style.access=true\\\n",
    "        --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false\\\n",
    "        --conf spark.driver.extraJavaOptions=\"-Daws.region=eu-central-1 -Daws.overrideDefaultRegion=true\"\\\n",
    "        --conf spark.executor.extraJavaOptions=\"-Daws.region=eu-central-1 -Daws.overrideDefaultRegion=true\"\\\n",
    "        --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.1,org.apache.hadoop:hadoop-aws:3.3.4,org.postgresql:postgresql:42.6.0,org.apache.spark:spark-avro_2.12:3.5.6\\\n",
    "        pyspark-shell'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc84db2-ebc2-4267-a9a7-1c476858c1a8",
   "metadata": {},
   "source": [
    "#### create spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7090738-2d78-4eba-89fd-88601e4a8b86",
   "metadata": {},
   "source": [
    "local session is defined by `.master(\"local[number_of_cores]\")`, to use all available cores, write `\"local[*]\"` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5676d95-3b03-4442-92c2-ff60a7bfc53b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T21:45:33.861354Z",
     "start_time": "2025-06-24T21:45:32.243208Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "org.postgresql#postgresql added as a dependency\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-565c8e27-8b53-4599-a628-3e34356241f6;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.9.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound org.postgresql#postgresql;42.6.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.5.6 in central\n",
      "\tfound org.tukaani#xz;1.9 in central\n",
      ":: resolution report :: resolve 118ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.9.1 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.5.6 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.6.0 from central in [default]\n",
      "\torg.tukaani#xz;1.9 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   8   |   0   |   0   |   0   ||   8   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-565c8e27-8b53-4599-a628-3e34356241f6\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 8 already retrieved (0kB/3ms)\n",
      "25/07/11 13:40:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/11 13:40:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master(\"local[4]\")\n",
    "         .config(\"spark.executor.memory\", \"6g\")\n",
    "         .config(\"spark.driver.memory\", \"6g\")\n",
    "         .config(\"spark.hadoop.fs.s3a.endpoint.region\",\"eu-central-1\")\n",
    "         .appName(\"spark-local\")\n",
    "        ).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ba046-d329-41e5-862a-2e9dc7e234e7",
   "metadata": {},
   "source": [
    "again, change the url to localhost:14040-14043, if the session from notebook 02 is still running, it will most probably be [localhost:14041](http://localhost:14041) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2da18cb-5489-441d-90e6-ad7c8e34e9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://1fdeeb2782da:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark-local</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff8e1a8dd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d36313f-66ad-4e02-b0ee-fa54cb597783",
   "metadata": {},
   "source": [
    "let's create a table from the json structure\n",
    "- first explode the array 'records'\n",
    "- then transform the struct field with select \"col.*\"\n",
    "- then transform string fields to dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2587206a-beb6-4041-8fce-b60e75de7bf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T21:45:59.807539Z",
     "start_time": "2025-06-24T21:45:59.805344Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dataset: string (nullable = true)\n",
      " |-- limit: long (nullable = true)\n",
      " |-- records: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- HourDK: string (nullable = true)\n",
      " |    |    |-- HourUTC: string (nullable = true)\n",
      " |    |    |-- PriceArea: string (nullable = true)\n",
      " |    |    |-- SpotPriceDKK: double (nullable = true)\n",
      " |    |    |-- SpotPriceEUR: double (nullable = true)\n",
      " |-- sort: string (nullable = true)\n",
      " |-- total: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the folder with jsons\n",
    "df = spark.read.json(\"testdata/\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3c3dfa3-505e-4b34-a656-a7d0b87ec196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col: struct (nullable = true)\n",
      " |    |-- HourDK: string (nullable = true)\n",
      " |    |-- HourUTC: string (nullable = true)\n",
      " |    |-- PriceArea: string (nullable = true)\n",
      " |    |-- SpotPriceDKK: double (nullable = true)\n",
      " |    |-- SpotPriceEUR: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# explode the array records\n",
    "df2=df.select(F.explode(\"records\"))\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e52a3593-645b-406e-9f2b-4d5a4de5d8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HourDK: string (nullable = true)\n",
      " |-- HourUTC: string (nullable = true)\n",
      " |-- PriceArea: string (nullable = true)\n",
      " |-- SpotPriceDKK: double (nullable = true)\n",
      " |-- SpotPriceEUR: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select only the columns in column `col`\n",
    "df3=df2.select(F.col(\"col.*\"))\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48a7b6aa-46c8-4887-b2a9-29b2647ff214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HourDK: timestamp_ntz (nullable = true)\n",
      " |-- HourUTC: timestamp_ntz (nullable = true)\n",
      " |-- PriceArea: string (nullable = true)\n",
      " |-- SpotPriceDKK: double (nullable = true)\n",
      " |-- SpotPriceEUR: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert strings\n",
    "df4=df3.select(F.to_timestamp_ntz(\"HourDK\").alias(\"HourDK\"),               \n",
    "            F.to_timestamp_ntz(\"HourUTC\").alias(\"HourUTC\"),\n",
    "           \"PriceArea\", \"SpotPriceDKK\",\"SpotPriceEUR\")\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f64d3913-0e91-44dd-b868-0b767d6de335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HourDK</th>\n",
       "      <th>HourUTC</th>\n",
       "      <th>PriceArea</th>\n",
       "      <th>SpotPriceDKK</th>\n",
       "      <th>SpotPriceEUR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-10-31 20:00:00</td>\n",
       "      <td>2023-10-31 19:00:00</td>\n",
       "      <td>SYSTEM</td>\n",
       "      <td>482.980011</td>\n",
       "      <td>64.709999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-10-31 20:00:00</td>\n",
       "      <td>2023-10-31 19:00:00</td>\n",
       "      <td>SE3</td>\n",
       "      <td>147.559998</td>\n",
       "      <td>19.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-10-31 20:00:00</td>\n",
       "      <td>2023-10-31 19:00:00</td>\n",
       "      <td>DK2</td>\n",
       "      <td>939.530029</td>\n",
       "      <td>125.879997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-10-31 20:00:00</td>\n",
       "      <td>2023-10-31 19:00:00</td>\n",
       "      <td>DK1</td>\n",
       "      <td>939.530029</td>\n",
       "      <td>125.879997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-10-31 20:00:00</td>\n",
       "      <td>2023-10-31 19:00:00</td>\n",
       "      <td>SE4</td>\n",
       "      <td>147.559998</td>\n",
       "      <td>19.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-10-31 20:00:00</td>\n",
       "      <td>2023-10-31 19:00:00</td>\n",
       "      <td>DE</td>\n",
       "      <td>939.530029</td>\n",
       "      <td>125.879997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-10-31 19:00:00</td>\n",
       "      <td>2023-10-31 18:00:00</td>\n",
       "      <td>DE</td>\n",
       "      <td>1141.349976</td>\n",
       "      <td>152.919998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-10-31 19:00:00</td>\n",
       "      <td>2023-10-31 18:00:00</td>\n",
       "      <td>SE4</td>\n",
       "      <td>165.320007</td>\n",
       "      <td>22.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-10-31 19:00:00</td>\n",
       "      <td>2023-10-31 18:00:00</td>\n",
       "      <td>DK1</td>\n",
       "      <td>1141.349976</td>\n",
       "      <td>152.919998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-10-31 19:00:00</td>\n",
       "      <td>2023-10-31 18:00:00</td>\n",
       "      <td>DK2</td>\n",
       "      <td>1141.349976</td>\n",
       "      <td>152.919998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               HourDK             HourUTC PriceArea  SpotPriceDKK  \\\n",
       "0 2023-10-31 20:00:00 2023-10-31 19:00:00    SYSTEM    482.980011   \n",
       "1 2023-10-31 20:00:00 2023-10-31 19:00:00       SE3    147.559998   \n",
       "2 2023-10-31 20:00:00 2023-10-31 19:00:00       DK2    939.530029   \n",
       "3 2023-10-31 20:00:00 2023-10-31 19:00:00       DK1    939.530029   \n",
       "4 2023-10-31 20:00:00 2023-10-31 19:00:00       SE4    147.559998   \n",
       "5 2023-10-31 20:00:00 2023-10-31 19:00:00        DE    939.530029   \n",
       "6 2023-10-31 19:00:00 2023-10-31 18:00:00        DE   1141.349976   \n",
       "7 2023-10-31 19:00:00 2023-10-31 18:00:00       SE4    165.320007   \n",
       "8 2023-10-31 19:00:00 2023-10-31 18:00:00       DK1   1141.349976   \n",
       "9 2023-10-31 19:00:00 2023-10-31 18:00:00       DK2   1141.349976   \n",
       "\n",
       "   SpotPriceEUR  \n",
       "0     64.709999  \n",
       "1     19.770000  \n",
       "2    125.879997  \n",
       "3    125.879997  \n",
       "4     19.770000  \n",
       "5    125.879997  \n",
       "6    152.919998  \n",
       "7     22.150000  \n",
       "8    152.919998  \n",
       "9    152.919998  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show few rows\n",
    "df4.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c839d67-2806-4fa6-8652-9ade9bc03bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:===============================================>           (4 + 1) / 5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------------------+-----------------+\n",
      "|summary|PriceArea|SpotPriceDKK      |SpotPriceEUR     |\n",
      "+-------+---------+------------------+-----------------+\n",
      "|count  |1794951  |1494238           |1494262          |\n",
      "|mean   |NULL     |339.9898769339158 |45.64850028315928|\n",
      "|stddev |NULL     |374.63314435438866|50.34498024112369|\n",
      "|min    |DE       |-3723.469971      |-500.019989      |\n",
      "|max    |SYSTEM   |18173.359375      |2436.629883      |\n",
      "+-------+---------+------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df4.describe().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0e0d98a-9f0e-46dc-96b7-c1af64a1fed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do it in one step\n",
    "df = (spark.read.json(\"testdata/\")\n",
    "        .select(F.explode(\"records\"))\n",
    "        .select(F.col(\"col.*\"))\n",
    "        .select(F.to_timestamp_ntz(\"HourDK\").alias(\"HourDK\"),\n",
    "            F.to_timestamp_ntz(\"HourUTC\").alias(\"HourUTC\"),\n",
    "           \"PriceArea\", \"SpotPriceDKK\",\"SpotPriceEUR\")\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40500320-827d-4edc-9c06-4ce89f9c55b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1794951"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a872878-8f66-4aee-8455-ce0bf27ebf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HourDK: timestamp_ntz (nullable = true)\n",
      " |-- HourUTC: timestamp_ntz (nullable = true)\n",
      " |-- PriceArea: string (nullable = true)\n",
      " |-- SpotPriceDKK: double (nullable = true)\n",
      " |-- SpotPriceEUR: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2587726-c948-4ebd-b578-c80a81d07723",
   "metadata": {},
   "source": [
    "**perfect**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7477262-65bb-4c2a-8b08-c0cee4c4a85f",
   "metadata": {},
   "source": [
    "now select iceberg catalog to work with and create db for the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58898622-95b4-4f3c-bd24-aa0228dcd3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CatalogMetadata(name='spark_catalog', description=None)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listCatalogs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a977cc5e-2bf1-45ea-a447-6d8044848073",
   "metadata": {},
   "source": [
    "we can see only spark sees only the default catalog, so we now set the iceberg_catalog:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09fb35d2-8f7e-484f-955b-300959ff9b0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T21:46:07.026297Z",
     "start_time": "2025-06-24T21:46:06.710906Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.catalog.iceberg_catalog\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "spark.conf.set(\"spark.sql.catalog.iceberg_catalog.uri\", \"jdbc:postgresql://postgres:5432/iceberg_catalog\")\n",
    "spark.conf.set(\"spark.sql.catalog.iceberg_catalog.jdbc.user\", \"iceberg\") \n",
    "spark.conf.set(\"spark.sql.catalog.iceberg_catalog.jdbc.password\", \"icebergpassword\") \n",
    "spark.conf.set(\"spark.sql.catalog.iceberg_catalog.warehouse\", \"s3a://iceberg-warehouse/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80593034-7a8a-437f-b7b7-00952968790b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T21:46:09.158888Z",
     "start_time": "2025-06-24T21:46:08.817938Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentCatalog('iceberg_catalog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0b12606-18ef-4694-b5a5-d52c07027d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CatalogMetadata(name='iceberg_catalog', description=None),\n",
       " CatalogMetadata(name='spark_catalog', description=None)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listCatalogs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35f1a313-4d68-4916-9b6c-ae7dcd565564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='spark_schema', catalog='iceberg_catalog', description=None, locationUri='s3a://iceberg-warehouse/spark_schema'),\n",
       " Database(name='trino_schema', catalog='iceberg_catalog', description=None, locationUri='s3a://iceberg-warehouse/trino_schema/')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7604c63-0fca-454c-9414-cb82389b5d1e",
   "metadata": {},
   "source": [
    "ok, let's do this on bigger data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f6292e-9b18-4f36-b490-1d9911d7de8a",
   "metadata": {},
   "source": [
    "and save it to minio using iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "891d2d21-edaf-4bf0-a5e7-8b0294bbcd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/11 13:41:35 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Table(name='spark_orders', catalog='iceberg_catalog', namespace=['spark_schema'], description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables(\"spark_schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b68d4e66-9275-4622-921f-63947cc30cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='employees', catalog='iceberg_catalog', namespace=['trino_schema'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='events', catalog='iceberg_catalog', namespace=['trino_schema'], description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables(\"trino_schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9158e5e-410a-49c4-b8d9-2d2b7ac0c27a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/11 13:42:11 WARN S3ABlockOutputStream: Application invoked the Syncable API against stream writing to spark_schema/elspotprices/data/PriceArea=DK2/HourUTC_month=2021-11/00002-43-c7fc0f2a-61b0-4a8a-9ead-1f0a47f8311c-0-00007.parquet. This is unsupported\n",
      "                                                                                "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/16 11:26:15 WARN Tasks: Retrying task after failure: sleepTimeMs=5437 Failed to open input stream for file: s3a://iceberg-warehouse/prices/elspotprices1000000/metadata/00000-b521a475-bd9e-40c1-bc3d-0930f611a5ed.metadata.json\n",
      "org.apache.iceberg.exceptions.RuntimeIOException: Failed to open input stream for file: s3a://iceberg-warehouse/prices/elspotprices1000000/metadata/00000-b521a475-bd9e-40c1-bc3d-0930f611a5ed.metadata.json\n",
      "\tat org.apache.iceberg.hadoop.HadoopInputFile.newStream(HadoopInputFile.java:187)\n",
      "\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:290)\n",
      "\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:284)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$0(BaseMetastoreTableOperations.java:180)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$1(BaseMetastoreTableOperations.java:199)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:199)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:176)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:167)\n",
      "\tat org.apache.iceberg.jdbc.JdbcTableOperations.doRefresh(JdbcTableOperations.java:100)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refresh(BaseMetastoreTableOperations.java:88)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.current(BaseMetastoreTableOperations.java:71)\n",
      "\tat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:49)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\n",
      "\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1916)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\n",
      "\tat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:147)\n",
      "\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:844)\n",
      "\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:169)\n",
      "\tat org.apache.spark.sql.connector.catalog.TableCatalog.loadTable(TableCatalog.java:130)\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:361)\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:337)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$5(Analyzer.scala:1315)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1311)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1296)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1138)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1117)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1117)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1076)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:201)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.append(DataFrameWriterV2.scala:153)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.hadoop.fs.s3a.AWSBadRequestException: getFileStatus on s3a://iceberg-warehouse/prices/elspotprices1000000/metadata/00000-b521a475-bd9e-40c1-bc3d-0930f611a5ed.metadata.json: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 18498247DDAFAC67; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8:400 Bad Request: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 18498247DDAFAC67; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:249)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:175)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3796)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.extractOrFetchSimpleFileStatus(S3AFileSystem.java:5401)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1465)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1441)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
      "\tat org.apache.iceberg.hadoop.HadoopInputFile.newStream(HadoopInputFile.java:183)\n",
      "\t... 88 more\n",
      "Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 18498247DDAFAC67; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1372)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$10(S3AFileSystem.java:2545)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2533)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2513)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3776)\n",
      "\t... 94 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/16 11:26:20 WARN Tasks: Retrying task after failure: sleepTimeMs=5471 Failed to open input stream for file: s3a://iceberg-warehouse/prices/elspotprices1000000/metadata/00000-b521a475-bd9e-40c1-bc3d-0930f611a5ed.metadata.json\n",
      "org.apache.iceberg.exceptions.RuntimeIOException: Failed to open input stream for file: s3a://iceberg-warehouse/prices/elspotprices1000000/metadata/00000-b521a475-bd9e-40c1-bc3d-0930f611a5ed.metadata.json\n",
      "\tat org.apache.iceberg.hadoop.HadoopInputFile.newStream(HadoopInputFile.java:187)\n",
      "\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:290)\n",
      "\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:284)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$0(BaseMetastoreTableOperations.java:180)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$1(BaseMetastoreTableOperations.java:199)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:199)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:176)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:167)\n",
      "\tat org.apache.iceberg.jdbc.JdbcTableOperations.doRefresh(JdbcTableOperations.java:100)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refresh(BaseMetastoreTableOperations.java:88)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.current(BaseMetastoreTableOperations.java:71)\n",
      "\tat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:49)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\n",
      "\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1916)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\n",
      "\tat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:147)\n",
      "\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:844)\n",
      "\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:169)\n",
      "\tat org.apache.spark.sql.connector.catalog.TableCatalog.loadTable(TableCatalog.java:130)\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:361)\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:337)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$5(Analyzer.scala:1315)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1311)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1296)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1138)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1117)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1117)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1076)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:201)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.append(DataFrameWriterV2.scala:153)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.hadoop.fs.s3a.AWSBadRequestException: getFileStatus on s3a://iceberg-warehouse/prices/elspotprices1000000/metadata/00000-b521a475-bd9e-40c1-bc3d-0930f611a5ed.metadata.json: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 184982492230334F; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8:400 Bad Request: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 184982492230334F; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:249)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:175)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3796)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.extractOrFetchSimpleFileStatus(S3AFileSystem.java:5401)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1465)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1441)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
      "\tat org.apache.iceberg.hadoop.HadoopInputFile.newStream(HadoopInputFile.java:183)\n",
      "\t... 88 more\n",
      "Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 184982492230334F; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1372)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$10(S3AFileSystem.java:2545)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2533)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2513)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3776)\n",
      "\t... 94 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/16 11:26:26 WARN Tasks: Retrying task after failure: sleepTimeMs=5272 Failed to open input stream for file: s3a://iceberg-warehouse/prices/elspotprices1000000/metadata/00000-b521a475-bd9e-40c1-bc3d-0930f611a5ed.metadata.json\n",
      "org.apache.iceberg.exceptions.RuntimeIOException: Failed to open input stream for file: s3a://iceberg-warehouse/prices/elspotprices1000000/metadata/00000-b521a475-bd9e-40c1-bc3d-0930f611a5ed.metadata.json\n",
      "\tat org.apache.iceberg.hadoop.HadoopInputFile.newStream(HadoopInputFile.java:187)\n",
      "\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:290)\n",
      "\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:284)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$0(BaseMetastoreTableOperations.java:180)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$1(BaseMetastoreTableOperations.java:199)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:199)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:176)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:167)\n",
      "\tat org.apache.iceberg.jdbc.JdbcTableOperations.doRefresh(JdbcTableOperations.java:100)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refresh(BaseMetastoreTableOperations.java:88)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.current(BaseMetastoreTableOperations.java:71)\n",
      "\tat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:49)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\n",
      "\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1916)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\n",
      "\tat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:147)\n",
      "\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:844)\n",
      "\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:169)\n",
      "\tat org.apache.spark.sql.connector.catalog.TableCatalog.loadTable(TableCatalog.java:130)\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:361)\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:337)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$5(Analyzer.scala:1315)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1311)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1296)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1138)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1117)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1117)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1076)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:201)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.append(DataFrameWriterV2.scala:153)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.hadoop.fs.s3a.AWSBadRequestException: getFileStatus on s3a://iceberg-warehouse/prices/elspotprices1000000/metadata/00000-b521a475-bd9e-40c1-bc3d-0930f611a5ed.metadata.json: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 1849824A68A84EE1; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8:400 Bad Request: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 1849824A68A84EE1; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:249)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:175)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3796)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.extractOrFetchSimpleFileStatus(S3AFileSystem.java:5401)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1465)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1441)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
      "\tat org.apache.iceberg.hadoop.HadoopInputFile.newStream(HadoopInputFile.java:183)\n",
      "\t... 88 more\n",
      "Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 1849824A68A84EE1; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1372)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$10(S3AFileSystem.java:2545)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2533)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2513)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3776)\n",
      "\t... 94 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/16 11:26:31 WARN Tasks: Retrying task after failure: sleepTimeMs=5115 Failed to open input stream for file: s3a://iceberg-warehouse/prices/elspotprices1000000/metadata/00000-b521a475-bd9e-40c1-bc3d-0930f611a5ed.metadata.json\n",
      "org.apache.iceberg.exceptions.RuntimeIOException: Failed to open input stream for file: s3a://iceberg-warehouse/prices/elspotprices1000000/metadata/00000-b521a475-bd9e-40c1-bc3d-0930f611a5ed.metadata.json\n",
      "\tat org.apache.iceberg.hadoop.HadoopInputFile.newStream(HadoopInputFile.java:187)\n",
      "\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:290)\n",
      "\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:284)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$0(BaseMetastoreTableOperations.java:180)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$1(BaseMetastoreTableOperations.java:199)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:199)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:176)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:167)\n",
      "\tat org.apache.iceberg.jdbc.JdbcTableOperations.doRefresh(JdbcTableOperations.java:100)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refresh(BaseMetastoreTableOperations.java:88)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.current(BaseMetastoreTableOperations.java:71)\n",
      "\tat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:49)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\n",
      "\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1916)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\n",
      "\tat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:147)\n",
      "\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:844)\n",
      "\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:169)\n",
      "\tat org.apache.spark.sql.connector.catalog.TableCatalog.loadTable(TableCatalog.java:130)\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:361)\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:337)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$5(Analyzer.scala:1315)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1311)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1296)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1138)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1117)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1117)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1076)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:201)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.append(DataFrameWriterV2.scala:153)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.hadoop.fs.s3a.AWSBadRequestException: getFileStatus on s3a://iceberg-warehouse/prices/elspotprices1000000/metadata/00000-b521a475-bd9e-40c1-bc3d-0930f611a5ed.metadata.json: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 1849824BA3469193; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8:400 Bad Request: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 1849824BA3469193; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:249)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:175)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3796)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.extractOrFetchSimpleFileStatus(S3AFileSystem.java:5401)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1465)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1441)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
      "\tat org.apache.iceberg.hadoop.HadoopInputFile.newStream(HadoopInputFile.java:183)\n",
      "\t... 88 more\n",
      "Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 1849824BA3469193; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1372)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$10(S3AFileSystem.java:2545)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2533)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2513)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3776)\n",
      "\t... 94 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/16 11:26:36 WARN Tasks: Retrying task after failure: sleepTimeMs=5179 Failed to open input stream for file: s3a://iceberg-warehouse/prices/elspotprices1000000/metadata/00000-b521a475-bd9e-40c1-bc3d-0930f611a5ed.metadata.json\n",
      "org.apache.iceberg.exceptions.RuntimeIOException: Failed to open input stream for file: s3a://iceberg-warehouse/prices/elspotprices1000000/metadata/00000-b521a475-bd9e-40c1-bc3d-0930f611a5ed.metadata.json\n",
      "\tat org.apache.iceberg.hadoop.HadoopInputFile.newStream(HadoopInputFile.java:187)\n",
      "\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:290)\n",
      "\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:284)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$0(BaseMetastoreTableOperations.java:180)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$1(BaseMetastoreTableOperations.java:199)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:199)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:176)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:167)\n",
      "\tat org.apache.iceberg.jdbc.JdbcTableOperations.doRefresh(JdbcTableOperations.java:100)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refresh(BaseMetastoreTableOperations.java:88)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.current(BaseMetastoreTableOperations.java:71)\n",
      "\tat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:49)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\n",
      "\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1916)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\n",
      "\tat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:147)\n",
      "\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:844)\n",
      "\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:169)\n",
      "\tat org.apache.spark.sql.connector.catalog.TableCatalog.loadTable(TableCatalog.java:130)\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:361)\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:337)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$5(Analyzer.scala:1315)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1311)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1296)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1138)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1117)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1117)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1076)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:201)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.append(DataFrameWriterV2.scala:153)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.hadoop.fs.s3a.AWSBadRequestException: getFileStatus on s3a://iceberg-warehouse/prices/elspotprices1000000/metadata/00000-b521a475-bd9e-40c1-bc3d-0930f611a5ed.metadata.json: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 1849824CD4FBDEFA; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8:400 Bad Request: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 1849824CD4FBDEFA; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:249)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:175)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3796)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.extractOrFetchSimpleFileStatus(S3AFileSystem.java:5401)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1465)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1441)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
      "\tat org.apache.iceberg.hadoop.HadoopInputFile.newStream(HadoopInputFile.java:183)\n",
      "\t... 88 more\n",
      "Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 1849824CD4FBDEFA; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1372)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$10(S3AFileSystem.java:2545)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2533)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2513)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3776)\n",
      "\t... 94 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/16 11:26:41 WARN Tasks: Retrying task after failure: sleepTimeMs=5087 Failed to open input stream for file: s3a://iceberg-warehouse/prices/elspotprices1000000/metadata/00000-b521a475-bd9e-40c1-bc3d-0930f611a5ed.metadata.json\n",
      "org.apache.iceberg.exceptions.RuntimeIOException: Failed to open input stream for file: s3a://iceberg-warehouse/prices/elspotprices1000000/metadata/00000-b521a475-bd9e-40c1-bc3d-0930f611a5ed.metadata.json\n",
      "\tat org.apache.iceberg.hadoop.HadoopInputFile.newStream(HadoopInputFile.java:187)\n",
      "\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:290)\n",
      "\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:284)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$0(BaseMetastoreTableOperations.java:180)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$1(BaseMetastoreTableOperations.java:199)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:199)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:176)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:167)\n",
      "\tat org.apache.iceberg.jdbc.JdbcTableOperations.doRefresh(JdbcTableOperations.java:100)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refresh(BaseMetastoreTableOperations.java:88)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.current(BaseMetastoreTableOperations.java:71)\n",
      "\tat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:49)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\n",
      "\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1916)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\n",
      "\tat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:147)\n",
      "\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:844)\n",
      "\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:169)\n",
      "\tat org.apache.spark.sql.connector.catalog.TableCatalog.loadTable(TableCatalog.java:130)\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:361)\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:337)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$5(Analyzer.scala:1315)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1311)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1296)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1138)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1117)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1117)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1076)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:201)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.append(DataFrameWriterV2.scala:153)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.hadoop.fs.s3a.AWSBadRequestException: getFileStatus on s3a://iceberg-warehouse/prices/elspotprices1000000/metadata/00000-b521a475-bd9e-40c1-bc3d-0930f611a5ed.metadata.json: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 1849824E0A258440; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8:400 Bad Request: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 1849824E0A258440; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:249)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:175)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3796)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.extractOrFetchSimpleFileStatus(S3AFileSystem.java:5401)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1465)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1441)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
      "\tat org.apache.iceberg.hadoop.HadoopInputFile.newStream(HadoopInputFile.java:183)\n",
      "\t... 88 more\n",
      "Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 1849824E0A258440; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1372)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$10(S3AFileSystem.java:2545)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2533)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2513)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3776)\n",
      "\t... 94 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/16 11:26:46 WARN Tasks: Retrying task after failure: sleepTimeMs=5457 Failed to open input stream for file: s3a://iceberg-warehouse/prices/elspotprices1000000/metadata/00000-b521a475-bd9e-40c1-bc3d-0930f611a5ed.metadata.json\n",
      "org.apache.iceberg.exceptions.RuntimeIOException: Failed to open input stream for file: s3a://iceberg-warehouse/prices/elspotprices1000000/metadata/00000-b521a475-bd9e-40c1-bc3d-0930f611a5ed.metadata.json\n",
      "\tat org.apache.iceberg.hadoop.HadoopInputFile.newStream(HadoopInputFile.java:187)\n",
      "\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:290)\n",
      "\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:284)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$0(BaseMetastoreTableOperations.java:180)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$1(BaseMetastoreTableOperations.java:199)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:199)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:176)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:167)\n",
      "\tat org.apache.iceberg.jdbc.JdbcTableOperations.doRefresh(JdbcTableOperations.java:100)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refresh(BaseMetastoreTableOperations.java:88)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.current(BaseMetastoreTableOperations.java:71)\n",
      "\tat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:49)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\n",
      "\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1916)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\n",
      "\tat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:147)\n",
      "\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:844)\n",
      "\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:169)\n",
      "\tat org.apache.spark.sql.connector.catalog.TableCatalog.loadTable(TableCatalog.java:130)\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:361)\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:337)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$5(Analyzer.scala:1315)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1311)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1296)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1138)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1117)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1117)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1076)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:201)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.append(DataFrameWriterV2.scala:153)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.hadoop.fs.s3a.AWSBadRequestException: getFileStatus on s3a://iceberg-warehouse/prices/elspotprices1000000/metadata/00000-b521a475-bd9e-40c1-bc3d-0930f611a5ed.metadata.json: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 1849824F39D1EBB7; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8:400 Bad Request: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 1849824F39D1EBB7; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:249)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:175)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3796)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.extractOrFetchSimpleFileStatus(S3AFileSystem.java:5401)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1465)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1441)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
      "\tat org.apache.iceberg.hadoop.HadoopInputFile.newStream(HadoopInputFile.java:183)\n",
      "\t... 88 more\n",
      "Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 1849824F39D1EBB7; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1372)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$10(S3AFileSystem.java:2545)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2533)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2513)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3776)\n",
      "\t... 94 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/16 11:26:52 WARN Tasks: Retrying task after failure: sleepTimeMs=5049 Failed to open input stream for file: s3a://iceberg-warehouse/prices/elspotprices1000000/metadata/00000-b521a475-bd9e-40c1-bc3d-0930f611a5ed.metadata.json\n",
      "org.apache.iceberg.exceptions.RuntimeIOException: Failed to open input stream for file: s3a://iceberg-warehouse/prices/elspotprices1000000/metadata/00000-b521a475-bd9e-40c1-bc3d-0930f611a5ed.metadata.json\n",
      "\tat org.apache.iceberg.hadoop.HadoopInputFile.newStream(HadoopInputFile.java:187)\n",
      "\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:290)\n",
      "\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:284)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$0(BaseMetastoreTableOperations.java:180)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$1(BaseMetastoreTableOperations.java:199)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:199)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:176)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:167)\n",
      "\tat org.apache.iceberg.jdbc.JdbcTableOperations.doRefresh(JdbcTableOperations.java:100)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refresh(BaseMetastoreTableOperations.java:88)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.current(BaseMetastoreTableOperations.java:71)\n",
      "\tat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:49)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\n",
      "\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1916)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\n",
      "\tat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:147)\n",
      "\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:844)\n",
      "\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:169)\n",
      "\tat org.apache.spark.sql.connector.catalog.TableCatalog.loadTable(TableCatalog.java:130)\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:361)\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:337)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$5(Analyzer.scala:1315)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1311)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1296)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1138)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1117)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1117)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1076)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:201)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.append(DataFrameWriterV2.scala:153)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.hadoop.fs.s3a.AWSBadRequestException: getFileStatus on s3a://iceberg-warehouse/prices/elspotprices1000000/metadata/00000-b521a475-bd9e-40c1-bc3d-0930f611a5ed.metadata.json: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 184982507F9F7904; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8:400 Bad Request: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 184982507F9F7904; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:249)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:175)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3796)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.extractOrFetchSimpleFileStatus(S3AFileSystem.java:5401)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1465)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1441)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
      "\tat org.apache.iceberg.hadoop.HadoopInputFile.newStream(HadoopInputFile.java:183)\n",
      "\t... 88 more\n",
      "Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 184982507F9F7904; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1372)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$10(S3AFileSystem.java:2545)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2533)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2513)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3776)\n",
      "\t... 94 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/16 11:26:57 WARN Tasks: Retrying task after failure: sleepTimeMs=5351 Failed to open input stream for file: s3a://iceberg-warehouse/prices/elspotprices1000000/metadata/00000-b521a475-bd9e-40c1-bc3d-0930f611a5ed.metadata.json\n",
      "org.apache.iceberg.exceptions.RuntimeIOException: Failed to open input stream for file: s3a://iceberg-warehouse/prices/elspotprices1000000/metadata/00000-b521a475-bd9e-40c1-bc3d-0930f611a5ed.metadata.json\n",
      "\tat org.apache.iceberg.hadoop.HadoopInputFile.newStream(HadoopInputFile.java:187)\n",
      "\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:290)\n",
      "\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:284)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$0(BaseMetastoreTableOperations.java:180)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$1(BaseMetastoreTableOperations.java:199)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:199)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:176)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:167)\n",
      "\tat org.apache.iceberg.jdbc.JdbcTableOperations.doRefresh(JdbcTableOperations.java:100)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refresh(BaseMetastoreTableOperations.java:88)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.current(BaseMetastoreTableOperations.java:71)\n",
      "\tat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:49)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\n",
      "\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1916)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\n",
      "\tat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:147)\n",
      "\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:844)\n",
      "\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:169)\n",
      "\tat org.apache.spark.sql.connector.catalog.TableCatalog.loadTable(TableCatalog.java:130)\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:361)\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:337)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$5(Analyzer.scala:1315)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1311)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1296)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1138)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1117)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1117)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1076)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:201)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.append(DataFrameWriterV2.scala:153)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.hadoop.fs.s3a.AWSBadRequestException: getFileStatus on s3a://iceberg-warehouse/prices/elspotprices1000000/metadata/00000-b521a475-bd9e-40c1-bc3d-0930f611a5ed.metadata.json: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 18498251AD40B57E; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8:400 Bad Request: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 18498251AD40B57E; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:249)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:175)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3796)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.extractOrFetchSimpleFileStatus(S3AFileSystem.java:5401)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1465)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1441)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
      "\tat org.apache.iceberg.hadoop.HadoopInputFile.newStream(HadoopInputFile.java:183)\n",
      "\t... 88 more\n",
      "Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 18498251AD40B57E; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1372)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$10(S3AFileSystem.java:2545)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2533)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2513)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3776)\n",
      "\t... 94 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/16 11:27:02 WARN Tasks: Retrying task after failure: sleepTimeMs=5254 Failed to open input stream for file: s3a://iceberg-warehouse/prices/elspotprices1000000/metadata/00000-b521a475-bd9e-40c1-bc3d-0930f611a5ed.metadata.json\n",
      "org.apache.iceberg.exceptions.RuntimeIOException: Failed to open input stream for file: s3a://iceberg-warehouse/prices/elspotprices1000000/metadata/00000-b521a475-bd9e-40c1-bc3d-0930f611a5ed.metadata.json\n",
      "\tat org.apache.iceberg.hadoop.HadoopInputFile.newStream(HadoopInputFile.java:187)\n",
      "\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:290)\n",
      "\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:284)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$0(BaseMetastoreTableOperations.java:180)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$1(BaseMetastoreTableOperations.java:199)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:199)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:176)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:167)\n",
      "\tat org.apache.iceberg.jdbc.JdbcTableOperations.doRefresh(JdbcTableOperations.java:100)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.refresh(BaseMetastoreTableOperations.java:88)\n",
      "\tat org.apache.iceberg.BaseMetastoreTableOperations.current(BaseMetastoreTableOperations.java:71)\n",
      "\tat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:49)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\n",
      "\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1916)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\n",
      "\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\n",
      "\tat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:147)\n",
      "\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:844)\n",
      "\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:169)\n",
      "\tat org.apache.spark.sql.connector.catalog.TableCatalog.loadTable(TableCatalog.java:130)\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:361)\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:337)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$5(Analyzer.scala:1315)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$1(Analyzer.scala:1311)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1296)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1138)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1117)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1117)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1076)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:201)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.append(DataFrameWriterV2.scala:153)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.hadoop.fs.s3a.AWSBadRequestException: getFileStatus on s3a://iceberg-warehouse/prices/elspotprices1000000/metadata/00000-b521a475-bd9e-40c1-bc3d-0930f611a5ed.metadata.json: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 18498252ECB13243; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8:400 Bad Request: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 18498252ECB13243; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:249)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:175)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3796)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.extractOrFetchSimpleFileStatus(S3AFileSystem.java:5401)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1465)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1441)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
      "\tat org.apache.iceberg.hadoop.HadoopInputFile.newStream(HadoopInputFile.java:183)\n",
      "\t... 88 more\n",
      "Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 18498252ECB13243; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1372)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$10(S3AFileSystem.java:2545)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2533)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2513)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3776)\n",
      "\t... 94 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteTo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcatalog_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdb_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.elspotprices1000000\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43musing\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miceberg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitionedBy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPriceArea\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmonths\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHourUTC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m----> 4\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pyspark/sql/readwriter.py:2107\u001b[0m, in \u001b[0;36mDataFrameWriterV2.append\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2102\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m3.1\u001b[39m)\n\u001b[1;32m   2103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mappend\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;124;03m    Append the contents of the data frame to the output table.\u001b[39;00m\n\u001b[1;32m   2106\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2107\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "(df.writeTo(f\"iceberg_catalog.spark_schema.elspotprices\")\n",
    "    .using(\"iceberg\")\n",
    "    .partitionedBy(F.col(\"PriceArea\"),F.months(F.col(\"HourUTC\")))\n",
    ").createOrReplace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20611522-5b87-4bea-972d-592f50bd62d5",
   "metadata": {},
   "source": [
    "- not sure what the warning means, but the table is created, so whatever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef6552f-f788-4ff6-99c9-5b569a93cff7",
   "metadata": {},
   "source": [
    "**different way to create a table from dataframe - first register the dataframe as view, then use CTAS to write it to iceberg with limit 0 to create its structure, then writeTo...append():**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d219407c-9533-42b7-9968-b76da33e08a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createTempView(\"elspotprices2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53200ee0-329a-48c3-b7b8-c6da2bb5464c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create table iceberg_catalog.spark_schema.elspotprices2 as select * from elspotprices2 limit 0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81fc1e3-9501-4e6f-985f-94c24d4f5f8c",
   "metadata": {},
   "source": [
    "showcasing compaction on bigger data, deliberately saving the dataframe into small files, with 1000 records each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe31fe66-2484-4289-9718-9b337ff102c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.repartition(1000).writeTo(f\"iceberg_catalog.spark_schema.elspotprices2\").option(\"maxRecordsPerFile\", 1000).using('iceberg').append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18c3b515-ee06-4057-801a-f26bfc7456bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------+------------+------------------+\n",
      "|file_path                                                                                                            |record_count|file_size_in_bytes|\n",
      "+---------------------------------------------------------------------------------------------------------------------+------------+------------------+\n",
      "|s3a://iceberg-warehouse/spark_schema/elspotprices2/data/00000-51-a7b6b968-aefc-4abf-b55c-715eb4934caa-0-00001.parquet|1795        |37486             |\n",
      "|s3a://iceberg-warehouse/spark_schema/elspotprices2/data/00001-52-a7b6b968-aefc-4abf-b55c-715eb4934caa-0-00001.parquet|1795        |37296             |\n",
      "|s3a://iceberg-warehouse/spark_schema/elspotprices2/data/00002-53-a7b6b968-aefc-4abf-b55c-715eb4934caa-0-00001.parquet|1795        |37080             |\n",
      "|s3a://iceberg-warehouse/spark_schema/elspotprices2/data/00003-54-a7b6b968-aefc-4abf-b55c-715eb4934caa-0-00001.parquet|1795        |37056             |\n",
      "|s3a://iceberg-warehouse/spark_schema/elspotprices2/data/00004-55-a7b6b968-aefc-4abf-b55c-715eb4934caa-0-00001.parquet|1795        |36994             |\n",
      "|s3a://iceberg-warehouse/spark_schema/elspotprices2/data/00005-56-a7b6b968-aefc-4abf-b55c-715eb4934caa-0-00001.parquet|1794        |37176             |\n",
      "|s3a://iceberg-warehouse/spark_schema/elspotprices2/data/00006-57-a7b6b968-aefc-4abf-b55c-715eb4934caa-0-00001.parquet|1794        |37122             |\n",
      "|s3a://iceberg-warehouse/spark_schema/elspotprices2/data/00007-58-a7b6b968-aefc-4abf-b55c-715eb4934caa-0-00001.parquet|1794        |37084             |\n",
      "|s3a://iceberg-warehouse/spark_schema/elspotprices2/data/00008-59-a7b6b968-aefc-4abf-b55c-715eb4934caa-0-00001.parquet|1794        |37043             |\n",
      "|s3a://iceberg-warehouse/spark_schema/elspotprices2/data/00009-60-a7b6b968-aefc-4abf-b55c-715eb4934caa-0-00001.parquet|1794        |37115             |\n",
      "|s3a://iceberg-warehouse/spark_schema/elspotprices2/data/00010-61-a7b6b968-aefc-4abf-b55c-715eb4934caa-0-00001.parquet|1794        |37160             |\n",
      "|s3a://iceberg-warehouse/spark_schema/elspotprices2/data/00011-62-a7b6b968-aefc-4abf-b55c-715eb4934caa-0-00001.parquet|1794        |36997             |\n",
      "|s3a://iceberg-warehouse/spark_schema/elspotprices2/data/00012-63-a7b6b968-aefc-4abf-b55c-715eb4934caa-0-00001.parquet|1794        |37103             |\n",
      "|s3a://iceberg-warehouse/spark_schema/elspotprices2/data/00013-64-a7b6b968-aefc-4abf-b55c-715eb4934caa-0-00001.parquet|1794        |37038             |\n",
      "|s3a://iceberg-warehouse/spark_schema/elspotprices2/data/00014-65-a7b6b968-aefc-4abf-b55c-715eb4934caa-0-00001.parquet|1794        |37302             |\n",
      "|s3a://iceberg-warehouse/spark_schema/elspotprices2/data/00015-66-a7b6b968-aefc-4abf-b55c-715eb4934caa-0-00001.parquet|1794        |37128             |\n",
      "|s3a://iceberg-warehouse/spark_schema/elspotprices2/data/00016-67-a7b6b968-aefc-4abf-b55c-715eb4934caa-0-00001.parquet|1794        |37123             |\n",
      "|s3a://iceberg-warehouse/spark_schema/elspotprices2/data/00017-68-a7b6b968-aefc-4abf-b55c-715eb4934caa-0-00001.parquet|1794        |37105             |\n",
      "|s3a://iceberg-warehouse/spark_schema/elspotprices2/data/00018-69-a7b6b968-aefc-4abf-b55c-715eb4934caa-0-00001.parquet|1794        |37303             |\n",
      "|s3a://iceberg-warehouse/spark_schema/elspotprices2/data/00019-70-a7b6b968-aefc-4abf-b55c-715eb4934caa-0-00001.parquet|1794        |37100             |\n",
      "+---------------------------------------------------------------------------------------------------------------------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    SELECT file_path, record_count, file_size_in_bytes\n",
    "    FROM iceberg_catalog.spark_schema.elspotprices2.files\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4da189f7-f679-47e0-a8b1-6ed3706fc491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "|rewritten_data_files_count|added_data_files_count|rewritten_bytes_count|failed_data_files_count|\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "|0                         |0                     |0                    |0                      |\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = spark.sql(f\"\"\"CALL iceberg_catalog.system.rewrite_data_files(\n",
    "                      table => 'spark_schema.elspotprices2', \n",
    "                      strategy => 'sort', sort_order => 'HourUTC ASC',\n",
    "                      options => map('min-input-files','5'))\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d847278f-00c3-4c06-9851-dd1a55523708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "|rewritten_data_files_count|added_data_files_count|rewritten_bytes_count|failed_data_files_count|\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "|0                         |0                     |0                    |0                      |\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# summary of what happened:\n",
    "result_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ae929b06-6a48-4a9f-8ccd-f85f138c09de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------+------------+------------------+\n",
      "|file_path                                                                                                              |record_count|file_size_in_bytes|\n",
      "+-----------------------------------------------------------------------------------------------------------------------+------------+------------------+\n",
      "|s3a://iceberg-warehouse/spark_schema/elspotprices2/data/00000-1084-41377737-1c8d-4269-a25b-29de199fdba6-0-00001.parquet|1794951     |11741509          |\n",
      "+-----------------------------------------------------------------------------------------------------------------------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"SELECT file_path, record_count, file_size_in_bytes\n",
    "FROM iceberg_catalog.spark_schema.elspotprices2.files\"\"\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b39317-4f44-4485-a546-a559a27a3849",
   "metadata": {},
   "source": [
    "view snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "95ef4b7e-ef5b-4b7a-9a35-778bf5ce989e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|        committed_at|        snapshot_id|          parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|2025-07-11 13:51:...|6092011551680698518|4259939819494638523|  replace|s3a://iceberg-war...|{added-data-files...|\n",
      "|2025-07-11 13:49:...|4259939819494638523| 673444911004302166|   append|s3a://iceberg-war...|{spark.app.id -> ...|\n",
      "|2025-07-11 13:43:...| 673444911004302166|               NULL|   append|s3a://iceberg-war...|{spark.app.id -> ...|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "\n",
      "+-------------------+--------------------------------------------------------------------------------------------------------------------------------+\n",
      "|snapshot_id        |manifest_list                                                                                                                   |\n",
      "+-------------------+--------------------------------------------------------------------------------------------------------------------------------+\n",
      "|6092011551680698518|s3a://iceberg-warehouse/spark_schema/elspotprices2/metadata/snap-6092011551680698518-1-d1c67091-97e7-4262-9fe3-4c1201a302a1.avro|\n",
      "|4259939819494638523|s3a://iceberg-warehouse/spark_schema/elspotprices2/metadata/snap-4259939819494638523-1-81d488d9-0cdd-4e86-bf7e-2267e6c0029a.avro|\n",
      "|673444911004302166 |s3a://iceberg-warehouse/spark_schema/elspotprices2/metadata/snap-673444911004302166-1-496b239d-720b-4aa9-a126-deddff4403b4.avro |\n",
      "+-------------------+--------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snapshot_id</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6092011551680698518</td>\n",
       "      <td>{'engine-version': '3.5.6', 'added-data-files': '1', 'total-equality-deletes': '0', 'app-id': 'local-1752241220564', 'added-records': '1794951', 'deleted-data-files': '1000', 'deleted-records': '1794951', 'total-records': '1794951', 'removed-files-size': '37125750', 'changed-partition-count': '1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4259939819494638523</td>\n",
       "      <td>{'engine-version': '3.5.6', 'added-data-files': '1000', 'total-equality-deletes': '0', 'app-id': 'local-1752241220564', 'added-records': '1794951', 'total-records': '1794951', 'spark.app.id': 'local-1752241220564', 'changed-partition-count': '1', 'engine-name': 'spark', 'total-position-deletes':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>673444911004302166</td>\n",
       "      <td>{'spark.app.id': 'local-1752241220564', 'changed-partition-count': '0', 'engine-version': '3.5.6', 'total-equality-deletes': '0', 'app-id': 'local-1752241220564', 'engine-name': 'spark', 'total-position-deletes': '0', 'total-delete-files': '0', 'iceberg-version': 'Apache Iceberg 1.9.1 (commit f4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           snapshot_id  \\\n",
       "0  6092011551680698518   \n",
       "1  4259939819494638523   \n",
       "2   673444911004302166   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                       summary  \n",
       "0  {'engine-version': '3.5.6', 'added-data-files': '1', 'total-equality-deletes': '0', 'app-id': 'local-1752241220564', 'added-records': '1794951', 'deleted-data-files': '1000', 'deleted-records': '1794951', 'total-records': '1794951', 'removed-files-size': '37125750', 'changed-partition-count': '1...  \n",
       "1  {'engine-version': '3.5.6', 'added-data-files': '1000', 'total-equality-deletes': '0', 'app-id': 'local-1752241220564', 'added-records': '1794951', 'total-records': '1794951', 'spark.app.id': 'local-1752241220564', 'changed-partition-count': '1', 'engine-name': 'spark', 'total-position-deletes':...  \n",
       "2  {'spark.app.id': 'local-1752241220564', 'changed-partition-count': '0', 'engine-version': '3.5.6', 'total-equality-deletes': '0', 'app-id': 'local-1752241220564', 'engine-name': 'spark', 'total-position-deletes': '0', 'total-delete-files': '0', 'iceberg-version': 'Apache Iceberg 1.9.1 (commit f4...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM iceberg_catalog.spark_schema.elspotprices2.snapshots ORDER BY committed_at DESC\").show(truncate=True)\n",
    "spark.sql(\"SELECT snapshot_id, manifest_list FROM iceberg_catalog.spark_schema.elspotprices2.snapshots ORDER BY committed_at DESC\").show(truncate=False)\n",
    "spark.sql(\"SELECT snapshot_id, summary FROM iceberg_catalog.spark_schema.elspotprices2.snapshots ORDER BY committed_at DESC\").toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a7b2224d-48be-4d0a-910f-83c3c70d02f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- committed_at: timestamp (nullable = false)\n",
      " |-- snapshot_id: long (nullable = false)\n",
      " |-- parent_id: long (nullable = true)\n",
      " |-- operation: string (nullable = true)\n",
      " |-- manifest_list: string (nullable = true)\n",
      " |-- summary: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snapshots_df = spark.sql(\"SELECT * FROM iceberg_catalog.spark_schema.elspotprices2.snapshots ORDER BY committed_at DESC\")\n",
    "snapshots_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4043f9e-a0a3-4fac-b3db-d5558399d154",
   "metadata": {},
   "source": [
    "to display summary in human friendly way let's explode it and pivot key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d959d7bd-a586-41d6-b0ab-75f26ccfabab",
   "metadata": {},
   "outputs": [],
   "source": [
    "snpshts_ex_summ_df = (snapshots_df\n",
    "                      .select(*[F.col(f\"{col}\") for col in snapshots_df.columns if col!=\"summary\"], F.explode(\"summary\"))\n",
    "                      .groupBy(*[F.col(f\"{col}\") for col in snapshots_df.columns if col!=\"summary\"])\n",
    "                          .pivot(\"key\")\n",
    "                          .agg(F.first(\"value\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e20fd6a7-2155-4986-9408-754f609c1125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>committed_at</th>\n",
       "      <th>snapshot_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>operation</th>\n",
       "      <th>manifest_list</th>\n",
       "      <th>added-data-files</th>\n",
       "      <th>added-files-size</th>\n",
       "      <th>added-records</th>\n",
       "      <th>app-id</th>\n",
       "      <th>changed-partition-count</th>\n",
       "      <th>deleted-data-files</th>\n",
       "      <th>deleted-records</th>\n",
       "      <th>engine-name</th>\n",
       "      <th>engine-version</th>\n",
       "      <th>iceberg-version</th>\n",
       "      <th>removed-files-size</th>\n",
       "      <th>spark.app.id</th>\n",
       "      <th>total-data-files</th>\n",
       "      <th>total-delete-files</th>\n",
       "      <th>total-equality-deletes</th>\n",
       "      <th>total-files-size</th>\n",
       "      <th>total-position-deletes</th>\n",
       "      <th>total-records</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-07-11 13:43:17.153</td>\n",
       "      <td>673444911004302166</td>\n",
       "      <td>NaN</td>\n",
       "      <td>append</td>\n",
       "      <td>s3a://iceberg-warehouse/spark_schema/elspotprices2/metadata/snap-673444911004302166-1-496b239d-720b-4aa9-a126-deddff4403b4.avro</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>local-1752241220564</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>spark</td>\n",
       "      <td>3.5.6</td>\n",
       "      <td>Apache Iceberg 1.9.1 (commit f40208ae6fb2f33e578c2637d3dea1db18739f31)</td>\n",
       "      <td>None</td>\n",
       "      <td>local-1752241220564</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-07-11 13:49:36.775</td>\n",
       "      <td>4259939819494638523</td>\n",
       "      <td>6.734449e+17</td>\n",
       "      <td>append</td>\n",
       "      <td>s3a://iceberg-warehouse/spark_schema/elspotprices2/metadata/snap-4259939819494638523-1-81d488d9-0cdd-4e86-bf7e-2267e6c0029a.avro</td>\n",
       "      <td>1000</td>\n",
       "      <td>37125750</td>\n",
       "      <td>1794951</td>\n",
       "      <td>local-1752241220564</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>spark</td>\n",
       "      <td>3.5.6</td>\n",
       "      <td>Apache Iceberg 1.9.1 (commit f40208ae6fb2f33e578c2637d3dea1db18739f31)</td>\n",
       "      <td>None</td>\n",
       "      <td>local-1752241220564</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37125750</td>\n",
       "      <td>0</td>\n",
       "      <td>1794951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-07-11 13:51:42.775</td>\n",
       "      <td>6092011551680698518</td>\n",
       "      <td>4.259940e+18</td>\n",
       "      <td>replace</td>\n",
       "      <td>s3a://iceberg-warehouse/spark_schema/elspotprices2/metadata/snap-6092011551680698518-1-d1c67091-97e7-4262-9fe3-4c1201a302a1.avro</td>\n",
       "      <td>1</td>\n",
       "      <td>11741509</td>\n",
       "      <td>1794951</td>\n",
       "      <td>local-1752241220564</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>1794951</td>\n",
       "      <td>spark</td>\n",
       "      <td>3.5.6</td>\n",
       "      <td>Apache Iceberg 1.9.1 (commit f40208ae6fb2f33e578c2637d3dea1db18739f31)</td>\n",
       "      <td>37125750</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11741509</td>\n",
       "      <td>0</td>\n",
       "      <td>1794951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             committed_at          snapshot_id     parent_id operation  \\\n",
       "0 2025-07-11 13:43:17.153   673444911004302166           NaN    append   \n",
       "1 2025-07-11 13:49:36.775  4259939819494638523  6.734449e+17    append   \n",
       "2 2025-07-11 13:51:42.775  6092011551680698518  4.259940e+18   replace   \n",
       "\n",
       "                                                                                                                      manifest_list  \\\n",
       "0   s3a://iceberg-warehouse/spark_schema/elspotprices2/metadata/snap-673444911004302166-1-496b239d-720b-4aa9-a126-deddff4403b4.avro   \n",
       "1  s3a://iceberg-warehouse/spark_schema/elspotprices2/metadata/snap-4259939819494638523-1-81d488d9-0cdd-4e86-bf7e-2267e6c0029a.avro   \n",
       "2  s3a://iceberg-warehouse/spark_schema/elspotprices2/metadata/snap-6092011551680698518-1-d1c67091-97e7-4262-9fe3-4c1201a302a1.avro   \n",
       "\n",
       "  added-data-files added-files-size added-records               app-id  \\\n",
       "0             None             None          None  local-1752241220564   \n",
       "1             1000         37125750       1794951  local-1752241220564   \n",
       "2                1         11741509       1794951  local-1752241220564   \n",
       "\n",
       "  changed-partition-count deleted-data-files deleted-records engine-name  \\\n",
       "0                       0               None            None       spark   \n",
       "1                       1               None            None       spark   \n",
       "2                       1               1000         1794951       spark   \n",
       "\n",
       "  engine-version  \\\n",
       "0          3.5.6   \n",
       "1          3.5.6   \n",
       "2          3.5.6   \n",
       "\n",
       "                                                          iceberg-version  \\\n",
       "0  Apache Iceberg 1.9.1 (commit f40208ae6fb2f33e578c2637d3dea1db18739f31)   \n",
       "1  Apache Iceberg 1.9.1 (commit f40208ae6fb2f33e578c2637d3dea1db18739f31)   \n",
       "2  Apache Iceberg 1.9.1 (commit f40208ae6fb2f33e578c2637d3dea1db18739f31)   \n",
       "\n",
       "  removed-files-size         spark.app.id total-data-files total-delete-files  \\\n",
       "0               None  local-1752241220564                0                  0   \n",
       "1               None  local-1752241220564             1000                  0   \n",
       "2           37125750                 None                1                  0   \n",
       "\n",
       "  total-equality-deletes total-files-size total-position-deletes total-records  \n",
       "0                      0                0                      0             0  \n",
       "1                      0         37125750                      0       1794951  \n",
       "2                      0         11741509                      0       1794951  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "pandas.set_option(\"display.max_colwidth\",300)\n",
    "snpshts_ex_summ_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5adfcc03-227d-4488-ab57-a11bb8db882a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentCatalog(\"iceberg_catalog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e6550ac9-f808-414d-9341-d4ef61e91599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='elspotprices', catalog='iceberg_catalog', namespace=['spark_schema'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='elspotprices2', catalog='iceberg_catalog', namespace=['spark_schema'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='spark_orders', catalog='iceberg_catalog', namespace=['spark_schema'], description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Table(name='employees', catalog='iceberg_catalog', namespace=['trino_schema'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='events', catalog='iceberg_catalog', namespace=['trino_schema'], description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(spark.catalog.listTables(\"spark_schema\"))\n",
    "print()\n",
    "display(spark.catalog.listTables(\"trino_schema\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e12e9ac1-b5a5-4b9b-b538-8bcb5a4675e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', catalog='spark_catalog', description='default database', locationUri='file:/home/jovyan/notebooks/spark-warehouse')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Table(name='elspotprices2', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.catalog.setCurrentCatalog(\"spark_catalog\")\n",
    "display(spark.catalog.listDatabases())\n",
    "display(spark.catalog.listTables(\"default\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dce58c6-6994-45e6-afe9-dc120d2ef4a3",
   "metadata": {},
   "source": [
    "oh, what we didn't try is to read spark-created table with trino, but why shouldn't that work..\n",
    "anyway:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "805b4bc7-f042-435c-a285-3a3d9310e79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "import pandas as pd\n",
    "\n",
    "TRINO_HOST = 'trino-coordinator' # Service name in docker-compose\n",
    "TRINO_PORT = 8080\n",
    "TRINO_USER = 'testuser' # Can be any string, Trino by default doesn't enforce auth in this setup\n",
    "CATALOG = 'iceberg' # Catalog name as defined in iceberg.properties\n",
    "\n",
    "# Connection string for Trino\n",
    "trino_conn_str = f'trino://{TRINO_USER}@{TRINO_HOST}:{TRINO_PORT}/{CATALOG}'\n",
    "engine = create_engine(trino_conn_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "159c8959-e155-47cd-991d-4c932b653195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Trino: trino://testuser@trino-coordinator:8080/iceberg\n"
     ]
    }
   ],
   "source": [
    "def run_trino_query(query, fetch_results=True):\n",
    "    \"\"\"Executes a Trino query and optionally fetches results into a Pandas DataFrame.\"\"\"\n",
    "    with engine.connect() as connection:\n",
    "        # For queries that modify data or schema, autocommit is usually the default or not needed to be set explicitly for Trino\n",
    "        # For DML/DDL, we might not always fetch results\n",
    "        result_proxy = connection.execute(text(query))\n",
    "        if fetch_results and result_proxy.returns_rows:\n",
    "            df = pd.DataFrame(result_proxy.fetchall(), columns=result_proxy.keys())\n",
    "            return df\n",
    "        elif fetch_results: # No rows returned but fetch_results was true\n",
    "            return pd.DataFrame(columns=result_proxy.keys() if result_proxy.returns_rows else [])\n",
    "        else:\n",
    "            print(f\"Query executed successfully (returns_rows={result_proxy.returns_rows}).\")\n",
    "            # For DDL/DML, we might want to check row count if available\n",
    "            # print(f\"Rows affected (approx): {result_proxy.rowcount}\") # rowcount might not be reliable for all statements/drivers\n",
    "            return None\n",
    "\n",
    "print(f\"Connected to Trino: {trino_conn_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1fd9e0f6-ae85-4fc4-8bfd-215363834c5b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available schemas in Iceberg catalog:\n",
      "               Schema\n",
      "0  information_schema\n",
      "1        spark_schema\n",
      "2              system\n",
      "3        trino_schema\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAvailable schemas in Iceberg catalog:\")\n",
    "schemas_df = run_trino_query(f\"SHOW SCHEMAS FROM {CATALOG}\")\n",
    "print(schemas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fea07d32-7416-4f37-b878-7652bebb11f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA_NAME = 'spark_schema'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b47511db-0bd3-4040-9eb8-6249eb2cb200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Table</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>elspotprices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elspotprices2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spark_orders</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Table\n",
       "0   elspotprices\n",
       "1  elspotprices2\n",
       "2   spark_orders"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_trino_query(f\"show tables from {CATALOG}.{SCHEMA_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ca3a75ae-9ffc-4d43-9573-66906426663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trino_df=run_trino_query(f\"select * from {CATALOG}.{SCHEMA_NAME}.elspotprices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "521c1058-94ed-4703-bb18-9d91f7eee294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hourdk</th>\n",
       "      <th>hourutc</th>\n",
       "      <th>pricearea</th>\n",
       "      <th>spotpricedkk</th>\n",
       "      <th>spotpriceeur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-07-01 01:00:00</td>\n",
       "      <td>2025-06-30 23:00:00</td>\n",
       "      <td>DK1</td>\n",
       "      <td>710.930110</td>\n",
       "      <td>95.290001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-04-01 01:00:00</td>\n",
       "      <td>2025-03-31 23:00:00</td>\n",
       "      <td>DK1</td>\n",
       "      <td>709.001210</td>\n",
       "      <td>95.019997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-04-01 01:00:00</td>\n",
       "      <td>2022-03-31 23:00:00</td>\n",
       "      <td>DK2</td>\n",
       "      <td>741.070007</td>\n",
       "      <td>99.639999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-04-01 00:00:00</td>\n",
       "      <td>2025-03-31 22:00:00</td>\n",
       "      <td>DK1</td>\n",
       "      <td>757.800081</td>\n",
       "      <td>101.559998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-03-31 23:00:00</td>\n",
       "      <td>2025-03-31 21:00:00</td>\n",
       "      <td>DK1</td>\n",
       "      <td>763.589412</td>\n",
       "      <td>102.339996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794946</th>\n",
       "      <td>1999-12-01 05:00:00</td>\n",
       "      <td>1999-12-01 04:00:00</td>\n",
       "      <td>DE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794947</th>\n",
       "      <td>1999-12-01 04:00:00</td>\n",
       "      <td>1999-12-01 03:00:00</td>\n",
       "      <td>DE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794948</th>\n",
       "      <td>1999-12-01 03:00:00</td>\n",
       "      <td>1999-12-01 02:00:00</td>\n",
       "      <td>DE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794949</th>\n",
       "      <td>1999-12-01 02:00:00</td>\n",
       "      <td>1999-12-01 01:00:00</td>\n",
       "      <td>DE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794950</th>\n",
       "      <td>1999-12-01 01:00:00</td>\n",
       "      <td>1999-12-01 00:00:00</td>\n",
       "      <td>DE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1794951 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     hourdk             hourutc pricearea  spotpricedkk  \\\n",
       "0       2025-07-01 01:00:00 2025-06-30 23:00:00       DK1    710.930110   \n",
       "1       2025-04-01 01:00:00 2025-03-31 23:00:00       DK1    709.001210   \n",
       "2       2022-04-01 01:00:00 2022-03-31 23:00:00       DK2    741.070007   \n",
       "3       2025-04-01 00:00:00 2025-03-31 22:00:00       DK1    757.800081   \n",
       "4       2025-03-31 23:00:00 2025-03-31 21:00:00       DK1    763.589412   \n",
       "...                     ...                 ...       ...           ...   \n",
       "1794946 1999-12-01 05:00:00 1999-12-01 04:00:00        DE           NaN   \n",
       "1794947 1999-12-01 04:00:00 1999-12-01 03:00:00        DE           NaN   \n",
       "1794948 1999-12-01 03:00:00 1999-12-01 02:00:00        DE           NaN   \n",
       "1794949 1999-12-01 02:00:00 1999-12-01 01:00:00        DE           NaN   \n",
       "1794950 1999-12-01 01:00:00 1999-12-01 00:00:00        DE           NaN   \n",
       "\n",
       "         spotpriceeur  \n",
       "0           95.290001  \n",
       "1           95.019997  \n",
       "2           99.639999  \n",
       "3          101.559998  \n",
       "4          102.339996  \n",
       "...               ...  \n",
       "1794946           NaN  \n",
       "1794947           NaN  \n",
       "1794948           NaN  \n",
       "1794949           NaN  \n",
       "1794950           NaN  \n",
       "\n",
       "[1794951 rows x 5 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trino_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9880e66b-7532-4cbb-990e-2b468b6696b1",
   "metadata": {},
   "source": [
    "ok, this is the end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
