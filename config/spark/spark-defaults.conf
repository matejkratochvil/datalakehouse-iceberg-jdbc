# --- Default Iceberg Catalog (using JDBC) ---
# This defines a catalog named 'iceberg_catalog'
spark.sql.catalog.iceberg_catalog               org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.iceberg_catalog.catalog-impl  org.apache.iceberg.jdbc.JdbcCatalog
spark.sql.catalog.iceberg_catalog.uri           jdbc:postgresql://postgres:5432/iceberg_catalog
spark.sql.catalog.iceberg_catalog.jdbc.user     iceberg
spark.sql.catalog.iceberg_catalog.jdbc.password icebergpassword
spark.sql.catalog.iceberg_catalog.driver        org.postgresql.Driver
spark.sql.catalog.iceberg_catalog.warehouse     s3a://iceberg-warehouse/

# --- S3A Configuration for MinIO (needed by Spark workers and driver) ---
spark.hadoop.fs.s3a.endpoint                 http://minio:9000
spark.hadoop.fs.s3a.endpoint.region          eu-central-1
spark.hadoop.fs.s3a.access.key               admin
spark.hadoop.fs.s3a.secret.key               password
spark.hadoop.fs.s3a.path.style.access        true
spark.hadoop.fs.s3a.connection.ssl.enabled   false
# Adding a dummy region can sometimes help with S3 clients
spark.driver.extraJavaOptions              -Daws.region=eu-central-1
spark.executor.extraJavaOptions            -Daws.region=eu-central-1

spark.jars.packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.1,org.apache.hadoop:hadoop-aws:3.3.4,org.postgresql:postgresql:42.6.0
spark.master                               spark://spark-master:7077
# Only if not set by PYSPARK_SUBMIT_ARGS or spark-submit


# --- Jars ---
# If not using --packages, you would list jars here or use spark.jars.packages

# --- Spark Master ---